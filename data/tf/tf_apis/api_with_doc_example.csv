API,Examples
"tf.DeviceSpec(
    job=None, replica=None, task=None, device_type=None, device_index=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import DeviceSpec
device_spec = DeviceSpec(job=""ps"", device_type=""GPU"", device_index=0)
with tf.device(device_spec.to_string()):
  my_var = tf.Variable(..., name=""my_variable"")
  squared_var = tf.square(my_var)
"
"tf.GradientTape(
    persistent=False, watch_accessed_variables=True
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GradientTape
x = tf.constant(3.0)
with tf.GradientTape() as g:
  g.watch(x)
  y = x * x
dy_dx = g.gradient(y, x)
print(dy_dx)
tf.Tensor(6.0, shape=(), dtype=float32)"
"tf.OptionalSpec(
    element_spec
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import OptionalSpec
@tf.function(input_signature=[tf.OptionalSpec(
  tf.TensorSpec(shape=(), dtype=tf.int32, name=None))])
def maybe_square(optional):
  if optional.has_value():
    x = optional.get_value()
    return x * x
  return -1
optional = tf.experimental.Optional.from_value(5)
print(maybe_square(optional))
tf.Tensor(25, shape=(), dtype=int32)"
"tf.RegisterGradient(
    op_type
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RegisterGradient
@tf.RegisterGradient(""Sub"")
def _sub_grad(unused_op, grad):
  return grad, tf.negative(grad)
"
"tf.Tensor(
    op, value_index, dtype
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Tensor
c = tf.constant([[1.0, 2.0], [3.0, 4.0]])
d = tf.constant([[1.0, 1.0], [0.0, 1.0]])
e = tf.matmul(c, d)
print(e)
tf.Tensor(
[[1. 3.]
 [3. 7.]], shape=(2, 2), dtype=float32)"
"tf.TensorArray(
    dtype,
    size=None,
    dynamic_size=None,
    clear_after_read=None,
    tensor_array_name=None,
    handle=None,
    flow=None,
    infer_shape=True,
    element_shape=None,
    colocate_with_first_write_call=True,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TensorArray
ta = tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False)
ta = ta.write(0, 10)
ta = ta.write(1, 20)
ta = ta.write(2, 30)
ta.read(0)
<tf.Tensor: shape=(), dtype=float32, numpy=10.0>
<tf.Tensor: shape=(), dtype=float32, numpy=10.0>
ta.read(1)
<tf.Tensor: shape=(), dtype=float32, numpy=20.0>
<tf.Tensor: shape=(), dtype=float32, numpy=20.0>
ta.read(2)
<tf.Tensor: shape=(), dtype=float32, numpy=30.0>
<tf.Tensor: shape=(), dtype=float32, numpy=30.0>
ta.stack()
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([10., 20., 30.],
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([10., 20., 30.],
dtype=float32)>"
"tf.TensorSpec(
    shape,
    dtype=tf.dtypes.float32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
experimental_as_proto() -> struct_pb2.TensorSpecProto
"
"tf.Variable(
    initial_value=None,
    trainable=None,
    validate_shape=True,
    caching_device=None,
    name=None,
    variable_def=None,
    dtype=None,
    import_scope=None,
    constraint=None,
    synchronization=tf.VariableSynchronization.AUTO,
    aggregation=tf.compat.v1.VariableAggregation.NONE,
    shape=None,
    experimental_enable_variable_lifting=True
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Variable
v = tf.Variable(1.)
v.assign(2.)
<tf.Variable ... shape=() dtype=float32, numpy=2.0>
<tf.Variable ... shape=() dtype=float32, numpy=2.0>
v.assign_add(0.5)
<tf.Variable ... shape=() dtype=float32, numpy=2.5>
<tf.Variable ... shape=() dtype=float32, numpy=2.5>"
"tf.math.abs(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import abs
x = tf.constant([-2.25, 3.25])
tf.abs(x)
<tf.Tensor: shape=(2,), dtype=float32,
<tf.Tensor: shape=(2,), dtype=float32,
numpy=array([2.25, 3.25], dtype=float32)>"
"tf.math.acos(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import acos
x = tf.constant([1.0, -0.5, 3.4, 0.2, 0.0, -2], dtype = tf.float32)
tf.math.acos(x)
<tf.Tensor: shape=(6,), dtype=float32,
<tf.Tensor: shape=(6,), dtype=float32,
numpy= array([0. , 2.0943952, nan, 1.3694383, 1.5707964, nan],
dtype=float32)>"
"tf.math.acosh(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import acosh
x = tf.constant([-2, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
tf.math.acosh(x) ==> [nan nan 0. 0.62236255 5.9914584 9.903487 inf]
"
"tf.math.add(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import add
x = [1, 2, 3, 4, 5]
y = 1
tf.add(x, y)
<tf.Tensor: shape=(5,), dtype=int32, numpy=array([2, 3, 4, 5, 6],
<tf.Tensor: shape=(5,), dtype=int32, numpy=array([2, 3, 4, 5, 6],
dtype=int32)>"
"tf.math.add_n(
    inputs, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import add_n
a = tf.constant([[3, 5], [4, 8]])
b = tf.constant([[1, 6], [2, 9]])
tf.math.add_n([a, b, a]).numpy()
array([[ 7, 16],
       [10, 25]], dtype=int32)"
"tf.math.argmax(
    input,
    axis=None,
    output_type=tf.dtypes.int64,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import argmax
A = tf.constant([2, 20, 30, 3, 6])
<tf.Tensor: shape=(), dtype=int64, numpy=2>
<tf.Tensor: shape=(), dtype=int64, numpy=2>
B = tf.constant([[2, 20, 30, 3, 6], [3, 11, 16, 1, 8],
                 [14, 45, 23, 5, 27]])
tf.math.argmax(B, 0)
<tf.Tensor: shape=(5,), dtype=int64, numpy=array([2, 2, 0, 2, 2])>
<tf.Tensor: shape=(5,), dtype=int64, numpy=array([2, 2, 0, 2, 2])>
tf.math.argmax(B, 1)
<tf.Tensor: shape=(3,), dtype=int64, numpy=array([2, 2, 1])>
<tf.Tensor: shape=(3,), dtype=int64, numpy=array([2, 2, 1])>
C = tf.constant([0, 0, 0, 0])
<tf.Tensor: shape=(), dtype=int64, numpy=0>
<tf.Tensor: shape=(), dtype=int64, numpy=0>"
"tf.math.argmin(
    input,
    axis=None,
    output_type=tf.dtypes.int64,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import argmin
import tensorflow as tf
a = [1, 10, 26.9, 2.8, 166.32, 62.3]
b = tf.math.argmin(input = a)
c = tf.keras.backend.eval(b)
"
"tf.argsort(
    values, axis=-1, direction='ASCENDING', stable=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import argsort
values = [1, 10, 26.9, 2.8, 166.32, 62.3]
sort_order = tf.argsort(values)
sort_order.numpy()
array([0, 3, 1, 2, 5, 4], dtype=int32)"
"tf.strings.as_string(
    input,
    precision=-1,
    scientific=False,
    shortest=False,
    width=-1,
    fill='',
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import as_string
tf.strings.as_string([3, 2])
<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'3', b'2'], dtype=object)>
<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'3', b'2'], dtype=object)>
tf.strings.as_string([3.1415926, 2.71828], precision=2).numpy()
array([b'3.14', b'2.72'], dtype=object)"
"tf.math.asin(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import asin
x = tf.constant([1.047, 0.785])

"
"tf.math.asinh(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import asinh
  x = tf.constant([-float(""inf""), -2, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
  tf.math.asinh(x) ==> [-inf -1.4436355 -0.4812118 0.8813736 1.0159732 5.991471 9.903487 inf]
"
"tf.math.atan(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import atan
x = tf.constant([1.047, 0.785])

"
"tf.math.atan2(
    y, x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import atan2
x = [1., 1.]
y = [1., -1.]
print((tf.math.atan2(y,x) * (180 / np.pi)).numpy())
[ 45. -45.]"
"tf.math.atanh(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import atanh
  x = tf.constant([-float(""inf""), -1, -0.5, 1, 0, 0.5, 10, float(""inf"")])
  tf.math.atanh(x) ==> [nan -inf -0.54930615 inf  0. 0.54930615 nan nan]
"
"tf.autodiff.ForwardAccumulator(
    primals, tangents
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ForwardAccumulator
x = tf.constant([[2.0, 3.0], [1.0, 4.0]])
targets = tf.constant([[1.], [-1.]])
dense = tf.keras.layers.Dense(1)
dense.build([None, 2])
with tf.autodiff.ForwardAccumulator(
   primals=dense.kernel,
   tangents=tf.constant([[1.], [0.]])) as acc:
  loss = tf.reduce_sum((dense(x) - targets) ** 2.)
acc.jvp(loss)
<tf.Tensor: shape=(), dtype=float32, numpy=...>
<tf.Tensor: shape=(), dtype=float32, numpy=...>"
"tf.GradientTape(
    persistent=False, watch_accessed_variables=True
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GradientTape
x = tf.constant(3.0)
with tf.GradientTape() as g:
  g.watch(x)
  y = x * x
dy_dx = g.gradient(y, x)
print(dy_dx)
tf.Tensor(6.0, shape=(), dtype=float32)"
"tf.autograph.experimental.set_loop_options(
    parallel_iterations=UNSPECIFIED,
    swap_memory=UNSPECIFIED,
    maximum_iterations=UNSPECIFIED,
    shape_invariants=UNSPECIFIED
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import set_loop_options
>>> @tf.function(autograph=True)
... def f():
...   n = 0
...   for i in tf.range(10):
...     tf.autograph.experimental.set_loop_options(maximum_iterations=3)
...     n += 1
...   return n
"
"tf.autograph.set_verbosity(
    level, alsologtostdout=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import set_verbosity
import os
import tensorflow as tf

os.environ['AUTOGRAPH_VERBOSITY'] = '5'

tf.autograph.set_verbosity(0)

os.environ['AUTOGRAPH_VERBOSITY'] = '1'
"
"tf.autograph.to_code(
    entity, recursive=True, experimental_optional_features=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import to_code
def f(x):
  if x < 0:
  if x < 0:
    x = -x
  return x
tf.autograph.to_code(f)
""...def tf__f(x):..."""
"tf.autograph.to_graph(
    entity, recursive=True, experimental_optional_features=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import to_graph
def f(x):
  if x > 0:
    y = x * x
  else:
    y = -x
  return y
converted_f = to_graph(f)
x = tf.constant(2)
<tf.Tensor: shape=(), dtype=int32, numpy=4>
<tf.Tensor: shape=(), dtype=int32, numpy=4>"
"tf.autograph.trace(
    *args
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import trace
import tensorflow as tf

for i in tf.range(10):
  tf.autograph.trace(i)
# Output: <Tensor ...>
"
"tf.bitcast(
    input, type, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bitcast
a = [1., 2., 3.]
equality_bitcast = tf.bitcast(a, tf.complex128)
Traceback (most recent call last):
InvalidArgumentError: Cannot bitcast from 1 to 18 [Op:Bitcast]
equality_cast = tf.cast(a, tf.complex128)
print(equality_cast)
tf.Tensor([1.+0.j 2.+0.j 3.+0.j], shape=(3,), dtype=complex128)"
"tf.bitwise.bitwise_and(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bitwise_and
import tensorflow as tf
from tensorflow.python.ops import bitwise_ops
dtype_list = [tf.int8, tf.int16, tf.int32, tf.int64,
              tf.uint8, tf.uint16, tf.uint32, tf.uint64]

for dtype in dtype_list:
  lhs = tf.constant([0, 5, 3, 14], dtype=dtype)
  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)
  exp = tf.constant([0, 0, 3, 10], dtype=tf.float32)

  res = bitwise_ops.bitwise_and(lhs, rhs)
"
"tf.bitwise.bitwise_or(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bitwise_or
import tensorflow as tf
from tensorflow.python.ops import bitwise_ops
dtype_list = [tf.int8, tf.int16, tf.int32, tf.int64,
              tf.uint8, tf.uint16, tf.uint32, tf.uint64]

for dtype in dtype_list:
  lhs = tf.constant([0, 5, 3, 14], dtype=dtype)
  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)
  exp = tf.constant([5, 5, 7, 15], dtype=tf.float32)

  res = bitwise_ops.bitwise_or(lhs, rhs)
"
"tf.bitwise.bitwise_xor(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bitwise_xor
import tensorflow as tf
from tensorflow.python.ops import bitwise_ops
dtype_list = [tf.int8, tf.int16, tf.int32, tf.int64,
              tf.uint8, tf.uint16, tf.uint32, tf.uint64]

for dtype in dtype_list:
  lhs = tf.constant([0, 5, 3, 14], dtype=dtype)
  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)
  exp = tf.constant([5, 5, 4, 5],  dtype=tf.float32)

  res = bitwise_ops.bitwise_xor(lhs, rhs)
"
"tf.bitwise.invert(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import invert
import tensorflow as tf
from tensorflow.python.ops import bitwise_ops

tf.assert_equal(-3, bitwise_ops.invert(2))

dtype_list = [dtypes.int8, dtypes.int16, dtypes.int32, dtypes.int64,
              dtypes.uint8, dtypes.uint16, dtypes.uint32, dtypes.uint64]

inputs = [0, 5, 3, 14]
for dtype in dtype_list:
  input_tensor = tf.constant([0, 5, 3, 14], dtype=dtype)
  not_a_and_a, not_a_or_a, not_0 = [bitwise_ops.bitwise_and(
                                      input_tensor, bitwise_ops.invert(input_tensor)),
                                    bitwise_ops.bitwise_or(
                                      input_tensor, bitwise_ops.invert(input_tensor)),
                                    bitwise_ops.invert(
                                      tf.constant(0, dtype=dtype))]

  expected = tf.constant([0, 0, 0, 0], dtype=tf.float32)
  tf.assert_equal(tf.cast(not_a_and_a, tf.float32), expected)

  expected = tf.cast([not_0] * 4, tf.float32)
  tf.assert_equal(tf.cast(not_a_or_a, tf.float32), expected)

  if dtype.is_unsigned:
    inverted = bitwise_ops.invert(input_tensor)
    expected = tf.constant([dtype.max - x for x in inputs], dtype=tf.float32)
    tf.assert_equal(tf.cast(inverted, tf.float32), tf.cast(expected, tf.float32))
"
"tf.bitwise.left_shift(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import left_shift
import tensorflow as tf
from tensorflow.python.ops import bitwise_ops
import numpy as np
dtype_list = [tf.int8, tf.int16, tf.int32, tf.int64]

for dtype in dtype_list:
  lhs = tf.constant([-1, -5, -3, -14], dtype=dtype)
  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)

  left_shift_result = bitwise_ops.left_shift(lhs, rhs)

  print(left_shift_result)


lhs = np.array([-2, 64, 101, 32], dtype=np.int8)
rhs = np.array([-1, -5, -3, -14], dtype=np.int8)
bitwise_ops.left_shift(lhs, rhs)
# <tf.Tensor: shape=(4,), dtype=int8, numpy=array([ -2,  64, 101,  32], dtype=int8)>
"
"tf.bitwise.right_shift(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import right_shift
import tensorflow as tf
from tensorflow.python.ops import bitwise_ops
import numpy as np
dtype_list = [tf.int8, tf.int16, tf.int32, tf.int64]

for dtype in dtype_list:
  lhs = tf.constant([-1, -5, -3, -14], dtype=dtype)
  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)

  right_shift_result = bitwise_ops.right_shift(lhs, rhs)

  print(right_shift_result)


lhs = np.array([-2, 64, 101, 32], dtype=np.int8)
rhs = np.array([-1, -5, -3, -14], dtype=np.int8)
bitwise_ops.right_shift(lhs, rhs)
# <tf.Tensor: shape=(4,), dtype=int8, numpy=array([ -2,  64, 101,  32], dtype=int8)>
"
"tf.boolean_mask(
    tensor, mask, axis=None, name='boolean_mask'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import boolean_mask
mask = np.array([True, False, True, False])
tf.boolean_mask(tensor, mask)
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 2], dtype=int32)>
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 2], dtype=int32)>"
"tf.broadcast_dynamic_shape(
    shape_x, shape_y
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import broadcast_dynamic_shape
shape_x = (1, 2, 3)
shape_y = (5, 1, 3)
tf.broadcast_dynamic_shape(shape_x, shape_y)
<tf.Tensor: shape=(3,), dtype=int32, numpy=array([5, 2, 3], ...>
<tf.Tensor: shape=(3,), dtype=int32, numpy=array([5, 2, 3], ...>"
"tf.broadcast_static_shape(
    shape_x, shape_y
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import broadcast_static_shape
shape_x = tf.TensorShape([1, 2, 3])
shape_y = tf.TensorShape([5, 1 ,3])
tf.broadcast_static_shape(shape_x, shape_y)
TensorShape([5, 2, 3])"
"tf.broadcast_to(
    input, shape, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import broadcast_to
y = tf.broadcast_to(x, [2, 3])
print(y)
tf.Tensor(
    [[1 2 3]
     [1 2 3]], shape=(2, 3), dtype=int32)"
"tf.cast(
    x, dtype, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import cast
x = tf.constant([1.8, 2.2], dtype=tf.float32)
tf.cast(x, tf.int32)
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>"
"tf.clip_by_norm(
    t, clip_norm, axes=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import clip_by_norm
some_nums = tf.constant([[1, 2, 3, 4, 5]], dtype=tf.float32)
tf.clip_by_norm(some_nums, 2.0).numpy()
array([[0.26967996, 0.5393599 , 0.80903983, 1.0787199 , 1.3483998 ]],
      dtype=float32)"
"tf.clip_by_value(
    t, clip_value_min, clip_value_max, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import clip_by_value
t = tf.constant([[-10., -1., 0.], [0., 2., 10.]])
t2 = tf.clip_by_value(t, clip_value_min=-1, clip_value_max=1)
t2.numpy()
array([[-1., -1.,  0.],
       [ 0.,  1.,  1.]], dtype=float32)"
"tf.compat.dimension_at_index(
    shape, index
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import dimension_at_index
dim = tensor_shape[i]

dim = dimension_at_index(tensor_shape, i)


if tensor_shape.rank is None:
  dim = Dimension(None)
else:
  dim = tensor_shape.dims[i]

"
"tf.compat.dimension_value(
    dimension
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import dimension_value
value = tensor_shape[i].value

value = dimension_value(tensor_shape[i])

"
"tf.compat.path_to_str(
    path
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import path_to_str
$ tf.compat.path_to_str('C:\XYZ\tensorflow\./.././tensorflow')
$ tf.compat.path_to_str(Path('C:\XYZ\tensorflow\./.././tensorflow'))
$ tf.compat.path_to_str(Path('./corpus'))
$ tf.compat.path_to_str('./.././Corpus')
$ tf.compat.path_to_str(Path('./.././Corpus'))
$ tf.compat.path_to_str(Path('./..////../'))

"
"tf.dtypes.complex(
    real, imag, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import complex
real = tf.constant([2.25, 3.25])
imag = tf.constant([4.75, 5.75])
"
"tf.cond(
    pred, true_fn=None, false_fn=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import cond
z = tf.multiply(a, b)
result = tf.cond(x < y, lambda: tf.add(x, z), lambda: tf.square(y))
result = tf.cond(x < y, lambda: tf.add(x, z), lambda: tf.square(y))
"
"tf.config.experimental.enable_tensor_float_32_execution(
    enabled
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import enable_tensor_float_32_execution
x = tf.fill((2, 2), 1.0001)
y = tf.fill((2, 2), 1.)
tf.config.experimental.enable_tensor_float_32_execution(False)
"
"tf.config.experimental.get_device_details(
    device
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get_device_details
gpu_devices = tf.config.list_physical_devices('GPU')
if gpu_devices:
  details = tf.config.experimental.get_device_details(gpu_devices[0])
  details.get('device_name', 'Unknown GPU')"
"tf.config.experimental.get_memory_growth(
    device
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get_memory_growth
physical_devices = tf.config.list_physical_devices('GPU')
try:
  tf.config.experimental.set_memory_growth(physical_devices[0], True)
  assert tf.config.experimental.get_memory_growth(physical_devices[0])
except:
  pass"
"tf.config.experimental.get_memory_info(
    device
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get_memory_info
if tf.config.list_physical_devices('GPU'):
  # Returns a dict in the form {'current': <current mem usage>,
  #                             'peak': <peak mem usage>}
  tf.config.experimental.get_memory_info('GPU:0')"
"tf.config.experimental.get_memory_usage(
    device
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get_memory_usage
gpu_devices = tf.config.list_physical_devices('GPU')
if gpu_devices:
  tf.config.experimental.get_memory_usage('GPU:0')"
"tf.config.get_logical_device_configuration(
    device
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get_logical_device_configuration
physical_devices = tf.config.list_physical_devices('CPU')
assert len(physical_devices) == 1, ""No CPUs found""
configs = tf.config.get_logical_device_configuration(
  physical_devices[0])
try:
  assert configs is None
  tf.config.set_logical_device_configuration(
    physical_devices[0],
    [tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration()])
  configs = tf.config.get_logical_device_configuration(
    physical_devices[0])
  assert len(configs) == 2
except:
  pass"
"tf.config.get_visible_devices(
    device_type=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get_visible_devices
physical_devices = tf.config.list_physical_devices('GPU')
try:
  tf.config.set_visible_devices([], 'GPU')
  visible_devices = tf.config.get_visible_devices()
  for device in visible_devices:
    assert device.device_type != 'GPU'
except:
  pass"
"tf.config.list_logical_devices(
    device_type=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import list_logical_devices
logical_devices = tf.config.list_logical_devices('GPU')
if len(logical_devices) > 0:
  with tf.device(logical_devices[0].name):
    one = tf.constant(1)
  with tf.device(logical_devices[1].name):
    two = tf.constant(2)"
"tf.config.list_physical_devices(
    device_type=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import list_physical_devices
physical_devices = tf.config.list_physical_devices('GPU')
print(""Num GPUs:"", len(physical_devices))
Num GPUs: ..."
"tf.config.experimental.reset_memory_stats(
    device
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reset_memory_stats
if tf.config.list_physical_devices('GPU'):
  tf.config.experimental.reset_memory_stats('GPU:0')
  x1 = tf.ones(1000 * 1000, dtype=tf.float64)
  peak1 = tf.config.experimental.get_memory_info('GPU:0')['peak']
  tf.config.experimental.reset_memory_stats('GPU:0')
  x2 = tf.ones(1000 * 1000, dtype=tf.float32)
  del x2
  peak2 = tf.config.experimental.get_memory_info('GPU:0')['peak']
  assert peak2 < peak1  # tf.float32 consumes less memory than tf.float64."
"tf.config.experimental.set_memory_growth(
    device, enable
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import set_memory_growth
physical_devices = tf.config.list_physical_devices('GPU')
try:
  tf.config.experimental.set_memory_growth(physical_devices[0], True)
except:
  pass"
"tf.config.set_logical_device_configuration(
    device, logical_devices
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import set_logical_device_configuration
physical_devices = tf.config.list_physical_devices('CPU')
assert len(physical_devices) == 1, ""No CPUs found""
try:
  tf.config.set_logical_device_configuration(
    physical_devices[0],
    [tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration()])
  logical_devices = tf.config.list_logical_devices('CPU')
  assert len(logical_devices) == 2
  tf.config.set_logical_device_configuration(
    physical_devices[0],
    [tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration()])
except:
  pass"
"tf.config.set_visible_devices(
    devices, device_type=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import set_visible_devices
physical_devices = tf.config.list_physical_devices('GPU')
try:
  tf.config.set_visible_devices(physical_devices[1:], 'GPU')
  logical_devices = tf.config.list_logical_devices('GPU')
  assert len(logical_devices) == len(physical_devices) - 1
except:
  pass"
"tf.config.experimental_connect_to_cluster(
    cluster_spec_or_resolver,
    job_name='localhost',
    task_index=0,
    protocol=None,
    make_master_device_default=True,
    cluster_device_filters=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import experimental_connect_to_cluster
cdf = tf.config.experimental.ClusterDeviceFilters()
for i in range(num_workers):
  cdf.set_device_filters('worker', i, ['/job:ps'])
for i in range(num_ps):
  cdf.set_device_filters('ps', i, ['/job:worker'])

tf.config.experimental_connect_to_cluster(cluster_def,
                                          cluster_device_filters=cdf)
"
"tf.config.experimental_connect_to_host(
    remote_host=None, job_name='worker'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import experimental_connect_to_host
tf.config.experimental_connect_to_host(""exampleaddr.com:9876"")

with ops.device(""job:worker/replica:0/task:1/device:CPU:0""):
  x1 = array_ops.ones([2, 2])
  x2 = array_ops.ones([2, 2])
  y = math_ops.matmul(x1, x2)
"
"tf.config.get_logical_device_configuration(
    device
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get_logical_device_configuration
physical_devices = tf.config.list_physical_devices('CPU')
assert len(physical_devices) == 1, ""No CPUs found""
configs = tf.config.get_logical_device_configuration(
  physical_devices[0])
try:
  assert configs is None
  tf.config.set_logical_device_configuration(
    physical_devices[0],
    [tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration()])
  configs = tf.config.get_logical_device_configuration(
    physical_devices[0])
  assert len(configs) == 2
except:
  pass"
"tf.config.get_visible_devices(
    device_type=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get_visible_devices
physical_devices = tf.config.list_physical_devices('GPU')
try:
  tf.config.set_visible_devices([], 'GPU')
  visible_devices = tf.config.get_visible_devices()
  for device in visible_devices:
    assert device.device_type != 'GPU'
except:
  pass"
"tf.config.list_logical_devices(
    device_type=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import list_logical_devices
logical_devices = tf.config.list_logical_devices('GPU')
if len(logical_devices) > 0:
  with tf.device(logical_devices[0].name):
    one = tf.constant(1)
  with tf.device(logical_devices[1].name):
    two = tf.constant(2)"
"tf.config.list_physical_devices(
    device_type=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import list_physical_devices
physical_devices = tf.config.list_physical_devices('GPU')
print(""Num GPUs:"", len(physical_devices))
Num GPUs: ..."
"tf.config.set_logical_device_configuration(
    device, logical_devices
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import set_logical_device_configuration
physical_devices = tf.config.list_physical_devices('CPU')
assert len(physical_devices) == 1, ""No CPUs found""
try:
  tf.config.set_logical_device_configuration(
    physical_devices[0],
    [tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration()])
  logical_devices = tf.config.list_logical_devices('CPU')
  assert len(logical_devices) == 2
  tf.config.set_logical_device_configuration(
    physical_devices[0],
    [tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration()])
except:
  pass"
"tf.config.set_visible_devices(
    devices, device_type=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import set_visible_devices
physical_devices = tf.config.list_physical_devices('GPU')
try:
  tf.config.set_visible_devices(physical_devices[1:], 'GPU')
  logical_devices = tf.config.list_logical_devices('GPU')
  assert len(logical_devices) == len(physical_devices) - 1
except:
  pass"
"tf.constant(
    value, dtype=None, shape=None, name='Const'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import constant
tf.constant([1, 2, 3, 4, 5, 6])
<tf.Tensor: shape=(6,), dtype=int32,
<tf.Tensor: shape=(6,), dtype=int32,
    numpy=array([1, 2, 3, 4, 5, 6], dtype=int32)>
a = np.array([[1, 2, 3], [4, 5, 6]])
tf.constant(a)
<tf.Tensor: shape=(2, 3), dtype=int64, numpy=
<tf.Tensor: shape=(2, 3), dtype=int64, numpy=
  array([[1, 2, 3],
         [4, 5, 6]])>"
"tf.constant_initializer(
    value=0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import constant_initializer
def make_variables(k, initializer):
  return (tf.Variable(initializer(shape=[k], dtype=tf.float32)),
          tf.Variable(initializer(shape=[k, k], dtype=tf.float32)))
v1, v2 = make_variables(3, tf.constant_initializer(2.))
v1
<tf.Variable ... shape=(3,) ... numpy=array([2., 2., 2.], dtype=float32)>
<tf.Variable ... shape=(3,) ... numpy=array([2., 2., 2.], dtype=float32)>
v2
<tf.Variable ... shape=(3, 3) ... numpy=
<tf.Variable ... shape=(3, 3) ... numpy=
array([[2., 2., 2.],
       [2., 2., 2.],
       [2., 2., 2.]], dtype=float32)>
make_variables(4, tf.random_uniform_initializer(minval=-1., maxval=1.))
(<tf.Variable...shape=(4,) dtype=float32...>, <tf.Variable...shape=(4, 4) ...
(<tf.Variable...shape=(4,) dtype=float32...>, <tf.Variable...shape=(4, 4) ..."
"tf.convert_to_tensor(
    value, dtype=None, dtype_hint=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import convert_to_tensor
import numpy as np
def my_func(arg):
  arg = tf.convert_to_tensor(arg, dtype=tf.float32)
  return arg"
"tf.math.cos(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import cos
  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
  tf.math.cos(x) ==> [nan -0.91113025 0.87758255 0.5403023 0.36235774 0.48718765 -0.95215535 nan]
"
"tf.math.cosh(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import cosh
  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 2, 10, float(""inf"")])
  tf.math.cosh(x) ==> [inf 4.0515420e+03 1.1276259e+00 1.5430807e+00 1.8106556e+00 3.7621956e+00 1.1013233e+04 inf]
"
"tf.math.cumsum(
    x, axis=0, exclusive=False, reverse=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import cumsum
x = tf.constant([2, 4, 6, 8])
tf.cumsum(x)
<tf.Tensor: shape=(4,), dtype=int32,
<tf.Tensor: shape=(4,), dtype=int32,
numpy=array([ 2,  6, 12, 20], dtype=int32)>"
"tf.data.Dataset(
    variant_tensor
)
","import pandas as pd
import numpy as np
import tensorflow as tf
dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
for element in dataset:
  print(element)
tf.Tensor(1, shape=(), dtype=int32)
tf.Tensor(2, shape=(), dtype=int32)
tf.Tensor(3, shape=(), dtype=int32)"
"tf.data.DatasetSpec(
    element_spec, dataset_shape=()
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import DatasetSpec
dataset = tf.data.Dataset.range(3)
tf.data.DatasetSpec.from_value(dataset)
DatasetSpec(TensorSpec(shape=(), dtype=tf.int64, name=None), TensorShape([]))"
"tf.data.IteratorSpec(
    element_spec
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import IteratorSpec
@tf.function(input_signature=[tf.data.IteratorSpec(
  tf.TensorSpec(shape=(), dtype=tf.int32, name=None))])
def square(iterator):
  x = iterator.get_next()
  return x * x
dataset = tf.data.Dataset.from_tensors(5)
iterator = iter(dataset)
print(square(iterator))
tf.Tensor(25, shape=(), dtype=int32)"
"tf.data.experimental.CheckpointInputPipelineHook(
    estimator, external_state_policy=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CheckpointInputPipelineHook
est = tf.estimator.Estimator(model_fn)
while True:
  est.train(
      train_input_fn,
      hooks=[tf.data.experimental.CheckpointInputPipelineHook(est)],
      steps=train_steps_per_eval)
  metrics = est.evaluate(eval_input_fn)
  if should_stop_the_training(metrics):
    break
"
"tf.data.experimental.Counter(
    start=0,
    step=1,
    dtype=tf.dtypes.int64
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Counter
dataset = tf.data.experimental.Counter().take(5)
list(dataset.as_numpy_iterator())
[0, 1, 2, 3, 4]
dataset.element_spec
TensorSpec(shape=(), dtype=tf.int64, name=None)
dataset = tf.data.experimental.Counter(dtype=tf.int32)
dataset.element_spec
TensorSpec(shape=(), dtype=tf.int32, name=None)
dataset = tf.data.experimental.Counter(start=2).take(5)
list(dataset.as_numpy_iterator())
[2, 3, 4, 5, 6]
dataset = tf.data.experimental.Counter(start=2, step=5).take(5)
list(dataset.as_numpy_iterator())
[2, 7, 12, 17, 22]
dataset = tf.data.experimental.Counter(start=10, step=-1).take(5)
list(dataset.as_numpy_iterator())
[10, 9, 8, 7, 6]"
"tf.data.experimental.DatasetInitializer(
    dataset
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import DatasetInitializer
keys = tf.data.Dataset.range(100)
values = tf.data.Dataset.range(100).map(
    lambda x: tf.strings.as_string(x * 2))
ds = tf.data.Dataset.zip((keys, values))
init = tf.data.experimental.DatasetInitializer(ds)
table = tf.lookup.StaticHashTable(init, """")
table.lookup(tf.constant([0, 1, 2], dtype=tf.int64)).numpy()
array([b'0', b'2', b'4'], dtype=object)"
"tf.data.experimental.Reducer(
    init_func, reduce_func, finalize_func
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Reducer
def init_func(_):
  return (0.0, 0.0)

def reduce_func(state, value):
  return (state[0] + value['features'], state[1] + 1)

def finalize_func(s, n):
  return s / n

reducer = tf.data.experimental.Reducer(init_func, reduce_func, finalize_func)
"
"tf.data.experimental.SqlDataset(
    driver_name, data_source_name, query, output_types
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SqlDataset
dataset = tf.data.experimental.SqlDataset(""sqlite"", ""/foo/bar.sqlite3"",
                                          ""SELECT name, age FROM people"",
                                          (tf.string, tf.int32))
for element in dataset:
  print(element)
"
"tf.data.experimental.TFRecordWriter(
    filename, compression_type=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TFRecordWriter
dataset = tf.data.Dataset.range(3)
dataset = dataset.map(tf.io.serialize_tensor)
writer = tf.data.experimental.TFRecordWriter(""/path/to/file.tfrecord"")
writer.write(dataset)
"
"tf.data.experimental.assert_cardinality(
    expected_cardinality
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import assert_cardinality
dataset = tf.data.TFRecordDataset(""examples.tfrecord"")
cardinality = tf.data.experimental.cardinality(dataset)
print((cardinality == tf.data.experimental.UNKNOWN_CARDINALITY).numpy())
True
dataset = dataset.apply(tf.data.experimental.assert_cardinality(42))
print(tf.data.experimental.cardinality(dataset).numpy())
42"
"tf.data.experimental.cardinality(
    dataset
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import cardinality
dataset = tf.data.Dataset.range(42)
print(tf.data.experimental.cardinality(dataset).numpy())
42
dataset = dataset.repeat()
cardinality = tf.data.experimental.cardinality(dataset)
print((cardinality == tf.data.experimental.INFINITE_CARDINALITY).numpy())
True
dataset = dataset.filter(lambda x: True)
cardinality = tf.data.experimental.cardinality(dataset)
print((cardinality == tf.data.experimental.UNKNOWN_CARDINALITY).numpy())
True"
"tf.data.experimental.choose_from_datasets(
    datasets, choice_dataset, stop_on_empty_dataset=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import choose_from_datasets
datasets = [tf.data.Dataset.from_tensors(""foo"").repeat(),
            tf.data.Dataset.from_tensors(""bar"").repeat(),
            tf.data.Dataset.from_tensors(""baz"").repeat()]

choice_dataset = tf.data.Dataset.range(3).repeat(3)

result = tf.data.experimental.choose_from_datasets(datasets, choice_dataset)
"
"tf.data.experimental.dense_to_ragged_batch(
    batch_size,
    drop_remainder=False,
    row_splits_dtype=tf.dtypes.int64
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import dense_to_ragged_batch
dataset = tf.data.Dataset.from_tensor_slices(np.arange(6))
dataset = dataset.map(lambda x: tf.range(x))
dataset.element_spec.shape
TensorShape([None])
dataset = dataset.apply(
    tf.data.experimental.dense_to_ragged_batch(batch_size=2))
for batch in dataset:
  print(batch)
<tf.RaggedTensor [[], [0]]>
<tf.RaggedTensor [[], [0]]>
<tf.RaggedTensor [[0, 1], [0, 1, 2]]>
<tf.RaggedTensor [[0, 1], [0, 1, 2]]>
<tf.RaggedTensor [[0, 1, 2, 3], [0, 1, 2, 3, 4]]>
<tf.RaggedTensor [[0, 1, 2, 3], [0, 1, 2, 3, 4]]>"
"tf.data.experimental.dense_to_sparse_batch(
    batch_size, row_shape
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import dense_to_sparse_batch
a = { ['a', 'b', 'c'], ['a', 'b'], ['a', 'b', 'c', 'd'] }

a.apply(tf.data.experimental.dense_to_sparse_batch(
    batch_size=2, row_shape=[6])) ==
{
    ([[0, 0], [0, 1], [0, 2], [0, 3]],
     ['a', 'b', 'c', 'd'],
     [1, 6])
}
"
"tf.data.experimental.enumerate_dataset(
    start=0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import enumerate_dataset
a = { 1, 2, 3 }
b = { (7, 8), (9, 10) }

a.apply(tf.data.experimental.enumerate_dataset(start=5))
=> { (5, 1), (6, 2), (7, 3) }
b.apply(tf.data.experimental.enumerate_dataset())
=> { (0, (7, 8)), (1, (9, 10)) }
"
"tf.data.experimental.from_list(
    elements, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import from_list
dataset = tf.data.experimental.from_list([(1, 'a'), (2, 'b'), (3, 'c')])
list(dataset.as_numpy_iterator())
[(1, b'a'), (2, b'b'), (3, b'c')]"
"tf.data.experimental.get_single_element(
    dataset
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get_single_element
def preprocessing_fn(raw_feature):
  return feature

dataset = (tf.data.Dataset.from_tensor_slices(raw_features)
           .map(preprocessing_fn, num_parallel_calls=BATCH_SIZE)
           .batch(BATCH_SIZE))

processed_features = tf.data.experimental.get_single_element(dataset)
"
"tf.data.experimental.get_structure(
    dataset_or_iterator
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get_structure
dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
tf.data.experimental.get_structure(dataset)
TensorSpec(shape=(), dtype=tf.int32, name=None)"
"tf.data.experimental.ignore_errors(
    log_warning=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ignore_errors
dataset = tf.data.Dataset.from_tensor_slices([1., 2., 0., 4.])

InvalidArgumentError.
dataset = dataset.map(lambda x: tf.debugging.check_numerics(1. / x, ""error""))

dataset =
"
"tf.data.experimental.index_table_from_dataset(
    dataset=None,
    num_oov_buckets=0,
    vocab_size=None,
    default_value=-1,
    hasher_spec=lookup_ops.FastHashSpec,
    key_dtype=tf.dtypes.string,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import index_table_from_dataset
ds = tf.data.Dataset.range(100).map(lambda x: tf.strings.as_string(x * 2))
table = tf.data.experimental.index_table_from_dataset(
                                    ds, key_dtype=dtypes.int64)
table.lookup(tf.constant(['0', '2', '4'], dtype=tf.string)).numpy()
array([0, 1, 2])"
"tf.data.experimental.load(
    path, element_spec=None, compression=None, reader_func=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import load
import tempfile
path = os.path.join(tempfile.gettempdir(), ""saved_data"")
dataset = tf.data.Dataset.range(2)
tf.data.experimental.save(dataset, path)
new_dataset = tf.data.experimental.load(path)
for elem in new_dataset:
  print(elem)
tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor(1, shape=(), dtype=int64)"
"tf.data.experimental.make_csv_dataset(
    file_pattern,
    batch_size,
    column_names=None,
    column_defaults=None,
    label_name=None,
    select_columns=None,
    field_delim=',',
    use_quote_delim=True,
    na_value='',
    header=True,
    num_epochs=None,
    shuffle=True,
    shuffle_buffer_size=10000,
    shuffle_seed=None,
    prefetch_buffer_size=None,
    num_parallel_reads=None,
    sloppy=False,
    num_rows_for_inference=100,
    compression_type=None,
    ignore_errors=False,
    encoding='utf-8'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import make_csv_dataset
dataset = tf.data.experimental.make_csv_dataset(filename, batch_size=2)
iterator = dataset.as_numpy_iterator()
print(dict(next(iterator)))
"
"tf.data.experimental.make_saveable_from_iterator(
    iterator, external_state_policy=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import make_saveable_from_iterator
with tf.Graph().as_default():
  ds = tf.data.Dataset.range(10)
  iterator = ds.make_initializable_iterator()
  saveable_obj = tf.data.experimental.make_saveable_from_iterator(iterator)
  tf.compat.v1.add_to_collection(tf.GraphKeys.SAVEABLE_OBJECTS, saveable_obj)
  saver = tf.compat.v1.train.Saver()

  while continue_training:
    ... Perform training ...
    if should_save_checkpoint:
      saver.save()
"
"tf.data.experimental.parallel_interleave(
    map_func,
    cycle_length,
    block_length=1,
    sloppy=False,
    buffer_output_elements=None,
    prefetch_input_elements=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import parallel_interleave
filenames = tf.data.Dataset.list_files(""/path/to/data/train*.tfrecords"")
dataset = filenames.apply(
    tf.data.experimental.parallel_interleave(
        lambda filename: tf.data.TFRecordDataset(filename),
        cycle_length=4))
"
"tf.data.experimental.prefetch_to_device(
    device, buffer_size=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import prefetch_to_device
>>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
>>> dataset = dataset.apply(tf.data.experimental.prefetch_to_device(""/cpu:0""))
>>> for element in dataset:
...   print(f'Tensor {element} is on device {element.device}')
Tensor 1 is on device /job:localhost/replica:0/task:0/device:CPU:0
Tensor 2 is on device /job:localhost/replica:0/task:0/device:CPU:0
Tensor 3 is on device /job:localhost/replica:0/task:0/device:CPU:0
"
"tf.data.experimental.save(
    dataset, path, compression=None, shard_func=None, checkpoint_args=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import save
import tempfile
path = os.path.join(tempfile.gettempdir(), ""saved_data"")
dataset = tf.data.Dataset.range(2)
tf.data.experimental.save(dataset, path)
new_dataset = tf.data.experimental.load(path)
for elem in new_dataset:
  print(elem)
tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor(1, shape=(), dtype=int64)"
"tf.data.experimental.service.CrossTrainerCache(
    trainer_id
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CrossTrainerCache
dataset = dataset.apply(tf.data.experimental.service.distribute(
    processing_mode=tf.data.experimental.service.ShardingPolicy.OFF,
    service=FLAGS.tf_data_service_address,
    job_name=""job"",
    cross_trainer_cache=data_service_ops.CrossTrainerCache(
        trainer_id=trainer_id())))
"
"tf.data.experimental.service.DispatchServer(
    config=None, start=True
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import DispatchServer
dispatcher = tf.data.experimental.service.DispatchServer()
dispatcher_address = dispatcher.target.split(""://"")[1]
worker = tf.data.experimental.service.WorkerServer(
    tf.data.experimental.service.WorkerConfig(
    dispatcher_address=dispatcher_address))
dataset = tf.data.Dataset.range(10)
dataset = dataset.apply(tf.data.experimental.service.distribute(
    processing_mode=""parallel_epochs"", service=dispatcher.target))
print(list(dataset.as_numpy_iterator()))
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
"tf.data.experimental.service.WorkerServer(
    config, start=True
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import WorkerServer
dispatcher = tf.data.experimental.service.DispatchServer()
dispatcher_address = dispatcher.target.split(""://"")[1]
worker = tf.data.experimental.service.WorkerServer(
    tf.data.experimental.service.WorkerConfig(
        dispatcher_address=dispatcher_address))
dataset = tf.data.Dataset.range(10)
dataset = dataset.apply(tf.data.experimental.service.distribute(
    processing_mode=""parallel_epochs"", service=dispatcher.target))
print(list(dataset.as_numpy_iterator()))
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
"tf.data.experimental.service.distribute(
    processing_mode,
    service,
    job_name=None,
    consumer_index=None,
    num_consumers=None,
    max_outstanding_requests=None,
    data_transfer_protocol=None,
    compression='AUTO',
    cross_trainer_cache=None,
    target_workers='AUTO'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import distribute
dispatcher = tf.data.experimental.service.DispatchServer()
dispatcher_address = dispatcher.target.split(""://"")[1]
workers = [
    tf.data.experimental.service.WorkerServer(
        tf.data.experimental.service.WorkerConfig(
            dispatcher_address=dispatcher_address)) for _ in range(2)
]
dataset = tf.data.Dataset.range(10)
dataset = dataset.apply(tf.data.experimental.service.distribute(
    processing_mode=""parallel_epochs"", service=dispatcher.target))
print(sorted(list(dataset.as_numpy_iterator())))
[0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9]"
"tf.data.experimental.service.from_dataset_id(
    processing_mode,
    service,
    dataset_id,
    element_spec=None,
    job_name=None,
    consumer_index=None,
    num_consumers=None,
    max_outstanding_requests=None,
    data_transfer_protocol=None,
    cross_trainer_cache=None,
    target_workers='AUTO'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import from_dataset_id
dispatcher = tf.data.experimental.service.DispatchServer()
dispatcher_address = dispatcher.target.split(""://"")[1]
worker = tf.data.experimental.service.WorkerServer(
    tf.data.experimental.service.WorkerConfig(
        dispatcher_address=dispatcher_address))
dataset = tf.data.Dataset.range(10)
dataset_id = tf.data.experimental.service.register_dataset(
    dispatcher.target, dataset)
dataset = tf.data.experimental.service.from_dataset_id(
    processing_mode=""parallel_epochs"",
    service=dispatcher.target,
    dataset_id=dataset_id,
    element_spec=dataset.element_spec)
print(list(dataset.as_numpy_iterator()))
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
"tf.data.experimental.service.register_dataset(
    service, dataset, compression='AUTO', dataset_id=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import register_dataset
dispatcher = tf.data.experimental.service.DispatchServer()
dispatcher_address = dispatcher.target.split(""://"")[1]
worker = tf.data.experimental.service.WorkerServer(
    tf.data.experimental.service.WorkerConfig(
        dispatcher_address=dispatcher_address))
dataset = tf.data.Dataset.range(10)
dataset_id = tf.data.experimental.service.register_dataset(
    dispatcher.target, dataset)
dataset = tf.data.experimental.service.from_dataset_id(
    processing_mode=""parallel_epochs"",
    service=dispatcher.target,
    dataset_id=dataset_id,
    element_spec=dataset.element_spec)
print(list(dataset.as_numpy_iterator()))
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
"tf.data.experimental.shuffle_and_repeat(
    buffer_size, count=None, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import shuffle_and_repeat
d = tf.data.Dataset.from_tensor_slices([1, 2, 3])
d = d.apply(tf.data.experimental.shuffle_and_repeat(2, count=2))
[2, 3, 1, 1, 3, 2]"
"tf.data.experimental.snapshot(
    path, compression='AUTO', reader_func=None, shard_func=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import snapshot
dataset = ...
dataset = dataset.enumerate()
dataset = dataset.apply(tf.data.experimental.snapshot(""/path/to/snapshot/dir"",
    shard_func=lambda x, y: x % NUM_SHARDS, ...))
dataset = dataset.map(lambda x, y: y)
"
"tf.data.experimental.table_from_dataset(
    dataset=None,
    num_oov_buckets=0,
    vocab_size=None,
    default_value=None,
    hasher_spec=lookup_ops.FastHashSpec,
    key_dtype=tf.dtypes.string,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import table_from_dataset
keys = tf.data.Dataset.range(100)
values = tf.data.Dataset.range(100).map(
    lambda x: tf.strings.as_string(x * 2))
ds = tf.data.Dataset.zip((keys, values))
table = tf.data.experimental.table_from_dataset(
                              ds, default_value='n/a', key_dtype=tf.int64)
table.lookup(tf.constant([0, 1, 2], dtype=tf.int64)).numpy()
array([b'0', b'2', b'4'], dtype=object)"
"tf.debugging.Assert(
    condition, data, summarize=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Assert
assert_op = tf.Assert(tf.less_equal(tf.reduce_max(x), 1.), [x])
with tf.control_dependencies([assert_op]):
  ... code using x ...
"
"tf.debugging.assert_shapes(
    shapes, data=None, summarize=None, message=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import assert_shapes
n = 10
q = 3
d = 7
x = tf.zeros([n,q])
y = tf.ones([n,d])
param = tf.Variable([1.0, 2.0, 3.0])
scalar = 1.0
tf.debugging.assert_shapes([
 (x, ('N', 'Q')),
 (y, ('N', 'D')),
 (param, ('Q',)),
 (scalar, ()),
])"
"tf.debugging.assert_type(
    tensor, tf_type, message=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import assert_type
a = tf.Variable(1.0)
tf.debugging.assert_type(a, tf_type= tf.float32)"
"tf.debugging.check_numerics(
    tensor, message, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import check_numerics
a = tf.Variable(1.0)
tf.debugging.check_numerics(a, message='')

b = tf.Variable(np.nan)
try:
  tf.debugging.check_numerics(b, message='Checking b')
except Exception as e:
  assert ""Checking b : Tensor had NaN values"" in e.message

c = tf.Variable(np.inf)
try:
  tf.debugging.check_numerics(c, message='Checking c')
except Exception as e:
  assert ""Checking c : Tensor had Inf values"" in e.message
"
"tf.debugging.enable_check_numerics(
    stack_height_limit=30, path_length_limit=50
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import enable_check_numerics
tf.config.set_soft_device_placement(True)
tf.debugging.enable_check_numerics()

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
strategy = tf.distribute.TPUStrategy(resolver)
with strategy.scope():
"
"tf.debugging.experimental.enable_dump_debug_info(
    dump_root,
    tensor_debug_mode=DEFAULT_TENSOR_DEBUG_MODE,
    circular_buffer_size=1000,
    op_regex=None,
    tensor_dtypes=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import enable_dump_debug_info
tf.debugging.experimental.enable_dump_debug_info('/tmp/my-tfdbg-dumps')

"
"tf.debugging.set_log_device_placement(
    enabled
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import set_log_device_placement
tf.debugging.set_log_device_placement(True)
tf.ones([])
with tf.device(""CPU""):
 tf.ones([])
tf.debugging.set_log_device_placement(False)"
"tf.device(
    device_name
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import device
with tf.device('/job:foo'):
  with tf.device('/job:bar/task:0/device:gpu:2'):
  with tf.device('/device:gpu:1'):
"
"tf.distribute.HierarchicalCopyAllReduce(
    num_packs=1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import HierarchicalCopyAllReduce
  strategy = tf.distribute.MirroredStrategy(
    cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())
"
"tf.distribute.InputOptions(
    experimental_fetch_to_device=None,
    experimental_replication_mode=tf.distribute.InputReplicationMode.PER_WORKER,
    experimental_place_dataset_on_device=False,
    experimental_per_replica_buffer_size=1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import InputOptions
resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.TPUStrategy(resolver)

dataset = tf.data.Dataset.range(16)
distributed_dataset_on_host = (
    strategy.experimental_distribute_dataset(
        dataset,
        tf.distribute.InputOptions(
            experimental_replication_mode=
            experimental_replication_mode.PER_WORKER,
            experimental_place_dataset_on_device=False,
            experimental_per_replica_buffer_size=1)))
"
"tf.distribute.MirroredStrategy(
    devices=None, cross_device_ops=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MirroredStrategy
strategy = tf.distribute.MirroredStrategy([""GPU:0"", ""GPU:1""])
with strategy.scope():
  x = tf.Variable(1.)
x
MirroredVariable:{
  0: <tf.Variable ... shape=() dtype=float32, numpy=1.0>,
  0: <tf.Variable ... shape=() dtype=float32, numpy=1.0>,
  1: <tf.Variable ... shape=() dtype=float32, numpy=1.0>
  1: <tf.Variable ... shape=() dtype=float32, numpy=1.0>
}"
"tf.distribute.NcclAllReduce(
    num_packs=1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import NcclAllReduce
  strategy = tf.distribute.MirroredStrategy(
    cross_device_ops=tf.distribute.NcclAllReduce())
"
"tf.distribute.OneDeviceStrategy(
    device
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import OneDeviceStrategy
strategy = tf.distribute.OneDeviceStrategy(device=""/gpu:0"")

with strategy.scope():
  v = tf.Variable(1.0)

def step_fn(x):
  return x * 2

result = 0
for i in range(10):
  result += strategy.run(step_fn, args=(i,))
"
"tf.distribute.experimental.ParameterServerStrategy(
    cluster_resolver, variable_partitioner=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ParameterServerStrategy
strategy = tf.distribute.experimental.ParameterServerStrategy(
    cluster_resolver=...,
    variable_partitioner=...)
coordinator = tf.distribute.experimental.coordinator.ClusterCoordinator(
    strategy=strategy)

distributed_dataset = coordinator.create_per_worker_dataset(dataset_fn=...)

with strategy.scope():
  model = ...
  checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)
  checkpoint_manager = tf.train.CheckpointManager(
      checkpoint, checkpoint_dir, max_to_keep=2)
  initial_epoch = load_checkpoint(checkpoint_manager) or 0

@tf.function
def worker_fn(iterator):

  def replica_fn(inputs):
    batch_data, labels = inputs

  strategy.run(replica_fn, args=(next(iterator),))

for epoch in range(initial_epoch, num_epoch):
  for step in range(steps_per_epoch):

    coordinator.schedule(worker_fn, args=(distributed_iterator,))

  coordinator.join()
  logging.info('Metric result: %r', metrics.result())
  train_accuracy.reset_states()
  checkpoint_manager.save()
"
"tf.distribute.ReductionToOneDevice(
    reduce_to_device=None, accumulation_fn=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ReductionToOneDevice
  strategy = tf.distribute.MirroredStrategy(
    cross_device_ops=tf.distribute.ReductionToOneDevice())
"
"tf.distribute.Server(
    server_or_cluster_def,
    job_name=None,
    task_index=None,
    protocol=None,
    config=None,
    start=True
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Server
server = tf.distribute.Server(...)
with tf.compat.v1.Session(server.target):
"
"tf.distribute.TPUStrategy(
    tpu_cluster_resolver=None,
    experimental_device_assignment=None,
    experimental_spmd_xla_partitioning=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TPUStrategy
resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.TPUStrategy(resolver)"
"tf.distribute.cluster_resolver.GCEClusterResolver(
    project,
    zone,
    instance_group,
    port,
    task_type='worker',
    task_id=0,
    rpc_layer='grpc',
    credentials='default',
    service=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GCEClusterResolver
  cluster_resolver = GCEClusterResolver(""my-project"", ""us-west1"",
                                        ""my-instance-group"",
                                        task_type=""worker"", task_id=0)
  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(
      cluster_resolver=cluster_resolver)

  cluster_resolver = GCEClusterResolver(""my-project"", ""us-west1"",
                                        ""my-instance-group"",
                                        task_type=""worker"", task_id=1)
  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(
      cluster_resolver=cluster_resolver)
"
"tf.distribute.cluster_resolver.KubernetesClusterResolver(
    job_to_label_mapping=None,
    tf_server_port=8470,
    rpc_layer='grpc',
    override_client=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import KubernetesClusterResolver
  cluster_resolver = KubernetesClusterResolver(
      {""worker"": [""job-name=worker-cluster-a"", ""job-name=worker-cluster-b""]})
  cluster_resolver.task_type = ""worker""
  cluster_resolver.task_id = 0
  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(
      cluster_resolver=cluster_resolver)

  cluster_resolver = KubernetesClusterResolver(
      {""worker"": [""job-name=worker-cluster-a"", ""job-name=worker-cluster-b""]})
  cluster_resolver.task_type = ""worker""
  cluster_resolver.task_id = 1
  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(
      cluster_resolver=cluster_resolver)
"
"tf.distribute.cluster_resolver.SimpleClusterResolver(
    cluster_spec,
    master='',
    task_type=None,
    task_id=None,
    environment='',
    num_accelerators=None,
    rpc_layer=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SimpleClusterResolver
  cluster = tf.train.ClusterSpec({""worker"": [""worker0.example.com:2222"",
                                             ""worker1.example.com:2222""]})

  cluster_resolver = SimpleClusterResolver(cluster, task_type=""worker"",
                                           task_id=0,
                                           num_accelerators={""GPU"": 8},
                                           rpc_layer=""grpc"")
  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(
      cluster_resolver=cluster_resolver)

  cluster_resolver = SimpleClusterResolver(cluster, task_type=""worker"",
                                           task_id=1,
                                           num_accelerators={""GPU"": 8},
                                           rpc_layer=""grpc"")
  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(
      cluster_resolver=cluster_resolver)
"
"tf.distribute.experimental.CentralStorageStrategy(
    compute_devices=None, parameter_device=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CentralStorageStrategy
strategy = tf.distribute.experimental.CentralStorageStrategy()
ds = tf.data.Dataset.range(5).batch(2)
dist_dataset = strategy.experimental_distribute_dataset(ds)

with strategy.scope():
  @tf.function
  def train_step(val):
    return val + 1

  for x in dist_dataset:
    strategy.run(train_step, args=(x,))
"
"tf.distribute.experimental.CollectiveHints(
    bytes_per_pack=0, timeout_seconds=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CollectiveHints
hints = tf.distribute.experimental.CollectiveHints(
    bytes_per_pack=50 * 1024 * 1024)
grads = tf.distribute.get_replica_context().all_reduce(
    'sum', grads, experimental_hints=hints)
optimizer.apply_gradients(zip(grads, vars),
    experimental_aggregate_gradients=False)
"
"tf.distribute.experimental.CommunicationOptions(
    bytes_per_pack=0,
    timeout_seconds=None,
    implementation=tf.distribute.experimental.CollectiveCommunication.AUTO
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CommunicationOptions
options = tf.distribute.experimental.CommunicationOptions(
    bytes_per_pack=50 * 1024 * 1024,
    timeout_seconds=120.0,
    implementation=tf.distribute.experimental.CommunicationImplementation.NCCL
)
grads = tf.distribute.get_replica_context().all_reduce(
    'sum', grads, options=options)
optimizer.apply_gradients(zip(grads, vars),
    experimental_aggregate_gradients=False)
"
"tf.distribute.experimental.ParameterServerStrategy(
    cluster_resolver, variable_partitioner=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ParameterServerStrategy
strategy = tf.distribute.experimental.ParameterServerStrategy(
    cluster_resolver=...,
    variable_partitioner=...)
coordinator = tf.distribute.experimental.coordinator.ClusterCoordinator(
    strategy=strategy)

distributed_dataset = coordinator.create_per_worker_dataset(dataset_fn=...)

with strategy.scope():
  model = ...
  checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)
  checkpoint_manager = tf.train.CheckpointManager(
      checkpoint, checkpoint_dir, max_to_keep=2)
  initial_epoch = load_checkpoint(checkpoint_manager) or 0

@tf.function
def worker_fn(iterator):

  def replica_fn(inputs):
    batch_data, labels = inputs

  strategy.run(replica_fn, args=(next(iterator),))

for epoch in range(initial_epoch, num_epoch):
  for step in range(steps_per_epoch):

    coordinator.schedule(worker_fn, args=(distributed_iterator,))

  coordinator.join()
  logging.info('Metric result: %r', metrics.result())
  train_accuracy.reset_states()
  checkpoint_manager.save()
"
"tf.distribute.experimental.PreemptionCheckpointHandler(
    cluster_resolver,
    checkpoint_or_checkpoint_manager,
    checkpoint_dir=None,
    termination_config=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import PreemptionCheckpointHandler
strategy = tf.distribute.MultiWorkerMirroredStrategy()

with strategy.scope():
  dataset, model, optimizer = ...

  checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)

  preemption_handler = tf.distribute.experimental.PreemptionCheckpointHandler(cluster_resolver, checkpoint, checkpoint_directory)

  for epoch in range(preemption_handler.total_run_calls // STEPS_PER_EPOCH, num_epochs):
    for step in range(preemption_handler.total_run_calls % STEPS_PER_EPOCH, STEPS_PER_EPOCH):
      loss += preemption_handler.run(distributed_train_step, args=(next(dataset),))
"
"tf.distribute.experimental.TPUStrategy(
    tpu_cluster_resolver=None, device_assignment=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TPUStrategy
resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.experimental.TPUStrategy(resolver)"
"tf.distribute.experimental.partitioners.FixedShardsPartitioner(
    num_shards
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import FixedShardsPartitioner
partitioner = FixedShardsPartitioner(num_shards=2)
partitions = partitioner(tf.TensorShape([10, 3]), tf.float32)
[2, 1]"
"tf.distribute.experimental.partitioners.MaxSizePartitioner(
    max_shard_bytes, max_shards=None, bytes_per_string=16
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MaxSizePartitioner
partitioner = MaxSizePartitioner(max_shard_bytes=4)
partitions = partitioner(tf.TensorShape([6, 1]), tf.float32)
[6, 1]
partitioner = MaxSizePartitioner(max_shard_bytes=4, max_shards=2)
partitions = partitioner(tf.TensorShape([6, 1]), tf.float32)
[2, 1]
partitioner = MaxSizePartitioner(max_shard_bytes=1024)
partitions = partitioner(tf.TensorShape([6, 1]), tf.float32)
[1, 1]"
"tf.distribute.experimental.partitioners.MinSizePartitioner(
    min_shard_bytes=(256 << 10), max_shards=1, bytes_per_string=16
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MinSizePartitioner
partitioner = MinSizePartitioner(min_shard_bytes=4, max_shards=2)
partitions = partitioner(tf.TensorShape([6, 1]), tf.float32)
[2, 1]
partitioner = MinSizePartitioner(min_shard_bytes=4, max_shards=10)
partitions = partitioner(tf.TensorShape([6, 1]), tf.float32)
[6, 1]"
"tf.distribute.experimental_set_strategy(
    strategy
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import experimental_set_strategy
tf.distribute.experimental_set_strategy(strategy1)
f()
tf.distribute.experimental_set_strategy(strategy2)
g()
tf.distribute.experimental_set_strategy(None)
h()
"
"tf.math.divide(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import divide
x = tf.constant([16, 12, 11])
y = tf.constant([4, 6, 2])
tf.divide(x,y)
<tf.Tensor: shape=(3,), dtype=float64,
<tf.Tensor: shape=(3,), dtype=float64,
numpy=array([4. , 2. , 5.5])>"
"tf.cast(
    x, dtype, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import cast
x = tf.constant([1.8, 2.2], dtype=tf.float32)
tf.cast(x, tf.int32)
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>"
"tf.dtypes.complex(
    real, imag, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import complex
real = tf.constant([2.25, 3.25])
imag = tf.constant([4.75, 5.75])
"
"tf.edit_distance(
    hypothesis, truth, normalize=True, name='edit_distance'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import edit_distance
hypothesis = tf.SparseTensor(
  [[0, 0, 0],
   [1, 0, 0]],
  [""a"", ""b""],
  (2, 1, 1))
truth = tf.SparseTensor(
  [[0, 1, 0],
   [1, 0, 0],
   [1, 0, 1],
   [1, 1, 0]],
   [""a"", ""b"", ""c"", ""a""],
   (2, 2, 2))
tf.edit_distance(hypothesis, truth, normalize=True)
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[inf, 1. ],
       [0.5, 1. ]], dtype=float32)>"
"tf.ensure_shape(
    x, shape, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ensure_shape
x = tf.constant([[1, 2, 3],
                 [4, 5, 6]])
x = tf.ensure_shape(x, [2, 3])"
"tf.math.equal(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import equal
x = tf.constant([2, 4])
y = tf.constant(2)
tf.math.equal(x, y)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  False])>
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  False])>"
"tf.errors.InvalidArgumentError(
    node_def, op, message, *args
)
","import pandas as pd
import numpy as np
import tensorflow as tf
tf.reshape([1, 2, 3], (2,))
Traceback (most recent call last):
InvalidArgumentError: ..."
"tf.estimator.BaselineClassifier(
    model_dir=None,
    n_classes=2,
    weight_column=None,
    label_vocabulary=None,
    optimizer='Ftrl',
    config=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import BaselineClassifier

classifier = tf.estimator.BaselineClassifier(n_classes=3)

def input_fn_train:
  pass

def input_fn_eval:
  pass

classifier.train(input_fn=input_fn_train)

loss = classifier.evaluate(input_fn=input_fn_eval)[""loss""]

predictions = classifier.predict(new_samples)

"
"tf.estimator.BaselineEstimator(
    head, model_dir=None, optimizer='Ftrl', config=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import BaselineEstimator

estimator = tf.estimator.BaselineEstimator(
    head=tf.estimator.MultiLabelHead(n_classes=3))

def input_fn_train:
  pass

def input_fn_eval:
  pass

estimator.train(input_fn=input_fn_train)

loss = estimator.evaluate(input_fn=input_fn_eval)[""loss""]

predictions = estimator.predict(new_samples)

"
"tf.estimator.BaselineRegressor(
    model_dir=None,
    label_dimension=1,
    weight_column=None,
    optimizer='Ftrl',
    config=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import BaselineRegressor

regressor = tf.estimator.BaselineRegressor()

def input_fn_train:
  pass

def input_fn_eval:
  pass

regressor.train(input_fn=input_fn_train)

loss = regressor.evaluate(input_fn=input_fn_eval)[""loss""]

predictions = regressor.predict(new_samples)
"
"tf.estimator.BinaryClassHead(
    weight_column=None,
    thresholds=None,
    label_vocabulary=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,
    loss_fn=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import BinaryClassHead
head = tf.estimator.BinaryClassHead()
logits = np.array(((45,), (-41,),), dtype=np.float32)
labels = np.array(((1,), (1,),), dtype=np.int32)
features = {'x': np.array(((42,),), dtype=np.float32)}
loss = head.loss(labels, logits, features=features)
print('{:.2f}'.format(loss.numpy()))
20.50
eval_metrics = head.metrics()
updated_metrics = head.update_metrics(
  eval_metrics, features, logits, labels)
for k in sorted(updated_metrics):
 print('{} : {:.2f}'.format(k, updated_metrics[k].result().numpy()))
  accuracy : 0.50
  accuracy_baseline : 1.00
  auc : 0.00
  auc_precision_recall : 1.00
  average_loss : 20.50
  label/mean : 1.00
  precision : 1.00
  prediction/mean : 0.50
  recall : 0.50
preds = head.predictions(logits)
print(preds['logits'])
tf.Tensor(
  [[ 45.]
   [-41.]], shape=(2, 1), dtype=float32)"
"tf.estimator.DNNClassifier(
    hidden_units,
    feature_columns,
    model_dir=None,
    n_classes=2,
    weight_column=None,
    label_vocabulary=None,
    optimizer='Adagrad',
    activation_fn=tf.nn.relu,
    dropout=None,
    config=None,
    warm_start_from=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,
    batch_norm=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import DNNClassifier
categorical_feature_a = categorical_column_with_hash_bucket(...)
categorical_feature_b = categorical_column_with_hash_bucket(...)

categorical_feature_a_emb = embedding_column(
    categorical_column=categorical_feature_a, ...)
categorical_feature_b_emb = embedding_column(
    categorical_column=categorical_feature_b, ...)

estimator = tf.estimator.DNNClassifier(
    feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],
    hidden_units=[1024, 512, 256])

estimator = tf.estimator.DNNClassifier(
    feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],
    hidden_units=[1024, 512, 256],
    optimizer=tf.compat.v1.train.ProximalAdagradOptimizer(
      learning_rate=0.1,
      l1_regularization_strength=0.001
    ))

estimator = tf.estimator.DNNClassifier(
    feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],
    hidden_units=[1024, 512, 256],
    optimizer=lambda: tf.keras.optimizers.Adam(
        learning_rate=tf.compat.v1.train.exponential_decay(
            learning_rate=0.1,
            global_step=tf.compat.v1.train.get_global_step(),
            decay_steps=10000,
            decay_rate=0.96))

estimator = tf.estimator.DNNClassifier(
    feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],
    hidden_units=[1024, 512, 256],
    warm_start_from=""/path/to/checkpoint/dir"")

def input_fn_train:
  pass
def input_fn_eval:
  pass
def input_fn_predict:
  pass
estimator.train(input_fn=input_fn_train)
metrics = estimator.evaluate(input_fn=input_fn_eval)
predictions = estimator.predict(input_fn=input_fn_predict)
"
"tf.estimator.DNNEstimator(
    head,
    hidden_units,
    feature_columns,
    model_dir=None,
    optimizer='Adagrad',
    activation_fn=tf.nn.relu,
    dropout=None,
    config=None,
    warm_start_from=None,
    batch_norm=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import DNNEstimator
sparse_feature_a = sparse_column_with_hash_bucket(...)
sparse_feature_b = sparse_column_with_hash_bucket(...)

sparse_feature_a_emb = embedding_column(sparse_id_column=sparse_feature_a,
                                        ...)
sparse_feature_b_emb = embedding_column(sparse_id_column=sparse_feature_b,
                                        ...)

estimator = tf.estimator.DNNEstimator(
    head=tf.estimator.MultiLabelHead(n_classes=3),
    feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],
    hidden_units=[1024, 512, 256])

estimator = tf.estimator.DNNEstimator(
    head=tf.estimator.MultiLabelHead(n_classes=3),
    feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],
    hidden_units=[1024, 512, 256],
    optimizer=tf.compat.v1.train.ProximalAdagradOptimizer(
      learning_rate=0.1,
      l1_regularization_strength=0.001
    ))

estimator = tf.estimator.DNNEstimator(
    head=tf.estimator.MultiLabelHead(n_classes=3),
    feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],
    hidden_units=[1024, 512, 256],
    optimizer=lambda: tf.keras.optimizers.Adam(
        learning_rate=tf.compat.v1.train.exponential_decay(
            learning_rate=0.1,
            global_step=tf.compat.v1.train.get_global_step(),
            decay_steps=10000,
            decay_rate=0.96))

estimator = tf.estimator.DNNEstimator(
    head=tf.estimator.MultiLabelHead(n_classes=3),
    feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],
    hidden_units=[1024, 512, 256],
    warm_start_from=""/path/to/checkpoint/dir"")

def input_fn_train:
  pass
def input_fn_eval:
  pass
def input_fn_predict:
  pass
estimator.train(input_fn=input_fn_train)
metrics = estimator.evaluate(input_fn=input_fn_eval)
predictions = estimator.predict(input_fn=input_fn_predict)
"
"tf.estimator.DNNLinearCombinedClassifier(
    model_dir=None,
    linear_feature_columns=None,
    linear_optimizer='Ftrl',
    dnn_feature_columns=None,
    dnn_optimizer='Adagrad',
    dnn_hidden_units=None,
    dnn_activation_fn=tf.nn.relu,
    dnn_dropout=None,
    n_classes=2,
    weight_column=None,
    label_vocabulary=None,
    config=None,
    warm_start_from=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,
    batch_norm=False,
    linear_sparse_combiner='sum'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import DNNLinearCombinedClassifier
numeric_feature = numeric_column(...)
categorical_column_a = categorical_column_with_hash_bucket(...)
categorical_column_b = categorical_column_with_hash_bucket(...)

categorical_feature_a_x_categorical_feature_b = crossed_column(...)
categorical_feature_a_emb = embedding_column(
    categorical_column=categorical_feature_a, ...)
categorical_feature_b_emb = embedding_column(
    categorical_id_column=categorical_feature_b, ...)

estimator = tf.estimator.DNNLinearCombinedClassifier(
    linear_feature_columns=[categorical_feature_a_x_categorical_feature_b],
    linear_optimizer=tf.keras.optimizers.Ftrl(...),
    dnn_feature_columns=[
        categorical_feature_a_emb, categorical_feature_b_emb,
        numeric_feature],
    dnn_hidden_units=[1000, 500, 100],
    dnn_optimizer=tf.keras.optimizers.Adagrad(...),
    warm_start_from=""/path/to/checkpoint/dir"")

tf.compat.v1.train.ProximalAdagradOptimizer(
    learning_rate=0.1,
    l1_regularization_strength=0.001,
    l2_regularization_strength=0.001)
lambda: tf.keras.optimizers.Adam(
    learning_rate=tf.compat.v1.train.exponential_decay(
        learning_rate=0.1,
        global_step=tf.compat.v1.train.get_global_step(),
        decay_steps=10000,
        decay_rate=0.96)

def input_fn_train:
  pass
def input_fn_eval:
  pass
def input_fn_predict:
  pass
estimator.train(input_fn=input_fn_train, steps=100)
metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)
predictions = estimator.predict(input_fn=input_fn_predict)
"
"tf.estimator.DNNLinearCombinedEstimator(
    head,
    model_dir=None,
    linear_feature_columns=None,
    linear_optimizer='Ftrl',
    dnn_feature_columns=None,
    dnn_optimizer='Adagrad',
    dnn_hidden_units=None,
    dnn_activation_fn=tf.nn.relu,
    dnn_dropout=None,
    config=None,
    batch_norm=False,
    linear_sparse_combiner='sum'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import DNNLinearCombinedEstimator
numeric_feature = numeric_column(...)
categorical_column_a = categorical_column_with_hash_bucket(...)
categorical_column_b = categorical_column_with_hash_bucket(...)

categorical_feature_a_x_categorical_feature_b = crossed_column(...)
categorical_feature_a_emb = embedding_column(
    categorical_column=categorical_feature_a, ...)
categorical_feature_b_emb = embedding_column(
    categorical_column=categorical_feature_b, ...)

estimator = tf.estimator.DNNLinearCombinedEstimator(
    head=tf.estimator.MultiLabelHead(n_classes=3),
    linear_feature_columns=[categorical_feature_a_x_categorical_feature_b],
    linear_optimizer=tf.keras.optimizers.Ftrl(...),
    dnn_feature_columns=[
        categorical_feature_a_emb, categorical_feature_b_emb,
        numeric_feature],
    dnn_hidden_units=[1000, 500, 100],
    dnn_optimizer=tf.keras.optimizers.Adagrad(...))

tf.compat.v1.train.ProximalAdagradOptimizer(
    learning_rate=0.1,
    l1_regularization_strength=0.001,
    l2_regularization_strength=0.001)
lambda: tf.keras.optimizers.Adam(
    learning_rate=tf.compat.v1.train.exponential_decay(
        learning_rate=0.1,
        global_step=tf.compat.v1.train.get_global_step(),
        decay_steps=10000,
        decay_rate=0.96)

def input_fn_train:
  pass
def input_fn_eval:
  pass
def input_fn_predict:
  pass
estimator.train(input_fn=input_fn_train, steps=100)
metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)
predictions = estimator.predict(input_fn=input_fn_predict)
"
"tf.estimator.DNNLinearCombinedRegressor(
    model_dir=None,
    linear_feature_columns=None,
    linear_optimizer='Ftrl',
    dnn_feature_columns=None,
    dnn_optimizer='Adagrad',
    dnn_hidden_units=None,
    dnn_activation_fn=tf.nn.relu,
    dnn_dropout=None,
    label_dimension=1,
    weight_column=None,
    config=None,
    warm_start_from=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,
    batch_norm=False,
    linear_sparse_combiner='sum'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import DNNLinearCombinedRegressor
numeric_feature = numeric_column(...)
categorical_column_a = categorical_column_with_hash_bucket(...)
categorical_column_b = categorical_column_with_hash_bucket(...)

categorical_feature_a_x_categorical_feature_b = crossed_column(...)
categorical_feature_a_emb = embedding_column(
    categorical_column=categorical_feature_a, ...)
categorical_feature_b_emb = embedding_column(
    categorical_column=categorical_feature_b, ...)

estimator = tf.estimator.DNNLinearCombinedRegressor(
    linear_feature_columns=[categorical_feature_a_x_categorical_feature_b],
    linear_optimizer=tf.keras.optimizers.Ftrl(...),
    dnn_feature_columns=[
        categorical_feature_a_emb, categorical_feature_b_emb,
        numeric_feature],
    dnn_hidden_units=[1000, 500, 100],
    dnn_optimizer=tf.keras.optimizers.Adagrad(...),
    warm_start_from=""/path/to/checkpoint/dir"")

tf.compat.v1.train.ProximalAdagradOptimizer(
    learning_rate=0.1,
    l1_regularization_strength=0.001,
    l2_regularization_strength=0.001)
lambda: tf.keras.optimizers.Adam(
    learning_rate=tf.compat.v1.train.exponential_decay(
        learning_rate=0.1,
        global_step=tf.compat.v1.train.get_global_step(),
        decay_steps=10000,
        decay_rate=0.96)

def input_fn_train:
  pass
def input_fn_eval:
  pass
def input_fn_predict:
  pass
estimator.train(input_fn=input_fn_train, steps=100)
metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)
predictions = estimator.predict(input_fn=input_fn_predict)
"
"tf.estimator.DNNRegressor(
    hidden_units,
    feature_columns,
    model_dir=None,
    label_dimension=1,
    weight_column=None,
    optimizer='Adagrad',
    activation_fn=tf.nn.relu,
    dropout=None,
    config=None,
    warm_start_from=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,
    batch_norm=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import DNNRegressor
categorical_feature_a = categorical_column_with_hash_bucket(...)
categorical_feature_b = categorical_column_with_hash_bucket(...)

categorical_feature_a_emb = embedding_column(
    categorical_column=categorical_feature_a, ...)
categorical_feature_b_emb = embedding_column(
    categorical_column=categorical_feature_b, ...)

estimator = tf.estimator.DNNRegressor(
    feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],
    hidden_units=[1024, 512, 256])

estimator = tf.estimator.DNNRegressor(
    feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],
    hidden_units=[1024, 512, 256],
    optimizer=tf.compat.v1.train.ProximalAdagradOptimizer(
      learning_rate=0.1,
      l1_regularization_strength=0.001
    ))

estimator = tf.estimator.DNNRegressor(
    feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],
    hidden_units=[1024, 512, 256],
    optimizer=lambda: tf.keras.optimizers.Adam(
        learning_rate=tf.compat.v1.train.exponential_decay(
            learning_rate=0.1,
            global_step=tf.compat.v1.train.get_global_step(),
            decay_steps=10000,
            decay_rate=0.96))

estimator = tf.estimator.DNNRegressor(
    feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],
    hidden_units=[1024, 512, 256],
    warm_start_from=""/path/to/checkpoint/dir"")

def input_fn_train:
  pass
def input_fn_eval:
  pass
def input_fn_predict:
  pass
estimator.train(input_fn=input_fn_train)
metrics = estimator.evaluate(input_fn=input_fn_eval)
predictions = estimator.predict(input_fn=input_fn_predict)
"
"tf.estimator.LinearClassifier(
    feature_columns,
    model_dir=None,
    n_classes=2,
    weight_column=None,
    label_vocabulary=None,
    optimizer='Ftrl',
    config=None,
    warm_start_from=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,
    sparse_combiner='sum'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearClassifier
categorical_column_a = categorical_column_with_hash_bucket(...)
categorical_column_b = categorical_column_with_hash_bucket(...)

categorical_feature_a_x_categorical_feature_b = crossed_column(...)

estimator = tf.estimator.LinearClassifier(
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b])

estimator = tf.estimator.LinearClassifier(
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b],
    optimizer=tf.keras.optimizers.Ftrl(
      learning_rate=0.1,
      l1_regularization_strength=0.001
    ))

estimator = tf.estimator.LinearClassifier(
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b],
    optimizer=lambda: tf.keras.optimizers.Ftrl(
        learning_rate=tf.exponential_decay(
            learning_rate=0.1,
            global_step=tf.get_global_step(),
            decay_steps=10000,
            decay_rate=0.96))

estimator = tf.estimator.LinearClassifier(
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b],
    warm_start_from=""/path/to/checkpoint/dir"")


def input_fn_train:
  pass
def input_fn_eval:
  pass
def input_fn_predict:
  pass
estimator.train(input_fn=input_fn_train)
metrics = estimator.evaluate(input_fn=input_fn_eval)
predictions = estimator.predict(input_fn=input_fn_predict)
"
"tf.estimator.LinearEstimator(
    head,
    feature_columns,
    model_dir=None,
    optimizer='Ftrl',
    config=None,
    sparse_combiner='sum',
    warm_start_from=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearEstimator
categorical_column_a = categorical_column_with_hash_bucket(...)
categorical_column_b = categorical_column_with_hash_bucket(...)

categorical_feature_a_x_categorical_feature_b = crossed_column(...)

estimator = tf.estimator.LinearEstimator(
    head=tf.estimator.MultiLabelHead(n_classes=3),
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b])

estimator = tf.estimator.LinearEstimator(
    head=tf.estimator.MultiLabelHead(n_classes=3),
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b],
    optimizer=lambda: tf.keras.optimizers.Ftrl(
        learning_rate=tf.compat.v1.train.exponential_decay(
            learning_rate=0.1,
            global_step=tf.compat.v1.train.get_global_step(),
            decay_steps=10000,
            decay_rate=0.96))

estimator = tf.estimator.LinearEstimator(
    head=tf.estimator.MultiLabelHead(n_classes=3),
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b])
    optimizer=tf.keras.optimizers.Ftrl(
        learning_rate=0.1,
        l1_regularization_strength=0.001
    ))

def input_fn_train:
  pass
def input_fn_eval:
  pass
def input_fn_predict:
  pass
estimator.train(input_fn=input_fn_train, steps=100)
metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)
predictions = estimator.predict(input_fn=input_fn_predict)
"
"tf.estimator.LinearRegressor(
    feature_columns,
    model_dir=None,
    label_dimension=1,
    weight_column=None,
    optimizer='Ftrl',
    config=None,
    warm_start_from=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,
    sparse_combiner='sum'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearRegressor
categorical_column_a = categorical_column_with_hash_bucket(...)
categorical_column_b = categorical_column_with_hash_bucket(...)

categorical_feature_a_x_categorical_feature_b = crossed_column(...)

estimator = tf.estimator.LinearRegressor(
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b])

estimator = tf.estimator.LinearRegressor(
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b],
    optimizer=tf.keras.optimizers.Ftrl(
      learning_rate=0.1,
      l1_regularization_strength=0.001
    ))

estimator = tf.estimator.LinearRegressor(
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b],
    optimizer=lambda: tf.keras.optimizers.Ftrl(
        learning_rate=tf.compat.v1.train.exponential_decay(
            learning_rate=0.1,
            global_step=tf.compat.v1.train.get_global_step(),
            decay_steps=10000,
            decay_rate=0.96))

estimator = tf.estimator.LinearRegressor(
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b],
    warm_start_from=""/path/to/checkpoint/dir"")


def input_fn_train:
  pass
def input_fn_eval:
  pass
def input_fn_predict:
  pass
estimator.train(input_fn=input_fn_train)
metrics = estimator.evaluate(input_fn=input_fn_eval)
predictions = estimator.predict(input_fn=input_fn_predict)
"
"tf.estimator.LogisticRegressionHead(
    weight_column=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LogisticRegressionHead
my_head = tf.estimator.LogisticRegressionHead()
my_estimator = tf.estimator.DNNEstimator(
    head=my_head,
    hidden_units=...,
    feature_columns=...)
"
"tf.estimator.MultiClassHead(
    n_classes,
    weight_column=None,
    label_vocabulary=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,
    loss_fn=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MultiClassHead
n_classes = 3
head = tf.estimator.MultiClassHead(n_classes)
logits = np.array(((10, 0, 0), (0, 10, 0),), dtype=np.float32)
labels = np.array(((1,), (1,)), dtype=np.int64)
features = {'x': np.array(((42,),), dtype=np.int32)}
loss = head.loss(labels, logits, features=features)
print('{:.2f}'.format(loss.numpy()))
5.00
eval_metrics = head.metrics()
updated_metrics = head.update_metrics(
  eval_metrics, features, logits, labels)
for k in sorted(updated_metrics):
  print('{} : {:.2f}'.format(k, updated_metrics[k].result().numpy()))
accuracy : 0.50
average_loss : 5.00
preds = head.predictions(logits)
print(preds['logits'])
tf.Tensor(
  [[10.  0.  0.]
   [ 0. 10.  0.]], shape=(2, 3), dtype=float32)"
"tf.estimator.MultiHead(
    heads, head_weights=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MultiHead
head1 = tf.estimator.MultiLabelHead(n_classes=2, name='head1')
head2 = tf.estimator.MultiLabelHead(n_classes=3, name='head2')
multi_head = tf.estimator.MultiHead([head1, head2])
logits = {
   'head1': np.array([[-10., 10.], [-15., 10.]], dtype=np.float32),
   'head2': np.array([[20., -20., 20.], [-30., 20., -20.]],
   dtype=np.float32),}
labels = {
   'head1': np.array([[1, 0], [1, 1]], dtype=np.int64),
   'head2': np.array([[0, 1, 0], [1, 1, 0]], dtype=np.int64),}
features = {'x': np.array(((42,),), dtype=np.float32)}
# loss = labels * (logits < 0) * (-logits) +
loss = multi_head.loss(labels, logits, features=features)
print('{:.2f}'.format(loss.numpy()))
23.75
eval_metrics = multi_head.metrics()
updated_metrics = multi_head.update_metrics(
  eval_metrics, features, logits, labels)
for k in sorted(updated_metrics):
 print('{} : {:.2f}'.format(k, updated_metrics[k].result().numpy()))
auc/head1 : 0.17
auc/head2 : 0.33
auc_precision_recall/head1 : 0.60
auc_precision_recall/head2 : 0.40
average_loss/head1 : 8.75
average_loss/head2 : 15.00
loss/head1 : 8.75
loss/head2 : 15.00
preds = multi_head.predictions(logits)
print(preds[('head1', 'logits')])
tf.Tensor(
  [[-10.  10.]
   [-15.  10.]], shape=(2, 2), dtype=float32)"
"tf.estimator.MultiLabelHead(
    n_classes,
    weight_column=None,
    thresholds=None,
    label_vocabulary=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,
    loss_fn=None,
    classes_for_class_based_metrics=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MultiLabelHead
n_classes = 2
head = tf.estimator.MultiLabelHead(n_classes)
logits = np.array([[-1., 1.], [-1.5, 1.5]], dtype=np.float32)
labels = np.array([[1, 0], [1, 1]], dtype=np.int64)
features = {'x': np.array([[41], [42]], dtype=np.int32)}
loss = head.loss(labels, logits, features=features)
print('{:.2f}'.format(loss.numpy()))
1.13
eval_metrics = head.metrics()
updated_metrics = head.update_metrics(
  eval_metrics, features, logits, labels)
for k in sorted(updated_metrics):
 print('{} : {:.2f}'.format(k, updated_metrics[k].result().numpy()))
auc : 0.33
auc_precision_recall : 0.77
average_loss : 1.13
preds = head.predictions(logits)
print(preds['logits'])
tf.Tensor(
  [[-1.   1. ]
   [-1.5  1.5]], shape=(2, 2), dtype=float32)"
"tf.estimator.PoissonRegressionHead(
    label_dimension=1,
    weight_column=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,
    compute_full_loss=True,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import PoissonRegressionHead
my_head = tf.estimator.PoissonRegressionHead()
my_estimator = tf.estimator.DNNEstimator(
    head=my_head,
    hidden_units=...,
    feature_columns=...)
"
"tf.estimator.RegressionHead(
    label_dimension=1,
    weight_column=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,
    loss_fn=None,
    inverse_link_fn=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RegressionHead
head = tf.estimator.RegressionHead()
logits = np.array(((45,), (41,),), dtype=np.float32)
labels = np.array(((43,), (44,),), dtype=np.int32)
features = {'x': np.array(((42,),), dtype=np.float32)}
loss = head.loss(labels, logits, features=features)
print('{:.2f}'.format(loss.numpy()))
6.50
eval_metrics = head.metrics()
updated_metrics = head.update_metrics(
  eval_metrics, features, logits, labels)
for k in sorted(updated_metrics):
 print('{} : {:.2f}'.format(k, updated_metrics[k].result().numpy()))
  average_loss : 6.50
  label/mean : 43.50
  prediction/mean : 43.00
preds = head.predictions(logits)
print(preds['predictions'])
tf.Tensor(
  [[45.]
   [41.]], shape=(2, 1), dtype=float32)"
"tf.estimator.TrainSpec(
    input_fn, max_steps=None, hooks=None, saving_listeners=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TrainSpec
train_spec = tf.estimator.TrainSpec(
   input_fn=lambda: 1,
   max_steps=100,
   hooks=[_StopAtSecsHook(stop_after_secs=10)],
   saving_listeners=[_NewCheckpointListenerForEvaluate(None, 20, None)])
train_spec.saving_listeners[0]._eval_throttle_secs
20
train_spec.hooks[0]._stop_after_secs
10
train_spec.max_steps
100"
"tf.estimator.add_metrics(
    estimator, metric_fn
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import add_metrics
  def my_auc(labels, predictions):
    auc_metric = tf.keras.metrics.AUC(name=""my_auc"")
    auc_metric.update_state(y_true=labels, y_pred=predictions['logistic'])
    return {'auc': auc_metric}

  estimator = tf.estimator.DNNClassifier(...)
  estimator = tf.estimator.add_metrics(estimator, my_auc)
  estimator.train(...)
  estimator.evaluate(...)
"
"tf.estimator.classifier_parse_example_spec(
    feature_columns,
    label_key,
    label_dtype=tf.dtypes.int64,
    label_default=None,
    weight_column=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import classifier_parse_example_spec
feature_b = tf.feature_column.numeric_column(...)
feature_c_bucketized = tf.feature_column.bucketized_column(
  tf.feature_column.numeric_column(""feature_c""), ...)
feature_a_x_feature_c = tf.feature_column.crossed_column(
    columns=[""feature_a"", feature_c_bucketized], ...)

feature_columns = [feature_b, feature_c_bucketized, feature_a_x_feature_c]
parsing_spec = tf.estimator.classifier_parse_example_spec(
    feature_columns, label_key='my-label', label_dtype=tf.string)

assert parsing_spec == {
  ""feature_a"": parsing_ops.VarLenFeature(tf.string),
  ""feature_b"": parsing_ops.FixedLenFeature([1], dtype=tf.float32),
  ""feature_c"": parsing_ops.FixedLenFeature([1], dtype=tf.float32)
  ""my-label"" : parsing_ops.FixedLenFeature([1], dtype=tf.string)
}
"
"tf.estimator.experimental.InMemoryEvaluatorHook(
    estimator, input_fn, steps=None, hooks=None, name=None, every_n_iter=100
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import InMemoryEvaluatorHook
def train_input_fn():
  ...
  return train_dataset

def eval_input_fn():
  ...
  return eval_dataset

estimator = tf.estimator.DNNClassifier(...)

evaluator = tf.estimator.experimental.InMemoryEvaluatorHook(
    estimator, eval_input_fn)
estimator.train(train_input_fn, hooks=[evaluator])
"
"tf.estimator.experimental.LinearSDCA(
    example_id_column,
    num_loss_partitions=1,
    num_table_shards=None,
    symmetric_l1_regularization=0.0,
    symmetric_l2_regularization=1.0,
    adaptive=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearSDCA
real_feature_column = numeric_column(...)
sparse_feature_column = categorical_column_with_hash_bucket(...)
linear_sdca = tf.estimator.experimental.LinearSDCA(
    example_id_column='example_id',
    num_loss_partitions=1,
    num_table_shards=1,
    symmetric_l2_regularization=2.0)
classifier = tf.estimator.LinearClassifier(
    feature_columns=[real_feature_column, sparse_feature_column],
    weight_column=...,
    optimizer=linear_sdca)
classifier.train(input_fn_train, steps=50)
classifier.evaluate(input_fn=input_fn_eval)
"
"tf.estimator.experimental.RNNClassifier(
    sequence_feature_columns,
    context_feature_columns=None,
    units=None,
    cell_type=USE_DEFAULT,
    rnn_cell_fn=None,
    return_sequences=False,
    model_dir=None,
    n_classes=2,
    weight_column=None,
    label_vocabulary=None,
    optimizer='Adagrad',
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,
    sequence_mask='sequence_mask',
    config=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RNNClassifier
token_sequence = sequence_categorical_column_with_hash_bucket(...)
token_emb = embedding_column(categorical_column=token_sequence, ...)

estimator = RNNClassifier(
    sequence_feature_columns=[token_emb],
    units=[32, 16], cell_type='lstm')

  pass
estimator.train(input_fn=input_fn_train, steps=100)

  pass
metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)
  pass
predictions = estimator.predict(input_fn=input_fn_predict)
"
"tf.estimator.experimental.RNNEstimator(
    head,
    sequence_feature_columns,
    context_feature_columns=None,
    units=None,
    cell_type=USE_DEFAULT,
    rnn_cell_fn=None,
    return_sequences=False,
    model_dir=None,
    optimizer='Adagrad',
    config=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RNNEstimator
token_sequence = sequence_categorical_column_with_hash_bucket(...)
token_emb = embedding_column(categorical_column=token_sequence, ...)

estimator = RNNEstimator(
    head=tf.estimator.RegressionHead(),
    sequence_feature_columns=[token_emb],
    units=[32, 16], cell_type='lstm')

def rnn_cell_fn(_):
  cells = [ tf.keras.layers.LSTMCell(size) for size in [32, 16] ]
  return tf.keras.layers.StackedRNNCells(cells)

estimator = RNNEstimator(
    head=tf.estimator.RegressionHead(),
    sequence_feature_columns=[token_emb],
    rnn_cell_fn=rnn_cell_fn)

  pass
estimator.train(input_fn=input_fn_train, steps=100)

  pass
metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)
  pass
predictions = estimator.predict(input_fn=input_fn_predict)
"
"tf.estimator.experimental.make_early_stopping_hook(
    estimator, should_stop_fn, run_every_secs=60, run_every_steps=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import make_early_stopping_hook
estimator = ...
hook = early_stopping.make_early_stopping_hook(
    estimator, should_stop_fn=make_stop_fn(...))
train_spec = tf.estimator.TrainSpec(..., hooks=[hook])
tf.estimator.train_and_evaluate(estimator, train_spec, ...)
"
"tf.estimator.experimental.stop_if_higher_hook(
    estimator,
    metric_name,
    threshold,
    eval_dir=None,
    min_steps=0,
    run_every_secs=60,
    run_every_steps=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stop_if_higher_hook
estimator = ...
hook = early_stopping.stop_if_higher_hook(estimator, ""accuracy"", 0.9)
train_spec = tf.estimator.TrainSpec(..., hooks=[hook])
tf.estimator.train_and_evaluate(estimator, train_spec, ...)
"
"tf.estimator.experimental.stop_if_lower_hook(
    estimator,
    metric_name,
    threshold,
    eval_dir=None,
    min_steps=0,
    run_every_secs=60,
    run_every_steps=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stop_if_lower_hook
estimator = ...
hook = early_stopping.stop_if_lower_hook(estimator, ""loss"", 100)
train_spec = tf.estimator.TrainSpec(..., hooks=[hook])
tf.estimator.train_and_evaluate(estimator, train_spec, ...)
"
"tf.estimator.experimental.stop_if_no_decrease_hook(
    estimator,
    metric_name,
    max_steps_without_decrease,
    eval_dir=None,
    min_steps=0,
    run_every_secs=60,
    run_every_steps=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stop_if_no_decrease_hook
estimator = ...
hook = early_stopping.stop_if_no_decrease_hook(estimator, ""loss"", 100000)
train_spec = tf.estimator.TrainSpec(..., hooks=[hook])
tf.estimator.train_and_evaluate(estimator, train_spec, ...)
"
"tf.estimator.experimental.stop_if_no_increase_hook(
    estimator,
    metric_name,
    max_steps_without_increase,
    eval_dir=None,
    min_steps=0,
    run_every_secs=60,
    run_every_steps=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stop_if_no_increase_hook
estimator = ...
hook = early_stopping.stop_if_no_increase_hook(estimator, ""accuracy"", 100000)
train_spec = tf.estimator.TrainSpec(..., hooks=[hook])
tf.estimator.train_and_evaluate(estimator, train_spec, ...)
"
"tf.estimator.regressor_parse_example_spec(
    feature_columns,
    label_key,
    label_dtype=tf.dtypes.float32,
    label_default=None,
    label_dimension=1,
    weight_column=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import regressor_parse_example_spec
feature_b = tf.feature_column.numeric_column(...)
feature_c_bucketized = tf.feature_column.bucketized_column(
  tf.feature_column.numeric_column(""feature_c""), ...)
feature_a_x_feature_c = tf.feature_column.crossed_column(
    columns=[""feature_a"", feature_c_bucketized], ...)

feature_columns = [feature_b, feature_c_bucketized, feature_a_x_feature_c]
parsing_spec = tf.estimator.regressor_parse_example_spec(
    feature_columns, label_key='my-label')

assert parsing_spec == {
  ""feature_a"": parsing_ops.VarLenFeature(tf.string),
  ""feature_b"": parsing_ops.FixedLenFeature([1], dtype=tf.float32),
  ""feature_c"": parsing_ops.FixedLenFeature([1], dtype=tf.float32)
  ""my-label"" : parsing_ops.FixedLenFeature([1], dtype=tf.float32)
}
"
"tf.estimator.train_and_evaluate(
    estimator, train_spec, eval_spec
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import train_and_evaluate
categorial_feature_a = categorial_column_with_hash_bucket(...)
categorial_feature_a_emb = embedding_column(
    categorical_column=categorial_feature_a, ...)

estimator = DNNClassifier(
    feature_columns=[categorial_feature_a_emb, ...],
    hidden_units=[1024, 512, 256])


  pass
  pass

train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=1000)
eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn)

tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
"
"tf.math.exp(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import exp
x = tf.constant(2.0)
tf.math.exp(x)
<tf.Tensor: shape=(), dtype=float32, numpy=7.389056>
<tf.Tensor: shape=(), dtype=float32, numpy=7.389056>"
"tf.experimental.BatchableExtensionType(
    *args, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
class Vehicle(tf.experimental.BatchableExtensionType):
  top_speed: tf.Tensor
  mpg: tf.Tensor
batch = Vehicle([120, 150, 80], [30, 40, 12])
tf.map_fn(lambda vehicle: vehicle.top_speed * vehicle.mpg, batch,
          fn_output_signature=tf.int32).numpy()
array([3600, 6000,  960], dtype=int32)"
"tf.experimental.DynamicRaggedShape.Spec(
    row_partitions: Tuple[RowPartitionSpec, ...],
    static_inner_shape: tf.TensorShape,
    dtype: tf.dtypes.DType
)
","import pandas as pd
import numpy as np
import tensorflow as tf
experimental_as_proto() -> struct_pb2.TypeSpecProto
"
"tf.experimental.ExtensionType(
    *args, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
class MaskedTensor(ExtensionType):
  values: tf.Tensor
  mask: tf.Tensor"
"tf.experimental.RowPartition(
    row_splits,
    row_lengths=None,
    value_rowids=None,
    nrows=None,
    uniform_row_length=None,
    nvals=None,
    internal=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
p1 = RowPartition.from_row_lengths([4, 0, 3, 1, 0])
p2 = RowPartition.from_row_splits([0, 4, 4, 7, 8, 8])
p3 = RowPartition.from_row_starts([0, 4, 4, 7, 8], nvals=8)
p4 = RowPartition.from_row_limits([4, 4, 7, 8, 8])
p5 = RowPartition.from_value_rowids([0, 0, 0, 0, 2, 2, 2, 3], nrows=5)"
"tf.experimental.StructuredTensor(
    fields: Mapping[str, _FieldValue],
    ragged_shape: tf.experimental.DynamicRaggedShape
)
","import pandas as pd
import numpy as np
import tensorflow as tf
s1 = tf.experimental.StructuredTensor.from_pyval(
    {""age"": 82, ""nicknames"": [""Bob"", ""Bobby""]})
s1.shape
TensorShape([])
s1[""age""]
<tf.Tensor: shape=(), dtype=int32, numpy=82>
<tf.Tensor: shape=(), dtype=int32, numpy=82>"
"tf.experimental.StructuredTensor(
    fields: Mapping[str, _FieldValue],
    ragged_shape: tf.experimental.DynamicRaggedShape
)
","import pandas as pd
import numpy as np
import tensorflow as tf
s1 = tf.experimental.StructuredTensor.from_pyval(
    {""age"": 82, ""nicknames"": [""Bob"", ""Bobby""]})
s1.shape
TensorShape([])
s1[""age""]
<tf.Tensor: shape=(), dtype=int32, numpy=82>
<tf.Tensor: shape=(), dtype=int32, numpy=82>"
"tf.experimental.StructuredTensor.Spec(
    _fields, _ragged_shape
)
","import pandas as pd
import numpy as np
import tensorflow as tf
experimental_as_proto() -> struct_pb2.TypeSpecProto
"
"tf.experimental.dispatch_for_binary_elementwise_apis(
    x_type, y_type
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import dispatch_for_binary_elementwise_apis
class MaskedTensor(tf.experimental.ExtensionType):
  values: tf.Tensor
  mask: tf.Tensor
@dispatch_for_binary_elementwise_apis(MaskedTensor, MaskedTensor)
def binary_elementwise_api_handler(api_func, x, y):
  return MaskedTensor(api_func(x.values, y.values), x.mask & y.mask)
a = MaskedTensor([1, 2, 3, 4, 5], [True, True, True, True, False])
b = MaskedTensor([2, 4, 6, 8, 0], [True, True, True, False, True])
c = tf.add(a, b)
print(f""values={c.values.numpy()}, mask={c.mask.numpy()}"")
values=[ 3 6 9 12 5], mask=[ True True True False False]"
"tf.experimental.dispatch_for_binary_elementwise_assert_apis(
    x_type, y_type
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import dispatch_for_binary_elementwise_assert_apis
class MaskedTensor(tf.experimental.ExtensionType):
  values: tf.Tensor
  mask: tf.Tensor
@dispatch_for_binary_elementwise_assert_apis(MaskedTensor, MaskedTensor)
def binary_elementwise_assert_api_handler(assert_func, x, y):
  merged_mask = tf.logical_and(x.mask, y.mask)
  selected_x_values = tf.boolean_mask(x.values, merged_mask)
  selected_y_values = tf.boolean_mask(y.values, merged_mask)
  assert_func(selected_x_values, selected_y_values)
a = MaskedTensor([1, 1, 0, 1, 1], [False, False, True, True, True])
b = MaskedTensor([2, 2, 0, 2, 2], [True, True, True, False, False])"
"tf.experimental.dispatch_for_unary_elementwise_apis(
    x_type
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import dispatch_for_unary_elementwise_apis
class MaskedTensor(tf.experimental.ExtensionType):
  values: tf.Tensor
  mask: tf.Tensor
@dispatch_for_unary_elementwise_apis(MaskedTensor)
def unary_elementwise_api_handler(api_func, x):
  return MaskedTensor(api_func(x.values), x.mask)
mt = MaskedTensor([1, -2, -3], [True, False, True])
abs_mt = tf.abs(mt)
print(f""values={abs_mt.values.numpy()}, mask={abs_mt.mask.numpy()}"")
values=[1 2 3], mask=[ True False True]"
"tf.experimental.dlpack.from_dlpack(
    dlcapsule
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import from_dlpack
  a = tf.experimental.dlpack.from_dlpack(dlcapsule)
"
"tf.experimental.dlpack.to_dlpack(
    tf_tensor
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import to_dlpack
  a = tf.tensor([1, 10])
  dlcapsule = tf.experimental.dlpack.to_dlpack(a)
"
"tf.experimental.dtensor.barrier(
    mesh: tf.experimental.dtensor.Mesh,
    barrier_name: Optional[str] = None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import barrier

x = [1, 2, 3]
x = dtensor.relayout(x, dtensor.Layout.batch_sharded(mesh, 'batch', 1))
dtensor.barrier(mesh)

sys.exit()
"
"tf.experimental.dtensor.device_name() -> str
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import device_name
import tensorflow as tf

with tf.device(dtensor.device_name()):
"
"tf.experimental.dtensor.pack(
    tensors: Sequence[Any],
    layout: tf.experimental.dtensor.Layout
) -> Any
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import pack
* unpack(pack(tensors)) == tensors
* pack(unpack(dtensor)) == dtensor
"
"tf.experimental.dtensor.unpack(
    tensor: Any
) -> Sequence[Any]
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import unpack
* unpack(pack(tensors)) == tensors
* pack(unpack(dtensor)) == dtensor
"
"tf.experimental.numpy.iinfo(
    int_type
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import iinfo
ii16 = np.iinfo(np.int16)
ii16.min
-32768
ii16.max
32767
ii32 = np.iinfo(np.int32)
ii32.min
-2147483648
ii32.max
2147483647"
"tf.experimental.numpy.issubdtype(
    arg1, arg2
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import issubdtype
ints = np.array([1, 2, 3], dtype=np.int32)
np.issubdtype(ints.dtype, np.integer)
True
np.issubdtype(ints.dtype, np.floating)
False"
"tf.Tensor(
    op, value_index, dtype
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Tensor
c = tf.constant([[1.0, 2.0], [3.0, 4.0]])
d = tf.constant([[1.0, 1.0], [0.0, 1.0]])
e = tf.matmul(c, d)
print(e)
tf.Tensor(
[[1. 3.]
 [3. 7.]], shape=(2, 2), dtype=float32)"
"tf.eye(
    num_rows,
    num_columns=None,
    batch_shape=None,
    dtype=tf.dtypes.float32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import eye
tf.eye(2)
==> [[1., 0.],
     [0., 1.]]

batch_identity = tf.eye(2, batch_shape=[3])

tf.eye(2, num_columns=3)
==> [[ 1.,  0.,  0.],
     [ 0.,  1.,  0.]]
"
"tf.feature_column.categorical_column_with_hash_bucket(
    key,
    hash_bucket_size,
    dtype=tf.dtypes.string
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import categorical_column_with_hash_bucket
import tensorflow as tf
keywords = tf.feature_column.categorical_column_with_hash_bucket(""keywords"",
10000)
columns = [keywords]
features = {'keywords': tf.constant([['Tensorflow', 'Keras', 'RNN', 'LSTM',
'CNN'], ['LSTM', 'CNN', 'Tensorflow', 'Keras', 'RNN'], ['CNN', 'Tensorflow',
'LSTM', 'Keras', 'RNN']])}
linear_prediction, _, _ = tf.compat.v1.feature_column.linear_model(features,
columns)

import tensorflow as tf
keywords = tf.feature_column.categorical_column_with_hash_bucket(""keywords"",
10000)
keywords_embedded = tf.feature_column.embedding_column(keywords, 16)
columns = [keywords_embedded]
features = {'keywords': tf.constant([['Tensorflow', 'Keras', 'RNN', 'LSTM',
'CNN'], ['LSTM', 'CNN', 'Tensorflow', 'Keras', 'RNN'], ['CNN', 'Tensorflow',
'LSTM', 'Keras', 'RNN']])}
input_layer = tf.keras.layers.DenseFeatures(columns)
dense_tensor = input_layer(features)
"
"tf.feature_column.categorical_column_with_identity(
    key, num_buckets, default_value=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import categorical_column_with_identity
import tensorflow as tf
video_id = tf.feature_column.categorical_column_with_identity(
    key='video_id', num_buckets=1000000, default_value=0)
columns = [video_id]
features = {'video_id': tf.sparse.from_dense([[2, 85, 0, 0, 0],
[33,78, 2, 73, 1]])}
linear_prediction = tf.compat.v1.feature_column.linear_model(features,
columns)
"
"tf.feature_column.categorical_column_with_vocabulary_file(
    key,
    vocabulary_file,
    vocabulary_size=None,
    dtype=tf.dtypes.string,
    default_value=None,
    num_oov_buckets=0,
    file_format=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import categorical_column_with_vocabulary_file
states = categorical_column_with_vocabulary_file(
    key='states', vocabulary_file='/us/states.txt', vocabulary_size=50,
    num_oov_buckets=5)
columns = [states, ...]
features = tf.io.parse_example(..., features=make_parse_example_spec(columns))
linear_prediction = linear_model(features, columns)
"
"tf.feature_column.categorical_column_with_vocabulary_list(
    key, vocabulary_list, dtype=None, default_value=-1, num_oov_buckets=0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import categorical_column_with_vocabulary_list
colors = categorical_column_with_vocabulary_list(
    key='colors', vocabulary_list=('R', 'G', 'B', 'Y'),
    num_oov_buckets=2)
columns = [colors, ...]
features = tf.io.parse_example(..., features=make_parse_example_spec(columns))
linear_prediction, _, _ = linear_model(features, columns)
"
"tf.feature_column.embedding_column(
    categorical_column,
    dimension,
    combiner='mean',
    initializer=None,
    ckpt_to_load_from=None,
    tensor_name_in_ckpt=None,
    max_norm=None,
    trainable=True,
    use_safe_embedding_lookup=True
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import embedding_column
video_id = categorical_column_with_identity(
    key='video_id', num_buckets=1000000, default_value=0)
columns = [embedding_column(video_id, 9),...]

estimator = tf.estimator.DNNClassifier(feature_columns=columns, ...)

label_column = ...
def input_fn():
  features = tf.io.parse_example(
      ..., features=make_parse_example_spec(columns + [label_column]))
  labels = features.pop(label_column.name)
  return features, labels

estimator.train(input_fn=input_fn, steps=100)
"
"tf.feature_column.indicator_column(
    categorical_column
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import indicator_column
name = indicator_column(categorical_column_with_vocabulary_list(
    'name', ['bob', 'george', 'wanda']))
columns = [name, ...]
features = tf.io.parse_example(..., features=make_parse_example_spec(columns))
dense_tensor = input_layer(features, columns)

"
"tf.feature_column.make_parse_example_spec(
    feature_columns
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import make_parse_example_spec
feature_a = tf.feature_column.categorical_column_with_vocabulary_file(...)
feature_b = tf.feature_column.numeric_column(...)
feature_c_bucketized = tf.feature_column.bucketized_column(
    tf.feature_column.numeric_column(""feature_c""), ...)
feature_a_x_feature_c = tf.feature_column.crossed_column(
    columns=[""feature_a"", feature_c_bucketized], ...)

feature_columns = set(
    [feature_b, feature_c_bucketized, feature_a_x_feature_c])
features = tf.io.parse_example(
    serialized=serialized_examples,
    features=tf.feature_column.make_parse_example_spec(feature_columns))
"
"tf.feature_column.sequence_categorical_column_with_hash_bucket(
    key,
    hash_bucket_size,
    dtype=tf.dtypes.string
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sequence_categorical_column_with_hash_bucket
tokens = sequence_categorical_column_with_hash_bucket(
    'tokens', hash_bucket_size=1000)
tokens_embedding = embedding_column(tokens, dimension=10)
columns = [tokens_embedding]

features = tf.io.parse_example(..., features=make_parse_example_spec(columns))
sequence_feature_layer = SequenceFeatures(columns)
sequence_input, sequence_length = sequence_feature_layer(features)
sequence_length_mask = tf.sequence_mask(sequence_length)

rnn_cell = tf.keras.layers.SimpleRNNCell(hidden_size)
rnn_layer = tf.keras.layers.RNN(rnn_cell)
outputs, state = rnn_layer(sequence_input, mask=sequence_length_mask)
"
"tf.feature_column.sequence_categorical_column_with_identity(
    key, num_buckets, default_value=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sequence_categorical_column_with_identity
watches = sequence_categorical_column_with_identity(
    'watches', num_buckets=1000)
watches_embedding = embedding_column(watches, dimension=10)
columns = [watches_embedding]

features = tf.io.parse_example(..., features=make_parse_example_spec(columns))
sequence_feature_layer = SequenceFeatures(columns)
sequence_input, sequence_length = sequence_feature_layer(features)
sequence_length_mask = tf.sequence_mask(sequence_length)

rnn_cell = tf.keras.layers.SimpleRNNCell(hidden_size)
rnn_layer = tf.keras.layers.RNN(rnn_cell)
outputs, state = rnn_layer(sequence_input, mask=sequence_length_mask)
"
"tf.feature_column.sequence_categorical_column_with_vocabulary_file(
    key,
    vocabulary_file,
    vocabulary_size=None,
    num_oov_buckets=0,
    default_value=None,
    dtype=tf.dtypes.string
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sequence_categorical_column_with_vocabulary_file
states = sequence_categorical_column_with_vocabulary_file(
    key='states', vocabulary_file='/us/states.txt', vocabulary_size=50,
    num_oov_buckets=5)
states_embedding = embedding_column(states, dimension=10)
columns = [states_embedding]

features = tf.io.parse_example(..., features=make_parse_example_spec(columns))
sequence_feature_layer = SequenceFeatures(columns)
sequence_input, sequence_length = sequence_feature_layer(features)
sequence_length_mask = tf.sequence_mask(sequence_length)

rnn_cell = tf.keras.layers.SimpleRNNCell(hidden_size)
rnn_layer = tf.keras.layers.RNN(rnn_cell)
outputs, state = rnn_layer(sequence_input, mask=sequence_length_mask)
"
"tf.feature_column.sequence_categorical_column_with_vocabulary_list(
    key, vocabulary_list, dtype=None, default_value=-1, num_oov_buckets=0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sequence_categorical_column_with_vocabulary_list
colors = sequence_categorical_column_with_vocabulary_list(
    key='colors', vocabulary_list=('R', 'G', 'B', 'Y'),
    num_oov_buckets=2)
colors_embedding = embedding_column(colors, dimension=3)
columns = [colors_embedding]

features = tf.io.parse_example(..., features=make_parse_example_spec(columns))
sequence_feature_layer = SequenceFeatures(columns)
sequence_input, sequence_length = sequence_feature_layer(features)
sequence_length_mask = tf.sequence_mask(sequence_length)

rnn_cell = tf.keras.layers.SimpleRNNCell(hidden_size)
rnn_layer = tf.keras.layers.RNN(rnn_cell)
outputs, state = rnn_layer(sequence_input, mask=sequence_length_mask)
"
"tf.feature_column.sequence_numeric_column(
    key,
    shape=(1,),
    default_value=0.0,
    dtype=tf.dtypes.float32,
    normalizer_fn=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sequence_numeric_column
temperature = sequence_numeric_column('temperature')
columns = [temperature]

features = tf.io.parse_example(..., features=make_parse_example_spec(columns))
sequence_feature_layer = SequenceFeatures(columns)
sequence_input, sequence_length = sequence_feature_layer(features)
sequence_length_mask = tf.sequence_mask(sequence_length)

rnn_cell = tf.keras.layers.SimpleRNNCell(hidden_size)
rnn_layer = tf.keras.layers.RNN(rnn_cell)
outputs, state = rnn_layer(sequence_input, mask=sequence_length_mask)
"
"tf.fill(
    dims, value, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import fill
tf.fill([2, 3], 9)
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
array([[9, 9, 9],
       [9, 9, 9]], dtype=int32)>"
"tf.fingerprint(
    data, method='farmhash64', name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import fingerprint
tf.fingerprint(data) == tf.fingerprint(tf.reshape(data, ...))
tf.fingerprint(data) == tf.fingerprint(tf.bitcast(data, ...))
"
"tf.math.floor(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import floor
x = tf.constant([1.3324, -1.5, 5.555, -2.532, 0.99, float(""inf"")])
tf.floor(x).numpy()
array([ 1., -2.,  5., -3.,  0., inf], dtype=float32)"
"tf.foldl(
    fn,
    elems,
    initializer=None,
    parallel_iterations=10,
    back_prop=True,
    swap_memory=False,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import foldl
elems = tf.constant([1, 2, 3, 4, 5, 6])
sum = tf.foldl(lambda a, x: a + x, elems)
"
"tf.foldr(
    fn,
    elems,
    initializer=None,
    parallel_iterations=10,
    back_prop=True,
    swap_memory=False,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import foldr
elems = [1, 2, 3, 4, 5, 6]
sum = tf.foldr(lambda a, x: a + x, elems)
"
"tf.function(
    func=None,
    input_signature=None,
    autograph=True,
    jit_compile=None,
    reduce_retracing=False,
    experimental_implements=None,
    experimental_autograph_options=None,
    experimental_relax_shapes=None,
    experimental_compile=None,
    experimental_follow_type_hints=None
) -> tf.types.experimental.GenericFunction
","import pandas as pd
import numpy as np
import tensorflow as tf
@tf.function
def f(x, y):
  return x ** 2 + y
x = tf.constant([2, 3])
y = tf.constant([3, -2])
f(x, y)
<tf.Tensor: ... numpy=array([7, 7], ...)>
<tf.Tensor: ... numpy=array([7, 7], ...)>"
"tf.gather(
    params, indices, validate_indices=None, axis=None, batch_dims=0, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import gather
params = tf.constant(['p0', 'p1', 'p2', 'p3', 'p4', 'p5'])
params[3].numpy()
b'p3'
tf.gather(params, 3).numpy()
b'p3'"
"tf.gather_nd(
    params, indices, batch_dims=0, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import gather_nd
tf.gather_nd(
    indices=[[0, 0],
             [1, 1]],
    params = [['a', 'b'],
              ['c', 'd']]).numpy()
array([b'a', b'd'], dtype=object)"
"tf.get_static_value(
    tensor, partial=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get_static_value
a = tf.constant(10)
tf.get_static_value(a)
10
b = tf.constant(20)
tf.get_static_value(tf.add(a, b))
30"
"tf.grad_pass_through(
    f
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import grad_pass_through
x = tf.Variable(1.0, name=""x"")
z = tf.Variable(3.0, name=""z"")

with tf.GradientTape() as tape:
  y = tf.grad_pass_through(x.assign)(z**2)
grads = tape.gradient(y, z)
"
"tf.gradients(
    ys,
    xs,
    grad_ys=None,
    name='gradients',
    gate_gradients=False,
    aggregation_method=None,
    stop_gradients=None,
    unconnected_gradients=tf.UnconnectedGradients.NONE
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import gradients
@tf.function
def example():
  a = tf.constant(0.)
  b = 2 * a
  return tf.gradients(a + b, [a, b], stop_gradients=[a, b])
example()
[<tf.Tensor: shape=(), dtype=float32, numpy=1.0>,
[<tf.Tensor: shape=(), dtype=float32, numpy=1.0>,
<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]
<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]"
"tf.math.greater(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import greater
x = tf.constant([5, 4, 6])
y = tf.constant([5, 2, 5])
tf.math.greater(x, y) ==> [False, True, True]

x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.greater(x, y) ==> [False, False, True]
"
"tf.math.greater_equal(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import greater_equal
x = tf.constant([5, 4, 6, 7])
y = tf.constant([5, 2, 5, 10])
tf.math.greater_equal(x, y) ==> [True, True, True, False]

x = tf.constant([5, 4, 6, 7])
y = tf.constant([5])
tf.math.greater_equal(x, y) ==> [True, False, True, True]
"
"tf.histogram_fixed_width(
    values,
    value_range,
    nbins=100,
    dtype=tf.dtypes.int32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import histogram_fixed_width
nbins = 5
value_range = [0.0, 5.0]
new_values = [-1.0, 0.0, 1.5, 2.0, 5.0, 15]
hist = tf.histogram_fixed_width(new_values, value_range, nbins=5)
hist.numpy()
array([2, 1, 1, 0, 2], dtype=int32)"
"tf.histogram_fixed_width_bins(
    values,
    value_range,
    nbins=100,
    dtype=tf.dtypes.int32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import histogram_fixed_width_bins
nbins = 5
value_range = [0.0, 5.0]
new_values = [-1.0, 0.0, 1.5, 2.0, 5.0, 15]
indices = tf.histogram_fixed_width_bins(new_values, value_range, nbins=5)
indices.numpy()
array([0, 0, 1, 2, 4, 4], dtype=int32)"
"tf.identity(
    input, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import identity
a = tf.constant([0.78])
a_identity = tf.identity(a)
a.numpy()
array([0.78], dtype=float32)
a_identity.numpy()
array([0.78], dtype=float32)"
"tf.identity_n(
    input, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import identity_n
with tf.get_default_graph().gradient_override_map(
    {'IdentityN': 'OverrideGradientWithG'}):
  y, _ = identity_n([f(x), x])

@tf.RegisterGradient('OverrideGradientWithG')
def ApplyG(op, dy, _):
"
"tf.image.adjust_brightness(
    image, delta
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import adjust_brightness
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.adjust_brightness(x, delta=0.1)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 1.1,  2.1,  3.1],
        [ 4.1,  5.1,  6.1]],
       [[ 7.1,  8.1,  9.1],
        [10.1, 11.1, 12.1]]], dtype=float32)>"
"tf.image.adjust_contrast(
    images, contrast_factor
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import adjust_contrast
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.adjust_contrast(x, 2.)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[-3.5, -2.5, -1.5],
        [ 2.5,  3.5,  4.5]],
       [[ 8.5,  9.5, 10.5],
        [14.5, 15.5, 16.5]]], dtype=float32)>"
"tf.image.adjust_gamma(
    image, gamma=1, gain=1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import adjust_gamma
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.adjust_gamma(x, 0.2)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[1.       , 1.1486983, 1.2457309],
        [1.319508 , 1.3797297, 1.4309691]],
       [[1.4757731, 1.5157166, 1.5518456],
        [1.5848932, 1.6153942, 1.6437519]]], dtype=float32)>"
"tf.image.adjust_hue(
    image, delta, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import adjust_hue
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.adjust_hue(x, 0.2)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 2.3999996,  1.       ,  3.       ],
        [ 5.3999996,  4.       ,  6.       ]],
      [[ 8.4      ,  7.       ,  9.       ],
        [11.4      , 10.       , 12.       ]]], dtype=float32)>"
"tf.image.adjust_jpeg_quality(
    image, jpeg_quality, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import adjust_jpeg_quality
x = [[[0.01, 0.02, 0.03],
      [0.04, 0.05, 0.06]],
     [[0.07, 0.08, 0.09],
      [0.10, 0.11, 0.12]]]
x_jpeg = tf.image.adjust_jpeg_quality(x, 75)
x_jpeg.numpy()
array([[[0.00392157, 0.01960784, 0.03137255],
        [0.02745098, 0.04313726, 0.05490196]],
       [[0.05882353, 0.07450981, 0.08627451],
        [0.08235294, 0.09803922, 0.10980393]]], dtype=float32)"
"tf.image.adjust_saturation(
    image, saturation_factor, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import adjust_saturation
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.adjust_saturation(x, 0.5)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 2. ,  2.5,  3. ],
        [ 5. ,  5.5,  6. ]],
       [[ 8. ,  8.5,  9. ],
        [11. , 11.5, 12. ]]], dtype=float32)>"
"tf.image.convert_image_dtype(
    image, dtype, saturate=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import convert_image_dtype
x = [[[1, 2, 3], [4, 5, 6]],
     [[7, 8, 9], [10, 11, 12]]]
x_int8 = tf.convert_to_tensor(x, dtype=tf.int8)
tf.image.convert_image_dtype(x_int8, dtype=tf.float16, saturate=False)
<tf.Tensor: shape=(2, 2, 3), dtype=float16, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=float16, numpy=
array([[[0.00787, 0.01575, 0.02362],
        [0.0315 , 0.03937, 0.04724]],
       [[0.0551 , 0.063  , 0.07086],
        [0.07874, 0.0866 , 0.0945 ]]], dtype=float16)>"
"tf.image.crop_and_resize(
    image,
    boxes,
    box_indices,
    crop_size,
    method='bilinear',
    extrapolation_value=0.0,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import crop_and_resize
import tensorflow as tf
BATCH_SIZE = 1
NUM_BOXES = 5
IMAGE_HEIGHT = 256
IMAGE_WIDTH = 256
CHANNELS = 3
CROP_SIZE = (24, 24)

image = tf.random.normal(shape=(BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WIDTH,
CHANNELS) )
boxes = tf.random.uniform(shape=(NUM_BOXES, 4))
box_indices = tf.random.uniform(shape=(NUM_BOXES,), minval=0,
maxval=BATCH_SIZE, dtype=tf.int32)
output = tf.image.crop_and_resize(image, boxes, box_indices, CROP_SIZE)
output.shape  #=> (5, 24, 24, 3)
"
"tf.image.crop_to_bounding_box(
    image, offset_height, offset_width, target_height, target_width
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import crop_to_bounding_box
image = tf.constant(np.arange(1, 28, dtype=np.float32), shape=[3, 3, 3])
<tf.Tensor: shape=(3, 3), dtype=float32, numpy=
<tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[ 1.,  4.,  7.],
       [10., 13., 16.],
       [19., 22., 25.]], dtype=float32)>
cropped_image = tf.image.crop_to_bounding_box(image, 0, 0, 2, 2)
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[ 1.,  4.],
       [10., 13.]], dtype=float32)>"
"tf.image.draw_bounding_boxes(
    images, boxes, colors, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import draw_bounding_boxes
img = tf.zeros([1, 3, 3, 3])
box = np.array([0, 0, 1, 1])
boxes = box.reshape([1, 1, 4])
colors = np.array([[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]])
tf.image.draw_bounding_boxes(img, boxes, colors)
<tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
<tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.]],
        [[1., 0., 0.],
        [0., 0., 0.],
        [1., 0., 0.]],
        [[1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.]]]], dtype=float32)>"
"tf.image.extract_glimpse(
    input,
    size,
    offsets,
    centered=True,
    normalized=True,
    noise='uniform',
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import extract_glimpse
x = [[[[0.0],
          [1.0],
          [2.0]],
         [[3.0],
          [4.0],
          [5.0]],
         [[6.0],
          [7.0],
          [8.0]]]]
tf.image.extract_glimpse(x, size=(2, 2), offsets=[[1, 1]],
                        centered=False, normalized=False)
<tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=
<tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=
array([[[[4.],
         [5.]],
        [[7.],
         [8.]]]], dtype=float32)>"
"tf.image.extract_patches(
    images, sizes, strides, rates, padding, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import extract_patches
  n = 10
  images = [[[[x * n + y + 1] for y in range(n)] for x in range(n)]]

  tf.image.extract_patches(images=images,
                           sizes=[1, 3, 3, 1],
                           strides=[1, 5, 5, 1],
                           rates=[1, 1, 1, 1],
                           padding='VALID')

  [[[[ 1  2  3 11 12 13 21 22 23]
     [ 6  7  8 16 17 18 26 27 28]]
    [[51 52 53 61 62 63 71 72 73]
     [56 57 58 66 67 68 76 77 78]]]]
"
"tf.image.flip_left_right(
    image
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import flip_left_right
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.flip_left_right(x)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 4.,  5.,  6.],
        [ 1.,  2.,  3.]],
       [[10., 11., 12.],
        [ 7.,  8.,  9.]]], dtype=float32)>"
"tf.image.flip_up_down(
    image
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import flip_up_down
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.flip_up_down(x)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 7.,  8.,  9.],
        [10., 11., 12.]],
       [[ 1.,  2.,  3.],
        [ 4.,  5.,  6.]]], dtype=float32)>"
"tf.image.grayscale_to_rgb(
    images, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import grayscale_to_rgb
original = tf.constant([[[1.0], [2.0], [3.0]]])
converted = tf.image.grayscale_to_rgb(original)
print(converted.numpy())
[[[1. 1. 1.]
  [2. 2. 2.]
  [3. 3. 3.]]]"
"tf.image.image_gradients(
    image
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import image_gradients
BATCH_SIZE = 1
IMAGE_HEIGHT = 5
IMAGE_WIDTH = 5
CHANNELS = 1
image = tf.reshape(tf.range(IMAGE_HEIGHT * IMAGE_WIDTH * CHANNELS,
  delta=1, dtype=tf.float32),
  shape=(BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WIDTH, CHANNELS))
dy, dx = tf.image.image_gradients(image)
print(image[0, :,:,0])
tf.Tensor(
  [[ 0.  1.  2.  3.  4.]
  [ 5.  6.  7.  8.  9.]
  [10. 11. 12. 13. 14.]
  [15. 16. 17. 18. 19.]
  [20. 21. 22. 23. 24.]], shape=(5, 5), dtype=float32)
print(dy[0, :,:,0])
tf.Tensor(
  [[5. 5. 5. 5. 5.]
  [5. 5. 5. 5. 5.]
  [5. 5. 5. 5. 5.]
  [5. 5. 5. 5. 5.]
  [0. 0. 0. 0. 0.]], shape=(5, 5), dtype=float32)
print(dx[0, :,:,0])
tf.Tensor(
  [[1. 1. 1. 1. 0.]
  [1. 1. 1. 1. 0.]
  [1. 1. 1. 1. 0.]
  [1. 1. 1. 1. 0.]
  [1. 1. 1. 1. 0.]], shape=(5, 5), dtype=float32)
"
"tf.image.non_max_suppression(
    boxes,
    scores,
    max_output_size,
    iou_threshold=0.5,
    score_threshold=float('-inf'),
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import non_max_suppression
  selected_indices = tf.image.non_max_suppression(
      boxes, scores, max_output_size, iou_threshold)
  selected_boxes = tf.gather(boxes, selected_indices)
"
"tf.image.non_max_suppression_overlaps(
    overlaps,
    scores,
    max_output_size,
    overlap_threshold=0.5,
    score_threshold=float('-inf'),
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import non_max_suppression_overlaps
  selected_indices = tf.image.non_max_suppression_overlaps(
      overlaps, scores, max_output_size, iou_threshold)
  selected_boxes = tf.gather(boxes, selected_indices)
"
"tf.image.non_max_suppression_padded(
    boxes,
    scores,
    max_output_size,
    iou_threshold=0.5,
    score_threshold=float('-inf'),
    pad_to_max_output_size=False,
    name=None,
    sorted_input=False,
    canonicalized_coordinates=False,
    tile_size=512
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import non_max_suppression_padded
  selected_indices_padded, num_valid = tf.image.non_max_suppression_padded(
      boxes, scores, max_output_size, iou_threshold,
      score_threshold, pad_to_max_output_size=True)
  selected_indices = tf.slice(
      selected_indices_padded, tf.constant([0]), num_valid)
  selected_boxes = tf.gather(boxes, selected_indices)
"
"tf.image.pad_to_bounding_box(
    image, offset_height, offset_width, target_height, target_width
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import pad_to_bounding_box
x = [[[1., 2., 3.],
      [4., 5., 6.]],
      [[7., 8., 9.],
      [10., 11., 12.]]]
padded_image = tf.image.pad_to_bounding_box(x, 1, 1, 4, 4)
padded_image
<tf.Tensor: shape=(4, 4, 3), dtype=float32, numpy=
<tf.Tensor: shape=(4, 4, 3), dtype=float32, numpy=
array([[[ 0.,  0.,  0.],
[ 0.,  0.,  0.],
[ 0.,  0.,  0.],
[ 0.,  0.,  0.]],
[[ 0.,  0.,  0.],
[ 1.,  2.,  3.],
[ 4.,  5.,  6.],
[ 0.,  0.,  0.]],
[[ 0.,  0.,  0.],
[ 7.,  8.,  9.],
[10., 11., 12.],
[ 0.,  0.,  0.]],
[[ 0.,  0.,  0.],
[ 0.,  0.,  0.],
[ 0.,  0.,  0.],
[ 0.,  0.,  0.]]], dtype=float32)>"
"tf.image.per_image_standardization(
    image
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import per_image_standardization
image = tf.constant(np.arange(1, 13, dtype=np.int32), shape=[2, 2, 3])
<tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=
array([[[ 1,  2,  3],
        [ 4,  5,  6]],
       [[ 7,  8,  9],
        [10, 11, 12]]], dtype=int32)>
new_image = tf.image.per_image_standardization(image)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[-1.593255  , -1.3035723 , -1.0138896 ],
        [-0.7242068 , -0.4345241 , -0.14484136]],
       [[ 0.14484136,  0.4345241 ,  0.7242068 ],
        [ 1.0138896 ,  1.3035723 ,  1.593255  ]]], dtype=float32)>"
"tf.image.psnr(
    a, b, max_val, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import psnr
    im1 = tf.decode_png('path/to/im1.png')
    im2 = tf.decode_png('path/to/im2.png')
    psnr1 = tf.image.psnr(im1, im2, max_val=255)

    im1 = tf.image.convert_image_dtype(im1, tf.float32)
    im2 = tf.image.convert_image_dtype(im2, tf.float32)
    psnr2 = tf.image.psnr(im1, im2, max_val=1.0)
"
"tf.image.random_brightness(
    image, max_delta, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import random_brightness
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
     [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.random_brightness(x, 0.2)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=...>
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=...>"
"tf.image.random_contrast(
    image, lower, upper, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import random_contrast
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.random_contrast(x, 0.2, 0.5)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=...>
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=...>"
"tf.image.random_crop(
    value, size, seed=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import random_crop
image = [[1, 2, 3], [4, 5, 6]]
result = tf.image.random_crop(value=image, size=(1, 3))
result.shape.as_list()
[1, 3]"
"tf.image.random_flip_left_right(
    image, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import random_flip_left_right
image = np.array([[[1], [2]], [[3], [4]]])
tf.image.random_flip_left_right(image, 5).numpy().tolist()
[[[2], [1]], [[4], [3]]]"
"tf.image.random_flip_up_down(
    image, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import random_flip_up_down
image = np.array([[[1], [2]], [[3], [4]]])
tf.image.random_flip_up_down(image, 3).numpy().tolist()
[[[3], [4]], [[1], [2]]]"
"tf.image.random_hue(
    image, max_delta, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import random_hue
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.random_hue(x, 0.2)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=...>
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=...>"
"tf.image.random_jpeg_quality(
    image, min_jpeg_quality, max_jpeg_quality, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import random_jpeg_quality
x = tf.constant([[[1, 2, 3],
                  [4, 5, 6]],
                 [[7, 8, 9],
                  [10, 11, 12]]], dtype=tf.uint8)
tf.image.random_jpeg_quality(x, 75, 95)
<tf.Tensor: shape=(2, 2, 3), dtype=uint8, numpy=...>
<tf.Tensor: shape=(2, 2, 3), dtype=uint8, numpy=...>"
"tf.image.random_saturation(
    image, lower, upper, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import random_saturation
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.random_saturation(x, 5, 10)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 0. ,  1.5,  3. ],
        [ 0. ,  3. ,  6. ]],
       [[ 0. ,  4.5,  9. ],
        [ 0. ,  6. , 12. ]]], dtype=float32)>"
"tf.image.resize(
    images,
    size,
    method=ResizeMethod.BILINEAR,
    preserve_aspect_ratio=False,
    antialias=False,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import resize
image = tf.constant([
 [1,0,0,0,0],
 [0,1,0,0,0],
 [0,0,1,0,0],
 [0,0,0,1,0],
 [0,0,0,0,1],
])
image = image[tf.newaxis, ..., tf.newaxis]
[1, 5, 5, 1]
tf.image.resize(image, [3,5])[0,...,0].numpy()
array([[0.6666667, 0.3333333, 0.       , 0.       , 0.       ],
       [0.       , 0.       , 1.       , 0.       , 0.       ],
       [0.       , 0.       , 0.       , 0.3333335, 0.6666665]],
      dtype=float32)"
"tf.image.resize_with_crop_or_pad(
    image, target_height, target_width
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import resize_with_crop_or_pad
array([[ 0,  3,  6,  9, 12],
       [15, 18, 21, 24, 27],
       [30, 33, 36, 39, 42],
       [45, 48, 51, 54, 57],
       [60, 63, 66, 69, 72]])
image[:,:,0]
<tf.Tensor: shape=(3, 3), dtype=int64, numpy=
<tf.Tensor: shape=(3, 3), dtype=int64, numpy=
array([[18, 21, 24],
       [33, 36, 39],
       [48, 51, 54]])>"
"tf.image.rgb_to_grayscale(
    images, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import rgb_to_grayscale
original = tf.constant([[[1.0, 2.0, 3.0]]])
converted = tf.image.rgb_to_grayscale(original)
print(converted.numpy())
[[[1.81...]]]"
"tf.image.rgb_to_hsv(
    images, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import rgb_to_hsv
blue_image = tf.stack([
   tf.zeros([5,5]),
   tf.zeros([5,5]),
   tf.ones([5,5])],
   axis=-1)
blue_hsv_image = tf.image.rgb_to_hsv(blue_image)
blue_hsv_image[0,0].numpy()
array([0.6666667, 1. , 1. ], dtype=float32)"
"tf.image.rgb_to_yiq(
    images
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import rgb_to_yiq
x = tf.constant([[[1.0, 2.0, 3.0]]])
tf.image.rgb_to_yiq(x)
<tf.Tensor: shape=(1, 1, 3), dtype=float32,
<tf.Tensor: shape=(1, 1, 3), dtype=float32,
numpy=array([[[ 1.815     , -0.91724455,  0.09962624]]], dtype=float32)>"
"tf.image.rot90(
    image, k=1, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import rot90
a=tf.constant([[[1],[2]],
               [[3],[4]]])
a_rot=tf.image.rot90(a)
print(a_rot[...,0].numpy())
[[2 4]
 [1 3]]
a_rot=tf.image.rot90(a, k=3)
print(a_rot[...,0].numpy())
[[3 1]
 [4 2]]
a_rot=tf.image.rot90(a, k=-2)
print(a_rot[...,0].numpy())
[[4 3]
 [2 1]]"
"tf.image.sample_distorted_bounding_box(
    image_size,
    bounding_boxes,
    seed=0,
    min_object_covered=0.1,
    aspect_ratio_range=None,
    area_range=None,
    max_attempts=None,
    use_image_if_no_bounding_boxes=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sample_distorted_bounding_box
    begin, size, bbox_for_draw = tf.image.sample_distorted_bounding_box(
        tf.shape(image),
        bounding_boxes=bounding_boxes,
        min_object_covered=0.1)

    image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),
                                                  bbox_for_draw)
    tf.compat.v1.summary.image('images_with_box', image_with_box)

    distorted_image = tf.slice(image, begin, size)
"
"tf.image.ssim(
    img1,
    img2,
    max_val,
    filter_size=11,
    filter_sigma=1.5,
    k1=0.01,
    k2=0.03,
    return_index_map=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ssim
    im1 = tf.image.decode_image(tf.io.read_file('path/to/im1.png'))
    im2 = tf.image.decode_image(tf.io.read_file('path/to/im2.png'))
    im1 = tf.expand_dims(im1, axis=0)
    im2 = tf.expand_dims(im2, axis=0)
    ssim1 = tf.image.ssim(im1, im2, max_val=255, filter_size=11,
                          filter_sigma=1.5, k1=0.01, k2=0.03)

    im1 = tf.image.convert_image_dtype(im1, tf.float32)
    im2 = tf.image.convert_image_dtype(im2, tf.float32)
    ssim2 = tf.image.ssim(im1, im2, max_val=1.0, filter_size=11,
                          filter_sigma=1.5, k1=0.01, k2=0.03)
"
"tf.image.stateless_random_brightness(
    image, max_delta, seed
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_random_brightness
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
     [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
seed = (1, 2)
tf.image.stateless_random_brightness(x, 0.2, seed)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 1.1376241,  2.1376243,  3.1376243],
        [ 4.1376243,  5.1376243,  6.1376243]],
       [[ 7.1376243,  8.137624 ,  9.137624 ],
        [10.137624 , 11.137624 , 12.137624 ]]], dtype=float32)>"
"tf.image.stateless_random_contrast(
    image, lower, upper, seed
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_random_contrast
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
     [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
seed = (1, 2)
tf.image.stateless_random_contrast(x, 0.2, 0.5, seed)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[3.4605184, 4.4605184, 5.4605184],
        [4.820173 , 5.820173 , 6.820173 ]],
       [[6.179827 , 7.179827 , 8.179828 ],
        [7.5394816, 8.539482 , 9.539482 ]]], dtype=float32)>"
"tf.image.stateless_random_crop(
    value, size, seed, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_random_crop
image = [[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]
seed = (1, 2)
tf.image.stateless_random_crop(value=image, size=(1, 2, 3), seed=seed)
<tf.Tensor: shape=(1, 2, 3), dtype=int32, numpy=
<tf.Tensor: shape=(1, 2, 3), dtype=int32, numpy=
array([[[1, 2, 3],
        [4, 5, 6]]], dtype=int32)>"
"tf.image.stateless_random_flip_left_right(
    image, seed
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_random_flip_left_right
image = np.array([[[1], [2]], [[3], [4]]])
seed = (2, 3)
tf.image.stateless_random_flip_left_right(image, seed).numpy().tolist()
[[[2], [1]], [[4], [3]]]"
"tf.image.stateless_random_flip_up_down(
    image, seed
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_random_flip_up_down
image = np.array([[[1], [2]], [[3], [4]]])
seed = (2, 3)
tf.image.stateless_random_flip_up_down(image, seed).numpy().tolist()
[[[3], [4]], [[1], [2]]]"
"tf.image.stateless_random_hue(
    image, max_delta, seed
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_random_hue
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
     [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
seed = (1, 2)
tf.image.stateless_random_hue(x, 0.2, seed)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 1.6514902,  1.       ,  3.       ],
        [ 4.65149  ,  4.       ,  6.       ]],
       [[ 7.65149  ,  7.       ,  9.       ],
        [10.65149  , 10.       , 12.       ]]], dtype=float32)>"
"tf.image.stateless_random_jpeg_quality(
    image, min_jpeg_quality, max_jpeg_quality, seed
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_random_jpeg_quality
x = tf.constant([[[1, 2, 3],
                  [4, 5, 6]],
                 [[7, 8, 9],
                  [10, 11, 12]]], dtype=tf.uint8)
seed = (1, 2)
tf.image.stateless_random_jpeg_quality(x, 75, 95, seed)
<tf.Tensor: shape=(2, 2, 3), dtype=uint8, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=uint8, numpy=
array([[[ 0,  4,  5],
        [ 1,  5,  6]],
       [[ 5,  9, 10],
        [ 5,  9, 10]]], dtype=uint8)>"
"tf.image.stateless_random_saturation(
    image, lower, upper, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_random_saturation
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
     [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
seed = (1, 2)
tf.image.stateless_random_saturation(x, 0.5, 1.0, seed)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 1.1559395,  2.0779698,  3.       ],
        [ 4.1559396,  5.07797  ,  6.       ]],
       [[ 7.1559396,  8.07797  ,  9.       ],
        [10.155939 , 11.07797  , 12.       ]]], dtype=float32)>"
"tf.image.stateless_sample_distorted_bounding_box(
    image_size,
    bounding_boxes,
    seed,
    min_object_covered=0.1,
    aspect_ratio_range=None,
    area_range=None,
    max_attempts=None,
    use_image_if_no_bounding_boxes=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_sample_distorted_bounding_box
image = np.array([[[1], [2], [3]], [[4], [5], [6]], [[7], [8], [9]]])
bbox = tf.constant(
  [0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])
seed = (1, 2)
bbox_begin, bbox_size, bbox_draw = (
  tf.image.stateless_sample_distorted_bounding_box(
    tf.shape(image), bounding_boxes=bbox, seed=seed))
tf.slice(image, bbox_begin, bbox_size)
<tf.Tensor: shape=(2, 2, 1), dtype=int64, numpy=
<tf.Tensor: shape=(2, 2, 1), dtype=int64, numpy=
array([[[1],
        [2]],
       [[4],
        [5]]])>
colors = np.array([[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]])
tf.image.draw_bounding_boxes(
  tf.expand_dims(tf.cast(image, tf.float32),0), bbox_draw, colors)
<tf.Tensor: shape=(1, 3, 3, 1), dtype=float32, numpy=
<tf.Tensor: shape=(1, 3, 3, 1), dtype=float32, numpy=
array([[[[1.],
         [1.],
         [3.]],
        [[1.],
         [1.],
         [6.]],
        [[7.],
         [8.],
         [9.]]]], dtype=float32)>"
"tf.image.transpose(
    image, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import transpose
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.transpose(x)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 1.,  2.,  3.],
        [ 7.,  8.,  9.]],
       [[ 4.,  5.,  6.],
        [10., 11., 12.]]], dtype=float32)>"
"tf.image.yuv_to_rgb(
    images
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import yuv_to_rgb
yuv_images = tf.random.uniform(shape=[100, 64, 64, 3], maxval=255)
last_dimension_axis = len(yuv_images.shape) - 1
yuv_tensor_images = tf.truediv(
    tf.subtract(
        yuv_images,
        tf.reduce_min(yuv_images)
    ),
    tf.subtract(
        tf.reduce_max(yuv_images),
        tf.reduce_min(yuv_images)
     )
)
y, u, v = tf.split(yuv_tensor_images, 3, axis=last_dimension_axis)
target_uv_min, target_uv_max = -0.5, 0.5
u = u * (target_uv_max - target_uv_min) + target_uv_min
v = v * (target_uv_max - target_uv_min) + target_uv_min
preprocessed_yuv_images = tf.concat([y, u, v], axis=last_dimension_axis)
rgb_tensor_images = tf.image.yuv_to_rgb(preprocessed_yuv_images)
"
"tf.keras.initializers.Constant(
    value=0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Constant
initializer = tf.keras.initializers.Constant(3.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.GlorotNormal(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GlorotNormal
initializer = tf.keras.initializers.GlorotNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.GlorotUniform(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GlorotUniform
initializer = tf.keras.initializers.GlorotUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.HeNormal(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import HeNormal
initializer = tf.keras.initializers.HeNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.HeUniform(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import HeUniform
initializer = tf.keras.initializers.HeUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.Identity(
    gain=1.0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Identity
initializer = tf.keras.initializers.Identity()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.LecunNormal(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LecunNormal
initializer = tf.keras.initializers.LecunNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.LecunUniform(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LecunUniform
initializer = tf.keras.initializers.LecunUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.Orthogonal(
    gain=1.0, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Orthogonal
initializer = tf.keras.initializers.Orthogonal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.RandomNormal(
    mean=0.0, stddev=0.05, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RandomNormal
initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.RandomUniform(
    minval=-0.05, maxval=0.05, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RandomUniform
initializer = tf.keras.initializers.RandomUniform(minval=0., maxval=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.TruncatedNormal(
    mean=0.0, stddev=0.05, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TruncatedNormal
initializer = tf.keras.initializers.TruncatedNormal(mean=0., stddev=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.VarianceScaling(
    scale=1.0,
    mode='fan_in',
    distribution='truncated_normal',
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import VarianceScaling
initializer = tf.keras.initializers.VarianceScaling(
scale=0.1, mode='fan_in', distribution='uniform')
values = initializer(shape=(2, 2))"
"tf.keras.initializers.Constant(
    value=0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Constant
initializer = tf.keras.initializers.Constant(3.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.GlorotNormal(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GlorotNormal
initializer = tf.keras.initializers.GlorotNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.GlorotUniform(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GlorotUniform
initializer = tf.keras.initializers.GlorotUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.HeNormal(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import HeNormal
initializer = tf.keras.initializers.HeNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.HeUniform(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import HeUniform
initializer = tf.keras.initializers.HeUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.Identity(
    gain=1.0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Identity
initializer = tf.keras.initializers.Identity()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.LecunNormal(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LecunNormal
initializer = tf.keras.initializers.LecunNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.LecunUniform(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LecunUniform
initializer = tf.keras.initializers.LecunUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.Orthogonal(
    gain=1.0, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Orthogonal
initializer = tf.keras.initializers.Orthogonal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.RandomNormal(
    mean=0.0, stddev=0.05, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RandomNormal
initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.RandomUniform(
    minval=-0.05, maxval=0.05, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RandomUniform
initializer = tf.keras.initializers.RandomUniform(minval=0., maxval=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.TruncatedNormal(
    mean=0.0, stddev=0.05, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TruncatedNormal
initializer = tf.keras.initializers.TruncatedNormal(mean=0., stddev=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.VarianceScaling(
    scale=1.0,
    mode='fan_in',
    distribution='truncated_normal',
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import VarianceScaling
initializer = tf.keras.initializers.VarianceScaling(
scale=0.1, mode='fan_in', distribution='uniform')
values = initializer(shape=(2, 2))"
"tf.io.decode_raw(
    input_bytes, out_type, little_endian=True, fixed_length=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import decode_raw
tf.io.decode_raw(tf.constant(""1""), tf.uint8)
<tf.Tensor: shape=(1,), dtype=uint8, numpy=array([49], dtype=uint8)>
<tf.Tensor: shape=(1,), dtype=uint8, numpy=array([49], dtype=uint8)>
tf.io.decode_raw(tf.constant(""1,2""), tf.uint8)
<tf.Tensor: shape=(3,), dtype=uint8, numpy=array([49, 44, 50], dtype=uint8)>
<tf.Tensor: shape=(3,), dtype=uint8, numpy=array([49, 44, 50], dtype=uint8)>"
"tf.io.gfile.GFile(
    name, mode='r'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GFile
with open(""/tmp/x"", ""w"") as f:
  f.write(""asdf"")
4
with tf.io.gfile.GFile(""/tmp/x"") as f:
  f.read()
'asdf'"
"tf.io.gfile.copy(
    src, dst, overwrite=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import copy
with open(""/tmp/x"", ""w"") as f:
  f.write(""asdf"")
4
tf.io.gfile.exists(""/tmp/x"")
True
tf.io.gfile.copy(""/tmp/x"", ""/tmp/y"")
tf.io.gfile.exists(""/tmp/y"")
True
tf.io.gfile.remove(""/tmp/y"")"
"tf.io.gfile.exists(
    path
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import exists
with open(""/tmp/x"", ""w"") as f:
  f.write(""asdf"")
4
tf.io.gfile.exists(""/tmp/x"")
True"
"tf.io.gfile.glob(
    pattern
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import glob
tf.io.gfile.glob(""*.py"")"
"tf.io.gfile.join(
    path, *paths
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import join
>>> tf.io.gfile.join(""gcs://folder"", ""file.py"")
'gcs://folder/file.py'
"
"tf.io.read_file(
    filename, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import read_file
with open(""/tmp/file.txt"", ""w"") as f:
  f.write(""asdf"")
4
tf.io.read_file(""/tmp/file.txt"")
<tf.Tensor: shape=(), dtype=string, numpy=b'asdf'>
<tf.Tensor: shape=(), dtype=string, numpy=b'asdf'>"
"tf.io.serialize_tensor(
    tensor, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import serialize_tensor
t = tf.constant(1)
tf.io.serialize_tensor(t)
<tf.Tensor: shape=(), dtype=string, numpy=b'\x08...\x00'>
<tf.Tensor: shape=(), dtype=string, numpy=b'\x08...\x00'>"
"tf.io.write_graph(
    graph_or_graph_def, logdir, name, as_text=True
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import write_graph
v = tf.Variable(0, name='my_variable')
sess = tf.compat.v1.Session()
tf.io.write_graph(sess.graph_def, '/tmp/my-model', 'train.pbtxt')
"
"tf.is_tensor(
    x
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import is_tensor
if not tf.is_tensor(t):
  t = tf.convert_to_tensor(t)
return t.shape, t.dtype
"
"tf.keras.Input(
    shape=None,
    batch_size=None,
    name=None,
    dtype=None,
    sparse=None,
    tensor=None,
    ragged=None,
    type_spec=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Input
x = Input(shape=(32,))
y = Dense(16, activation='softmax')(x)
model = Model(x, y)
"
"tf.keras.Model(
    *args, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Model
import tensorflow as tf

inputs = tf.keras.Input(shape=(3,))
x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)
outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)
"
"tf.keras.Sequential(
    layers=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Sequential
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8, input_shape=(16,)))
model.add(tf.keras.layers.Dense(4))

model = tf.keras.Sequential()
model.add(tf.keras.Input(shape=(16,)))
model.add(tf.keras.layers.Dense(8))

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8))
model.add(tf.keras.layers.Dense(4))

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8, input_shape=(16,)))
model.add(tf.keras.layers.Dense(4))
len(model.weights)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8))
model.add(tf.keras.layers.Dense(4))
model.build((None, 16))
len(model.weights)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8))
model.add(tf.keras.layers.Dense(1))
model.compile(optimizer='sgd', loss='mse')
model.fit(x, y, batch_size=32, epochs=10)
"
"tf.keras.activations.deserialize(
    name, custom_objects=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import deserialize
tf.keras.activations.deserialize('linear')
 <function linear at 0x1239596a8>
 <function linear at 0x1239596a8>
tf.keras.activations.deserialize('sigmoid')
 <function sigmoid at 0x123959510>
 <function sigmoid at 0x123959510>
tf.keras.activations.deserialize('abcd')
Traceback (most recent call last):
ValueError: Unknown activation function:abcd"
"tf.keras.activations.elu(
    x, alpha=1.0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
import tensorflow as tf
model = tf.keras.Sequential()
model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='elu',
         input_shape=(28, 28, 1)))
model.add(tf.keras.layers.MaxPooling2D((2, 2)))
model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='elu'))
model.add(tf.keras.layers.MaxPooling2D((2, 2)))
model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='elu'))"
"tf.keras.activations.exponential(
    x
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import exponential
a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)
b = tf.keras.activations.exponential(a)
b.numpy()
array([0.04978707,  0.36787945,  1.,  2.7182817 , 20.085537], dtype=float32)"
"tf.keras.activations.gelu(
    x, approximate=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import gelu
x = tf.constant([-3.0, -1.0, 0.0, 1.0, 3.0], dtype=tf.float32)
y = tf.keras.activations.gelu(x)
y.numpy()
array([-0.00404951, -0.15865529,  0.        ,  0.8413447 ,  2.9959507 ],
    dtype=float32)
y = tf.keras.activations.gelu(x, approximate=True)
y.numpy()
array([-0.00363752, -0.15880796,  0.        ,  0.841192  ,  2.9963627 ],
    dtype=float32)"
"tf.keras.activations.get(
    identifier
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get
tf.keras.activations.get('softmax')
 <function softmax at 0x1222a3d90>
 <function softmax at 0x1222a3d90>
tf.keras.activations.get(tf.keras.activations.softmax)
 <function softmax at 0x1222a3d90>
 <function softmax at 0x1222a3d90>
tf.keras.activations.get(None)
 <function linear at 0x1239596a8>
 <function linear at 0x1239596a8>
tf.keras.activations.get(abs)
 <built-in function abs>
 <built-in function abs>
tf.keras.activations.get('abcd')
Traceback (most recent call last):
ValueError: Unknown activation function:abcd"
"tf.keras.activations.hard_sigmoid(
    x
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import hard_sigmoid
a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)
b = tf.keras.activations.hard_sigmoid(a)
b.numpy()
array([0. , 0.3, 0.5, 0.7, 1. ], dtype=float32)"
"tf.keras.activations.linear(
    x
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import linear
a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)
b = tf.keras.activations.linear(a)
b.numpy()
array([-3., -1.,  0.,  1.,  3.], dtype=float32)"
"tf.keras.activations.relu(
    x, alpha=0.0, max_value=None, threshold=0.0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import relu
foo = tf.constant([-10, -5, 0.0, 5, 10], dtype = tf.float32)
tf.keras.activations.relu(foo).numpy()
array([ 0.,  0.,  0.,  5., 10.], dtype=float32)
tf.keras.activations.relu(foo, alpha=0.5).numpy()
array([-5. , -2.5,  0. ,  5. , 10. ], dtype=float32)
tf.keras.activations.relu(foo, max_value=5.).numpy()
array([0., 0., 0., 5., 5.], dtype=float32)
tf.keras.activations.relu(foo, threshold=5.).numpy()
array([-0., -0.,  0.,  0., 10.], dtype=float32)"
"tf.keras.activations.selu(
    x
)
","import pandas as pd
import numpy as np
import tensorflow as tf
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(64, kernel_initializer='lecun_normal',
                                activation='selu'))
model.add(tf.keras.layers.Dense(32, kernel_initializer='lecun_normal',
                                activation='selu'))
model.add(tf.keras.layers.Dense(16, kernel_initializer='lecun_normal',
                                activation='selu'))
model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))"
"tf.keras.activations.serialize(
    activation
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import serialize
tf.keras.activations.serialize(tf.keras.activations.tanh)
'tanh'
tf.keras.activations.serialize(tf.keras.activations.sigmoid)
'sigmoid'
tf.keras.activations.serialize('abcd')
Traceback (most recent call last):
ValueError: ('Cannot serialize', 'abcd')"
"tf.keras.activations.sigmoid(
    x
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sigmoid
a = tf.constant([-20, -1.0, 0.0, 1.0, 20], dtype = tf.float32)
b = tf.keras.activations.sigmoid(a)
b.numpy()
array([2.0611537e-09, 2.6894143e-01, 5.0000000e-01, 7.3105860e-01,
         1.0000000e+00], dtype=float32)"
"tf.keras.activations.softmax(
    x, axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import softmax
inputs = tf.random.normal(shape=(32, 10))
outputs = tf.keras.activations.softmax(inputs)
<tf.Tensor: shape=(), dtype=float32, numpy=1.0000001>
<tf.Tensor: shape=(), dtype=float32, numpy=1.0000001>"
"tf.keras.activations.softplus(
    x
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import softplus
a = tf.constant([-20, -1.0, 0.0, 1.0, 20], dtype = tf.float32)
b = tf.keras.activations.softplus(a)
b.numpy()
array([2.0611537e-09, 3.1326166e-01, 6.9314718e-01, 1.3132616e+00,
         2.0000000e+01], dtype=float32)"
"tf.keras.activations.softsign(
    x
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import softsign
a = tf.constant([-1.0, 0.0, 1.0], dtype = tf.float32)
b = tf.keras.activations.softsign(a)
b.numpy()
array([-0.5,  0. ,  0.5], dtype=float32)"
"tf.keras.activations.swish(
    x
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import swish
a = tf.constant([-20, -1.0, 0.0, 1.0, 20], dtype = tf.float32)
b = tf.keras.activations.swish(a)
b.numpy()
array([-4.1223075e-08, -2.6894143e-01,  0.0000000e+00,  7.3105860e-01,
          2.0000000e+01], dtype=float32)"
"tf.keras.activations.tanh(
    x
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import tanh
a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)
b = tf.keras.activations.tanh(a)
b.numpy()
array([-0.9950547, -0.7615942,  0.,  0.7615942,  0.9950547], dtype=float32)"
"tf.keras.applications.densenet.preprocess_input(
    x, data_format=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import preprocess_input
i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.imagenet_utils.preprocess_input(
    x, data_format=None, mode='caffe'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import preprocess_input
i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.inception_resnet_v2.preprocess_input(
    x, data_format=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import preprocess_input
i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.inception_v3.preprocess_input(
    x, data_format=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import preprocess_input
i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.mobilenet.preprocess_input(
    x, data_format=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import preprocess_input
i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.mobilenet_v2.preprocess_input(
    x, data_format=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import preprocess_input
i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.nasnet.preprocess_input(
    x, data_format=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import preprocess_input
i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.resnet50.preprocess_input(
    x, data_format=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import preprocess_input
i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.resnet50.preprocess_input(
    x, data_format=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import preprocess_input
i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.resnet_v2.preprocess_input(
    x, data_format=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import preprocess_input
i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.vgg16.preprocess_input(
    x, data_format=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import preprocess_input
i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.vgg19.preprocess_input(
    x, data_format=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import preprocess_input
i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.xception.preprocess_input(
    x, data_format=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import preprocess_input
i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.backend.get_uid(
    prefix=''
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get_uid
get_uid('dense')
1
get_uid('dense')
2"
"tf.keras.backend.is_keras_tensor(
    x
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import is_keras_tensor
np_var = np.array([1, 2])
tf.keras.backend.is_keras_tensor(np_var)
Traceback (most recent call last):
ValueError: Unexpectedly found an instance of type
`<class 'numpy.ndarray'>`.
`<class 'numpy.ndarray'>`.
Expected a symbolic tensor instance.
keras_var = tf.keras.backend.variable(np_var)
tf.keras.backend.is_keras_tensor(keras_var)
False
keras_placeholder = tf.keras.backend.placeholder(shape=(2, 4, 5))
tf.keras.backend.is_keras_tensor(keras_placeholder)
True
keras_input = tf.keras.layers.Input([10])
tf.keras.backend.is_keras_tensor(keras_input)
True
keras_layer_output = tf.keras.layers.Dense(10)(keras_input)
tf.keras.backend.is_keras_tensor(keras_layer_output)
True"
"tf.keras.backend.set_epsilon(
    value
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import set_epsilon
tf.keras.backend.epsilon()
1e-07
tf.keras.backend.set_epsilon(1e-5)
tf.keras.backend.epsilon()
1e-05
tf.keras.backend.set_epsilon(1e-7)"
"tf.keras.backend.set_floatx(
    value
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import set_floatx
tf.keras.backend.floatx()
'float32'
tf.keras.backend.set_floatx('float64')
tf.keras.backend.floatx()
'float64'
tf.keras.backend.set_floatx('float32')"
"tf.keras.backend.set_image_data_format(
    data_format
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import set_image_data_format
tf.keras.backend.image_data_format()
'channels_last'
tf.keras.backend.set_image_data_format('channels_first')
tf.keras.backend.image_data_format()
'channels_first'
tf.keras.backend.set_image_data_format('channels_last')"
"tf.keras.callbacks.BackupAndRestore(
    backup_dir,
    save_freq='epoch',
    delete_checkpoint=True,
    save_before_preemption=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import BackupAndRestore
class InterruptingCallback(tf.keras.callbacks.Callback):
  def on_epoch_begin(self, epoch, logs=None):
    if epoch == 4:
      raise RuntimeError('Interrupting!')
callback = tf.keras.callbacks.BackupAndRestore(backup_dir=""/tmp/backup"")
model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
model.compile(tf.keras.optimizers.SGD(), loss='mse')
try:
  model.fit(np.arange(100).reshape(5, 20), np.zeros(5), epochs=10,
            batch_size=1, callbacks=[callback, InterruptingCallback()],
            verbose=0)
except:
  pass
history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),
                    epochs=10, batch_size=1, callbacks=[callback],
                    verbose=0)
len(history.history['loss'])
6"
"tf.keras.callbacks.CSVLogger(
    filename, separator=',', append=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CSVLogger
csv_logger = CSVLogger('training.log')
model.fit(X_train, Y_train, callbacks=[csv_logger])
"
"tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    min_delta=0,
    patience=0,
    verbose=0,
    mode='auto',
    baseline=None,
    restore_best_weights=False,
    start_from_epoch=0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import EarlyStopping
callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)
model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
model.compile(tf.keras.optimizers.SGD(), loss='mse')
history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),
                    epochs=10, batch_size=1, callbacks=[callback],
                    verbose=0)
4"
"tf.keras.callbacks.LambdaCallback(
    on_epoch_begin=None,
    on_epoch_end=None,
    on_batch_begin=None,
    on_batch_end=None,
    on_train_begin=None,
    on_train_end=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LambdaCallback
batch_print_callback = LambdaCallback(
    on_batch_begin=lambda batch,logs: print(batch))

import json
json_log = open('loss_log.json', mode='wt', buffering=1)
json_logging_callback = LambdaCallback(
    on_epoch_end=lambda epoch, logs: json_log.write(
        json.dumps({'epoch': epoch, 'loss': logs['loss']}) + '\n'),
    on_train_end=lambda logs: json_log.close()
)

processes = ...
cleanup_callback = LambdaCallback(
    on_train_end=lambda logs: [
        p.terminate() for p in processes if p.is_alive()])

model.fit(...,
          callbacks=[batch_print_callback,
                     json_logging_callback,
                     cleanup_callback])
"
"tf.keras.callbacks.ModelCheckpoint(
    filepath,
    monitor: str = 'val_loss',
    verbose: int = 0,
    save_best_only: bool = False,
    save_weights_only: bool = False,
    mode: str = 'auto',
    save_freq='epoch',
    options=None,
    initial_value_threshold=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ModelCheckpoint
model.compile(loss=..., optimizer=...,
              metrics=['accuracy'])

EPOCHS = 10
checkpoint_filepath = '/tmp/checkpoint'
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=True,
    monitor='val_accuracy',
    mode='max',
    save_best_only=True)

model.fit(epochs=EPOCHS, callbacks=[model_checkpoint_callback])

model.load_weights(checkpoint_filepath)
"
"tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.1,
    patience=10,
    verbose=0,
    mode='auto',
    min_delta=0.0001,
    cooldown=0,
    min_lr=0,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ReduceLROnPlateau
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,
                              patience=5, min_lr=0.001)
model.fit(X_train, Y_train, callbacks=[reduce_lr])
"
"tf.keras.datasets.cifar100.load_data(
    label_mode='fine'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import load_data
(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()
assert x_train.shape == (50000, 32, 32, 3)
assert x_test.shape == (10000, 32, 32, 3)
assert y_train.shape == (50000, 1)
assert y_test.shape == (10000, 1)
"
"tf.keras.datasets.imdb.get_word_index(
    path='imdb_word_index.json'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get_word_index
start_char = 1
oov_char = 2
index_from = 3
(x_train, _), _ = keras.datasets.imdb.load_data(
    start_char=start_char, oov_char=oov_char, index_from=index_from
)
word_index = keras.datasets.imdb.get_word_index()
inverted_word_index = dict(
    (i + index_from, word) for (word, i) in word_index.items()
)
inverted_word_index[start_char] = ""[START]""
inverted_word_index[oov_char] = ""[OOV]""
decoded_sequence = "" "".join(inverted_word_index[i] for i in x_train[0])
"
"tf.keras.datasets.mnist.load_data(
    path='mnist.npz'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import load_data
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
assert x_train.shape == (60000, 28, 28)
assert x_test.shape == (10000, 28, 28)
assert y_train.shape == (60000,)
assert y_test.shape == (10000,)
"
"tf.keras.dtensor.experimental.LayoutMap(
    mesh=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LayoutMap
map = LayoutMap(mesh=None)
map['.*dense.*kernel'] = layout_2d
map['.*dense.*bias'] = layout_1d
map['.*conv2d.*kernel'] = layout_4d
map['.*conv2d.*bias'] = layout_1d

"
"tf.keras.dtensor.experimental.optimizers.RMSprop(
    learning_rate=0.001,
    rho=0.9,
    momentum=0.0,
    epsilon=1e-07,
    centered=False,
    gradients_clip_option=None,
    ema_option=None,
    jit_compile=False,
    name='RMSprop',
    mesh=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RMSprop
opt = tf.keras.optimizers.experimental.RMSprop(learning_rate=0.1)
var1 = tf.Variable(10.0)
opt.minimize(loss, [var1])
var1.numpy()
9.683772"
"tf.keras.estimator.model_to_estimator(
    keras_model=None,
    keras_model_path=None,
    custom_objects=None,
    model_dir=None,
    config=None,
    checkpoint_format='checkpoint',
    metric_names_map=None,
    export_outputs=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import model_to_estimator
keras_model = tf.keras.Model(...)
keras_model.compile(...)

estimator = tf.keras.estimator.model_to_estimator(keras_model)

def input_fn():
  return dataset_ops.Dataset.from_tensors(
      ({'features': features, 'sample_weights': sample_weights},
       targets))

estimator.train(input_fn, steps=1)
"
"tf.keras.optimizers.schedules.CosineDecayRestarts(
    initial_learning_rate,
    first_decay_steps,
    t_mul=2.0,
    m_mul=1.0,
    alpha=0.0,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CosineDecayRestarts
first_decay_steps = 1000
lr_decayed_fn = (
  tf.keras.optimizers.schedules.CosineDecayRestarts(
      initial_learning_rate,
      first_decay_steps))
"
"tf.keras.experimental.LinearModel(
    units=1,
    activation=None,
    use_bias=True,
    kernel_initializer='zeros',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearModel
model = LinearModel()
model.compile(optimizer='sgd', loss='mse')
model.fit(x, y, epochs=epochs)
"
"tf.keras.experimental.SequenceFeatures(
    feature_columns, trainable=True, name=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SequenceFeatures

import tensorflow as tf

training = True
rating = tf.feature_column.sequence_numeric_column('rating')
watches = tf.feature_column.sequence_categorical_column_with_identity(
    'watches', num_buckets=1000)
watches_embedding = tf.feature_column.embedding_column(watches,
                                            dimension=10)
columns = [rating, watches_embedding]

features = {
 'rating': tf.sparse.from_dense([[1.0,1.1, 0, 0, 0],
                                             [2.0,2.1,2.2, 2.3, 2.5]]),
 'watches': tf.sparse.from_dense([[2, 85, 0, 0, 0],[33,78, 2, 73, 1]])
}

sequence_input_layer = tf.keras.experimental.SequenceFeatures(columns)
sequence_input, sequence_length = sequence_input_layer(
   features, training=training)
sequence_length_mask = tf.sequence_mask(sequence_length)
hidden_size = 32
rnn_cell = tf.keras.layers.SimpleRNNCell(hidden_size)
rnn_layer = tf.keras.layers.RNN(rnn_cell)
outputs, state = rnn_layer(sequence_input, mask=sequence_length_mask)
"
"tf.keras.experimental.WideDeepModel(
    linear_model, dnn_model, activation=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import WideDeepModel
linear_model = LinearModel()
dnn_model = keras.Sequential([keras.layers.Dense(units=64),
                             keras.layers.Dense(units=1)])
combined_model = WideDeepModel(linear_model, dnn_model)
combined_model.compile(optimizer=['sgd', 'adam'],
                       loss='mse', metrics=['mse'])
combined_model.fit([linear_inputs, dnn_inputs], y, epochs)
dataset = tf.data.Dataset.from_tensors(([linear_inputs, dnn_inputs], y))
combined_model.fit(dataset, epochs)
"
"tf.keras.initializers.Constant(
    value=0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Constant
initializer = tf.keras.initializers.Constant(3.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.GlorotNormal(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GlorotNormal
initializer = tf.keras.initializers.GlorotNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.GlorotUniform(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GlorotUniform
initializer = tf.keras.initializers.GlorotUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.HeNormal(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import HeNormal
initializer = tf.keras.initializers.HeNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.HeUniform(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import HeUniform
initializer = tf.keras.initializers.HeUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.Identity(
    gain=1.0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Identity
initializer = tf.keras.initializers.Identity()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.LecunNormal(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LecunNormal
initializer = tf.keras.initializers.LecunNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.LecunUniform(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LecunUniform
initializer = tf.keras.initializers.LecunUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.Orthogonal(
    gain=1.0, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Orthogonal
initializer = tf.keras.initializers.Orthogonal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.RandomNormal(
    mean=0.0, stddev=0.05, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RandomNormal
initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.RandomUniform(
    minval=-0.05, maxval=0.05, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RandomUniform
initializer = tf.keras.initializers.RandomUniform(minval=0., maxval=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.TruncatedNormal(
    mean=0.0, stddev=0.05, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TruncatedNormal
initializer = tf.keras.initializers.TruncatedNormal(mean=0., stddev=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.VarianceScaling(
    scale=1.0,
    mode='fan_in',
    distribution='truncated_normal',
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import VarianceScaling
initializer = tf.keras.initializers.VarianceScaling(
scale=0.1, mode='fan_in', distribution='uniform')
values = initializer(shape=(2, 2))"
"tf.keras.initializers.Constant(
    value=0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Constant
initializer = tf.keras.initializers.Constant(3.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.GlorotNormal(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GlorotNormal
initializer = tf.keras.initializers.GlorotNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.GlorotUniform(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GlorotUniform
initializer = tf.keras.initializers.GlorotUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.HeNormal(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import HeNormal
initializer = tf.keras.initializers.HeNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.HeUniform(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import HeUniform
initializer = tf.keras.initializers.HeUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.Identity(
    gain=1.0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Identity
initializer = tf.keras.initializers.Identity()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.LecunNormal(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LecunNormal
initializer = tf.keras.initializers.LecunNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.LecunUniform(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LecunUniform
initializer = tf.keras.initializers.LecunUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.Orthogonal(
    gain=1.0, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Orthogonal
initializer = tf.keras.initializers.Orthogonal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.RandomNormal(
    mean=0.0, stddev=0.05, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RandomNormal
initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.RandomUniform(
    minval=-0.05, maxval=0.05, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RandomUniform
initializer = tf.keras.initializers.RandomUniform(minval=0., maxval=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.TruncatedNormal(
    mean=0.0, stddev=0.05, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TruncatedNormal
initializer = tf.keras.initializers.TruncatedNormal(mean=0., stddev=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.VarianceScaling(
    scale=1.0,
    mode='fan_in',
    distribution='truncated_normal',
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import VarianceScaling
initializer = tf.keras.initializers.VarianceScaling(
scale=0.1, mode='fan_in', distribution='uniform')
values = initializer(shape=(2, 2))"
"tf.keras.layers.AbstractRNNCell(
    trainable=True, name=None, dtype=None, dynamic=False, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
  class MinimalRNNCell(AbstractRNNCell):

    def __init__(self, units, **kwargs):
      self.units = units
      super(MinimalRNNCell, self).__init__(**kwargs)

    @property
    def state_size(self):
      return self.units

    def build(self, input_shape):
      self.kernel = self.add_weight(shape=(input_shape[-1], self.units),
                                    initializer='uniform',
                                    name='kernel')
      self.recurrent_kernel = self.add_weight(
          shape=(self.units, self.units),
          initializer='uniform',
          name='recurrent_kernel')
      self.built = True

    def call(self, inputs, states):
      prev_output = states[0]
      h = backend.dot(inputs, self.kernel)
      output = h + backend.dot(prev_output, self.recurrent_kernel)
      return output, output
"
"tf.keras.layers.Activation(
    activation, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Activation
layer = tf.keras.layers.Activation('relu')
output = layer([-3.0, -1.0, 0.0, 2.0])
list(output.numpy())
[0.0, 0.0, 0.0, 2.0]
layer = tf.keras.layers.Activation(tf.nn.relu)
output = layer([-3.0, -1.0, 0.0, 2.0])
list(output.numpy())
[0.0, 0.0, 0.0, 2.0]"
"tf.keras.layers.Add(
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Add
input_shape = (2, 3, 4)
x1 = tf.random.normal(input_shape)
x2 = tf.random.normal(input_shape)
y = tf.keras.layers.Add()([x1, x2])
print(y.shape)
(2, 3, 4)"
"tf.keras.layers.AdditiveAttention(
    use_scale=True, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import AdditiveAttention
query_input = tf.keras.Input(shape=(None,), dtype='int32')
value_input = tf.keras.Input(shape=(None,), dtype='int32')

token_embedding = tf.keras.layers.Embedding(max_tokens, dimension)
query_embeddings = token_embedding(query_input)
value_embeddings = token_embedding(value_input)

cnn_layer = tf.keras.layers.Conv1D(
    filters=100,
    kernel_size=4,
    padding='same')
query_seq_encoding = cnn_layer(query_embeddings)
value_seq_encoding = cnn_layer(value_embeddings)

query_value_attention_seq = tf.keras.layers.AdditiveAttention()(
    [query_seq_encoding, value_seq_encoding])

query_encoding = tf.keras.layers.GlobalAveragePooling1D()(
    query_seq_encoding)
query_value_attention = tf.keras.layers.GlobalAveragePooling1D()(
    query_value_attention_seq)

input_layer = tf.keras.layers.Concatenate()(
    [query_encoding, query_value_attention])

"
"tf.keras.layers.Attention(
    use_scale=False, score_mode='dot', **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Attention
query_input = tf.keras.Input(shape=(None,), dtype='int32')
value_input = tf.keras.Input(shape=(None,), dtype='int32')

token_embedding = tf.keras.layers.Embedding(input_dim=1000, output_dim=64)
query_embeddings = token_embedding(query_input)
value_embeddings = token_embedding(value_input)

cnn_layer = tf.keras.layers.Conv1D(
    filters=100,
    kernel_size=4,
    padding='same')
query_seq_encoding = cnn_layer(query_embeddings)
value_seq_encoding = cnn_layer(value_embeddings)

query_value_attention_seq = tf.keras.layers.Attention()(
    [query_seq_encoding, value_seq_encoding])

query_encoding = tf.keras.layers.GlobalAveragePooling1D()(
    query_seq_encoding)
query_value_attention = tf.keras.layers.GlobalAveragePooling1D()(
    query_value_attention_seq)

input_layer = tf.keras.layers.Concatenate()(
    [query_encoding, query_value_attention])

"
"tf.keras.layers.Average(
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Average
x1 = np.ones((2, 2))
x2 = np.zeros((2, 2))
y = tf.keras.layers.Average()([x1, x2])
y.numpy().tolist()
[[0.5, 0.5], [0.5, 0.5]]"
"tf.keras.layers.AveragePooling1D(
    pool_size=2,
    strides=None,
    padding='valid',
    data_format='channels_last',
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import AveragePooling1D
x = tf.constant([1., 2., 3., 4., 5.])
x = tf.reshape(x, [1, 5, 1])
x
<tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=
<tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=
  array([[[1.],
          [2.],
          [3.],
          [4.],
          [5.]], dtype=float32)>
avg_pool_1d = tf.keras.layers.AveragePooling1D(pool_size=2,
   strides=1, padding='valid')
avg_pool_1d(x)
<tf.Tensor: shape=(1, 4, 1), dtype=float32, numpy=
<tf.Tensor: shape=(1, 4, 1), dtype=float32, numpy=
array([[[1.5],
        [2.5],
        [3.5],
        [4.5]]], dtype=float32)>"
"tf.keras.layers.AveragePooling2D(
    pool_size=(2, 2),
    strides=None,
    padding='valid',
    data_format=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import AveragePooling2D
x = tf.constant([[1., 2., 3.],
                 [4., 5., 6.],
                 [7., 8., 9.]])
x = tf.reshape(x, [1, 3, 3, 1])
avg_pool_2d = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),
   strides=(1, 1), padding='valid')
avg_pool_2d(x)
<tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=
<tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=
  array([[[[3.],
           [4.]],
          [[6.],
           [7.]]]], dtype=float32)>"
"tf.keras.layers.AveragePooling3D(
    pool_size=(2, 2, 2),
    strides=None,
    padding='valid',
    data_format=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import AveragePooling3D
depth = 30
height = 30
width = 30
input_channels = 3

inputs = tf.keras.Input(shape=(depth, height, width, input_channels))
layer = tf.keras.layers.AveragePooling3D(pool_size=3)
"
"tf.keras.layers.AveragePooling1D(
    pool_size=2,
    strides=None,
    padding='valid',
    data_format='channels_last',
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import AveragePooling1D
x = tf.constant([1., 2., 3., 4., 5.])
x = tf.reshape(x, [1, 5, 1])
x
<tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=
<tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=
  array([[[1.],
          [2.],
          [3.],
          [4.],
          [5.]], dtype=float32)>
avg_pool_1d = tf.keras.layers.AveragePooling1D(pool_size=2,
   strides=1, padding='valid')
avg_pool_1d(x)
<tf.Tensor: shape=(1, 4, 1), dtype=float32, numpy=
<tf.Tensor: shape=(1, 4, 1), dtype=float32, numpy=
array([[[1.5],
        [2.5],
        [3.5],
        [4.5]]], dtype=float32)>"
"tf.keras.layers.AveragePooling2D(
    pool_size=(2, 2),
    strides=None,
    padding='valid',
    data_format=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import AveragePooling2D
x = tf.constant([[1., 2., 3.],
                 [4., 5., 6.],
                 [7., 8., 9.]])
x = tf.reshape(x, [1, 3, 3, 1])
avg_pool_2d = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),
   strides=(1, 1), padding='valid')
avg_pool_2d(x)
<tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=
<tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=
  array([[[[3.],
           [4.]],
          [[6.],
           [7.]]]], dtype=float32)>"
"tf.keras.layers.AveragePooling3D(
    pool_size=(2, 2, 2),
    strides=None,
    padding='valid',
    data_format=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import AveragePooling3D
depth = 30
height = 30
width = 30
input_channels = 3

inputs = tf.keras.Input(shape=(depth, height, width, input_channels))
layer = tf.keras.layers.AveragePooling3D(pool_size=3)
"
"tf.keras.layers.Bidirectional(
    layer,
    merge_mode='concat',
    weights=None,
    backward_layer=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Bidirectional
model = Sequential()
model.add(Bidirectional(LSTM(10, return_sequences=True),
                             input_shape=(5, 10)))
model.add(Bidirectional(LSTM(10)))
model.add(Dense(5))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

model = Sequential()
forward_layer = LSTM(10, return_sequences=True)
backward_layer = LSTM(10, activation='relu', return_sequences=True,
                      go_backwards=True)
model.add(Bidirectional(forward_layer, backward_layer=backward_layer,
                        input_shape=(5, 10)))
model.add(Dense(5))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
"
"tf.keras.layers.CategoryEncoding(
    num_tokens=None, output_mode='multi_hot', sparse=False, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CategoryEncoding
layer = tf.keras.layers.CategoryEncoding(
          num_tokens=4, output_mode=""one_hot"")
layer([3, 2, 0, 1])
<tf.Tensor: shape=(4, 4), dtype=float32, numpy=
<tf.Tensor: shape=(4, 4), dtype=float32, numpy=
  array([[0., 0., 0., 1.],
         [0., 0., 1., 0.],
         [1., 0., 0., 0.],
         [0., 1., 0., 0.]], dtype=float32)>"
"tf.keras.layers.Concatenate(
    axis=-1, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Concatenate
x = np.arange(20).reshape(2, 2, 5)
print(x)
[[[ 0  1  2  3  4]
  [ 5  6  7  8  9]]
 [[10 11 12 13 14]
  [15 16 17 18 19]]]
y = np.arange(20, 30).reshape(2, 1, 5)
print(y)
[[[20 21 22 23 24]]
 [[25 26 27 28 29]]]
tf.keras.layers.Concatenate(axis=1)([x, y])
<tf.Tensor: shape=(2, 3, 5), dtype=int64, numpy=
<tf.Tensor: shape=(2, 3, 5), dtype=int64, numpy=
array([[[ 0,  1,  2,  3,  4],
        [ 5,  6,  7,  8,  9],
        [20, 21, 22, 23, 24]],
       [[10, 11, 12, 13, 14],
        [15, 16, 17, 18, 19],
        [25, 26, 27, 28, 29]]])>"
"tf.keras.layers.Conv1D(
    filters,
    kernel_size,
    strides=1,
    padding='valid',
    data_format='channels_last',
    dilation_rate=1,
    groups=1,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Conv1D
input_shape = (4, 10, 128)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv1D(
32, 3, activation='relu',input_shape=input_shape[1:])(x)
print(y.shape)
(4, 8, 32)"
"tf.keras.layers.Conv2D(
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    data_format=None,
    dilation_rate=(1, 1),
    groups=1,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Conv2D
input_shape = (4, 28, 28, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv2D(
2, 3, activation='relu', input_shape=input_shape[1:])(x)
print(y.shape)
(4, 26, 26, 2)"
"tf.keras.layers.Conv3D(
    filters,
    kernel_size,
    strides=(1, 1, 1),
    padding='valid',
    data_format=None,
    dilation_rate=(1, 1, 1),
    groups=1,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Conv3D
input_shape =(4, 28, 28, 28, 1)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv3D(
2, 3, activation='relu', input_shape=input_shape[1:])(x)
print(y.shape)
(4, 26, 26, 26, 2)"
"tf.keras.layers.Conv1D(
    filters,
    kernel_size,
    strides=1,
    padding='valid',
    data_format='channels_last',
    dilation_rate=1,
    groups=1,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Conv1D
input_shape = (4, 10, 128)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv1D(
32, 3, activation='relu',input_shape=input_shape[1:])(x)
print(y.shape)
(4, 8, 32)"
"tf.keras.layers.Conv2D(
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    data_format=None,
    dilation_rate=(1, 1),
    groups=1,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Conv2D
input_shape = (4, 28, 28, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv2D(
2, 3, activation='relu', input_shape=input_shape[1:])(x)
print(y.shape)
(4, 26, 26, 2)"
"tf.keras.layers.Conv3D(
    filters,
    kernel_size,
    strides=(1, 1, 1),
    padding='valid',
    data_format=None,
    dilation_rate=(1, 1, 1),
    groups=1,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Conv3D
input_shape =(4, 28, 28, 28, 1)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv3D(
2, 3, activation='relu', input_shape=input_shape[1:])(x)
print(y.shape)
(4, 26, 26, 26, 2)"
"tf.keras.layers.Cropping1D(
    cropping=(1, 1), **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Cropping1D
input_shape = (2, 3, 2)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
print(x)
[[[ 0  1]
  [ 2  3]
  [ 4  5]]
 [[ 6  7]
  [ 8  9]
  [10 11]]]
y = tf.keras.layers.Cropping1D(cropping=1)(x)
print(y)
tf.Tensor(
  [[[2 3]]
   [[8 9]]], shape=(2, 1, 2), dtype=int64)"
"tf.keras.layers.Cropping2D(
    cropping=((0, 0), (0, 0)), data_format=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Cropping2D
input_shape = (2, 28, 28, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
y = tf.keras.layers.Cropping2D(cropping=((2, 2), (4, 4)))(x)
print(y.shape)
(2, 24, 20, 3)"
"tf.keras.layers.Cropping3D(
    cropping=((1, 1), (1, 1), (1, 1)), data_format=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Cropping3D
input_shape = (2, 28, 28, 10, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
y = tf.keras.layers.Cropping3D(cropping=(2, 4, 2))(x)
print(y.shape)
(2, 24, 20, 6, 3)"
"tf.keras.layers.Dense(
    units,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Dense
model = tf.keras.models.Sequential()
model.add(tf.keras.Input(shape=(16,)))
model.add(tf.keras.layers.Dense(32, activation='relu'))
model.add(tf.keras.layers.Dense(32))
model.output_shape
(None, 32)"
"tf.keras.layers.DenseFeatures(
    feature_columns, trainable=True, name=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import DenseFeatures
price = tf.feature_column.numeric_column('price')
keywords_embedded = tf.feature_column.embedding_column(
    tf.feature_column.categorical_column_with_hash_bucket(""keywords"",
                                                          10000),
    dimensions=16)
columns = [price, keywords_embedded, ...]
feature_layer = tf.keras.layers.DenseFeatures(columns)

features = tf.io.parse_example(
    ..., features=tf.feature_column.make_parse_example_spec(columns))
dense_tensor = feature_layer(features)
for units in [128, 64, 32]:
  dense_tensor = tf.keras.layers.Dense(units, activation='relu')(
    dense_tensor)
prediction = tf.keras.layers.Dense(1)(dense_tensor)
"
"tf.keras.layers.Discretization(
    bin_boundaries=None,
    num_bins=None,
    epsilon=0.01,
    output_mode='int',
    sparse=False,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Discretization
>>> input = np.array([[-1.5, 1.0, 3.4, .5], [0.0, 3.0, 1.3, 0.0]])
>>> layer = tf.keras.layers.Discretization(bin_boundaries=[0., 1., 2.])
>>> layer(input)
<tf.Tensor: shape=(2, 4), dtype=int64, numpy=
<tf.Tensor: shape=(2, 4), dtype=int64, numpy=
array([[0, 2, 3, 1],
       [1, 3, 2, 1]])>
"
"tf.keras.layers.Dot(
    axes, normalize=False, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Dot
x = np.arange(10).reshape(1, 5, 2)
print(x)
[[[0 1]
  [2 3]
  [4 5]
  [6 7]
  [8 9]]]
y = np.arange(10, 20).reshape(1, 2, 5)
print(y)
[[[10 11 12 13 14]
  [15 16 17 18 19]]]
tf.keras.layers.Dot(axes=(1, 2))([x, y])
<tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=
<tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=
array([[[260, 360],
        [320, 445]]])>"
"tf.keras.layers.Dropout(
    rate, noise_shape=None, seed=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Dropout
tf.random.set_seed(0)
layer = tf.keras.layers.Dropout(.2, input_shape=(2,))
data = np.arange(10).reshape(5, 2).astype(np.float32)
print(data)
[[0. 1.]
 [2. 3.]
 [4. 5.]
 [6. 7.]
 [8. 9.]]
outputs = layer(data, training=True)
print(outputs)
tf.Tensor(
[[ 0.    1.25]
 [ 2.5   3.75]
 [ 5.    6.25]
 [ 7.5   8.75]
 [10.    0.  ]], shape=(5, 2), dtype=float32)"
"tf.keras.layers.EinsumDense(
    equation,
    output_shape,
    activation=None,
    bias_axes=None,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import EinsumDense
layer = tf.keras.layers.EinsumDense(""ab,bc->ac"",
                                    output_shape=64,
                                    bias_axes=""c"")
input_tensor = tf.keras.Input(shape=[32])
output_tensor = layer(input_tensor)
output_tensor
<... shape=(None, 64) dtype=...>
<... shape=(None, 64) dtype=...>"
"tf.keras.layers.Embedding(
    input_dim,
    output_dim,
    embeddings_initializer='uniform',
    embeddings_regularizer=None,
    activity_regularizer=None,
    embeddings_constraint=None,
    mask_zero=False,
    input_length=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Embedding
model = tf.keras.Sequential()
model.add(tf.keras.layers.Embedding(1000, 64, input_length=10))
input_array = np.random.randint(1000, size=(32, 10))
model.compile('rmsprop', 'mse')
output_array = model.predict(input_array)
print(output_array.shape)
(32, 10, 64)"
"tf.keras.layers.GRU(
    units,
    activation='tanh',
    recurrent_activation='sigmoid',
    use_bias=True,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros',
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    dropout=0.0,
    recurrent_dropout=0.0,
    return_sequences=False,
    return_state=False,
    go_backwards=False,
    stateful=False,
    unroll=False,
    time_major=False,
    reset_after=True,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GRU
inputs = tf.random.normal([32, 10, 8])
gru = tf.keras.layers.GRU(4)
output = gru(inputs)
print(output.shape)
(32, 4)
gru = tf.keras.layers.GRU(4, return_sequences=True, return_state=True)
whole_sequence_output, final_state = gru(inputs)
print(whole_sequence_output.shape)
(32, 10, 4)
print(final_state.shape)
(32, 4)"
"tf.keras.layers.GRUCell(
    units,
    activation='tanh',
    recurrent_activation='sigmoid',
    use_bias=True,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros',
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    dropout=0.0,
    recurrent_dropout=0.0,
    reset_after=True,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GRUCell
inputs = tf.random.normal([32, 10, 8])
rnn = tf.keras.layers.RNN(tf.keras.layers.GRUCell(4))
output = rnn(inputs)
print(output.shape)
(32, 4)
rnn = tf.keras.layers.RNN(
   tf.keras.layers.GRUCell(4),
   return_sequences=True,
   return_state=True)
whole_sequence_output, final_state = rnn(inputs)
print(whole_sequence_output.shape)
(32, 10, 4)
print(final_state.shape)
(32, 4)"
"tf.keras.layers.GlobalAveragePooling1D(
    data_format='channels_last', **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GlobalAveragePooling1D
input_shape = (2, 3, 4)
x = tf.random.normal(input_shape)
y = tf.keras.layers.GlobalAveragePooling1D()(x)
print(y.shape)
(2, 4)"
"tf.keras.layers.GlobalAveragePooling2D(
    data_format=None, keepdims=False, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GlobalAveragePooling2D
input_shape = (2, 4, 5, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.GlobalAveragePooling2D()(x)
print(y.shape)
(2, 3)"
"tf.keras.layers.GlobalAveragePooling1D(
    data_format='channels_last', **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GlobalAveragePooling1D
input_shape = (2, 3, 4)
x = tf.random.normal(input_shape)
y = tf.keras.layers.GlobalAveragePooling1D()(x)
print(y.shape)
(2, 4)"
"tf.keras.layers.GlobalAveragePooling2D(
    data_format=None, keepdims=False, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GlobalAveragePooling2D
input_shape = (2, 4, 5, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.GlobalAveragePooling2D()(x)
print(y.shape)
(2, 3)"
"tf.keras.layers.GlobalMaxPool2D(
    data_format=None, keepdims=False, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GlobalMaxPool2D
input_shape = (2, 4, 5, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.GlobalMaxPool2D()(x)
print(y.shape)
(2, 3)"
"tf.keras.layers.GlobalMaxPool2D(
    data_format=None, keepdims=False, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GlobalMaxPool2D
input_shape = (2, 4, 5, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.GlobalMaxPool2D()(x)
print(y.shape)
(2, 3)"
"tf.keras.layers.Hashing(
    num_bins,
    mask_value=None,
    salt=None,
    output_mode='int',
    sparse=False,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Hashing
layer = tf.keras.layers.Hashing(num_bins=3)
inp = [['A'], ['B'], ['C'], ['D'], ['E']]
layer(inp)
<tf.Tensor: shape=(5, 1), dtype=int64, numpy=
<tf.Tensor: shape=(5, 1), dtype=int64, numpy=
  array([[1],
         [0],
         [1],
         [1],
         [2]])>"
"tf.keras.Input(
    shape=None,
    batch_size=None,
    name=None,
    dtype=None,
    sparse=None,
    tensor=None,
    ragged=None,
    type_spec=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Input
x = Input(shape=(32,))
y = Dense(16, activation='softmax')(x)
model = Model(x, y)
"
"tf.keras.layers.InputLayer(
    input_shape=None,
    batch_size=None,
    dtype=None,
    input_tensor=None,
    sparse=None,
    name=None,
    ragged=None,
    type_spec=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import InputLayer
model = tf.keras.Sequential([
  tf.keras.layers.InputLayer(input_shape=(4,)),
  tf.keras.layers.Dense(8)])
model.compile(tf.keras.optimizers.RMSprop(0.001), loss='mse')
model.fit(np.zeros((10, 4)),
          np.ones((10, 8)))

model = tf.keras.Sequential([
  tf.keras.layers.Dense(8, input_shape=(4,))])
model.compile(tf.keras.optimizers.RMSprop(0.001), loss='mse')
model.fit(np.zeros((10, 4)),
          np.ones((10, 8)))
"
"tf.keras.layers.InputSpec(
    dtype=None,
    shape=None,
    ndim=None,
    max_ndim=None,
    min_ndim=None,
    axes=None,
    allow_last_axis_squeeze=False,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import InputSpec
class MyLayer(Layer):
    def __init__(self):
        super(MyLayer, self).__init__()
        self.input_spec = InputSpec(
            shape=(None, 28, 28, 1),
            allow_last_axis_squeeze=True)
"
"tf.keras.layers.IntegerLookup(
    max_tokens=None,
    num_oov_indices=1,
    mask_token=None,
    oov_token=-1,
    vocabulary=None,
    vocabulary_dtype='int64',
    idf_weights=None,
    invert=False,
    output_mode='int',
    sparse=False,
    pad_to_max_tokens=False,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import IntegerLookup
vocab = [12, 36, 1138, 42]
layer = tf.keras.layers.IntegerLookup(vocabulary=vocab)
layer(data)
<tf.Tensor: shape=(2, 3), dtype=int64, numpy=
<tf.Tensor: shape=(2, 3), dtype=int64, numpy=
array([[1, 3, 4],
       [4, 0, 2]])>"
"tf.keras.layers.LSTM(
    units,
    activation='tanh',
    recurrent_activation='sigmoid',
    use_bias=True,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros',
    unit_forget_bias=True,
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    dropout=0.0,
    recurrent_dropout=0.0,
    return_sequences=False,
    return_state=False,
    go_backwards=False,
    stateful=False,
    time_major=False,
    unroll=False,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LSTM
inputs = tf.random.normal([32, 10, 8])
lstm = tf.keras.layers.LSTM(4)
output = lstm(inputs)
print(output.shape)
(32, 4)
lstm = tf.keras.layers.LSTM(4, return_sequences=True, return_state=True)
whole_seq_output, final_memory_state, final_carry_state = lstm(inputs)
print(whole_seq_output.shape)
(32, 10, 4)
print(final_memory_state.shape)
(32, 4)
print(final_carry_state.shape)
(32, 4)"
"tf.keras.layers.LSTMCell(
    units,
    activation='tanh',
    recurrent_activation='sigmoid',
    use_bias=True,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros',
    unit_forget_bias=True,
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    dropout=0.0,
    recurrent_dropout=0.0,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LSTMCell
inputs = tf.random.normal([32, 10, 8])
rnn = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(4))
output = rnn(inputs)
print(output.shape)
(32, 4)
rnn = tf.keras.layers.RNN(
   tf.keras.layers.LSTMCell(4),
   return_sequences=True,
   return_state=True)
whole_seq_output, final_memory_state, final_carry_state = rnn(inputs)
print(whole_seq_output.shape)
(32, 10, 4)
print(final_memory_state.shape)
(32, 4)
print(final_carry_state.shape)
(32, 4)"
"tf.keras.layers.Lambda(
    function, output_shape=None, mask=None, arguments=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Lambda
model.add(Lambda(lambda x: x ** 2))
"
"tf.keras.layers.Layer(
    trainable=True, name=None, dtype=None, dynamic=False, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
class SimpleDense(Layer):

  def __init__(self, units=32):
      super(SimpleDense, self).__init__()
      self.units = units

    w_init = tf.random_normal_initializer()
    self.w = tf.Variable(
        initial_value=w_init(shape=(input_shape[-1], self.units),
                             dtype='float32'),
        trainable=True)
    b_init = tf.zeros_initializer()
    self.b = tf.Variable(
        initial_value=b_init(shape=(self.units,), dtype='float32'),
        trainable=True)

      return tf.matmul(inputs, self.w) + self.b

linear_layer = SimpleDense(4)

y = linear_layer(tf.ones((2, 2)))
assert len(linear_layer.weights) == 2

assert len(linear_layer.trainable_weights) == 2
"
"tf.keras.layers.LocallyConnected1D(
    filters,
    kernel_size,
    strides=1,
    padding='valid',
    data_format=None,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    implementation=1,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LocallyConnected1D
    model = Sequential()
    model.add(LocallyConnected1D(64, 3, input_shape=(10, 32)))
    model.add(LocallyConnected1D(32, 3))
"
"tf.keras.layers.LocallyConnected2D(
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    data_format=None,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    implementation=1,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LocallyConnected2D
    32x32 image
    model = Sequential()
    model.add(LocallyConnected2D(64, (3, 3), input_shape=(32, 32, 3)))
    parameters

    model.add(LocallyConnected2D(32, (3, 3)))
"
"tf.keras.layers.Masking(
    mask_value=0.0, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Masking
samples, timesteps, features = 32, 10, 8
inputs = np.random.random([samples, timesteps, features]).astype(np.float32)
inputs[:, 3, :] = 0.
inputs[:, 5, :] = 0.

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Masking(mask_value=0.,
                                  input_shape=(timesteps, features)))
model.add(tf.keras.layers.LSTM(32))

output = model(inputs)
"
"tf.keras.layers.Maximum(
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Maximum
tf.keras.layers.Maximum()([np.arange(5).reshape(5, 1),
                           np.arange(5, 10).reshape(5, 1)])
<tf.Tensor: shape=(5, 1), dtype=int64, numpy=
<tf.Tensor: shape=(5, 1), dtype=int64, numpy=
array([[5],
     [6],
     [7],
     [8],
     [9]])>"
"tf.keras.layers.Minimum(
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Minimum
tf.keras.layers.Minimum()([np.arange(5).reshape(5, 1),
                           np.arange(5, 10).reshape(5, 1)])
<tf.Tensor: shape=(5, 1), dtype=int64, numpy=
<tf.Tensor: shape=(5, 1), dtype=int64, numpy=
array([[0],
     [1],
     [2],
     [3],
     [4]])>"
"tf.keras.layers.MultiHeadAttention(
    num_heads,
    key_dim,
    value_dim=None,
    dropout=0.0,
    use_bias=True,
    output_shape=None,
    attention_axes=None,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MultiHeadAttention
layer = MultiHeadAttention(num_heads=2, key_dim=2)
target = tf.keras.Input(shape=[8, 16])
source = tf.keras.Input(shape=[4, 16])
output_tensor, weights = layer(target, source,
                               return_attention_scores=True)
print(output_tensor.shape)
(None, 8, 16)
print(weights.shape)
(None, 2, 8, 4)"
"tf.keras.layers.Multiply(
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Multiply
tf.keras.layers.Multiply()([np.arange(5).reshape(5, 1),
                            np.arange(5, 10).reshape(5, 1)])
<tf.Tensor: shape=(5, 1), dtype=int64, numpy=
<tf.Tensor: shape=(5, 1), dtype=int64, numpy=
array([[ 0],
     [ 6],
     [14],
     [24],
     [36]])>"
"tf.keras.layers.Normalization(
    axis=-1, mean=None, variance=None, invert=False, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Normalization
adapt_data = np.array([1., 2., 3., 4., 5.], dtype='float32')
input_data = np.array([1., 2., 3.], dtype='float32')
layer = tf.keras.layers.Normalization(axis=None)
layer.adapt(adapt_data)
layer(input_data)
<tf.Tensor: shape=(3,), dtype=float32, numpy=
<tf.Tensor: shape=(3,), dtype=float32, numpy=
array([-1.4142135, -0.70710677, 0.], dtype=float32)>"
"tf.keras.layers.Permute(
    dims, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Permute
model = Sequential()
model.add(Permute((2, 1), input_shape=(10, 64)))
"
"tf.keras.layers.RandomBrightness(
    factor, value_range=(0, 255), seed=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RandomBrightness
random_bright = tf.keras.layers.RandomBrightness(factor=0.2)

image = [[[1, 2, 3], [4 ,5 ,6]], [[7, 8, 9], [10, 11, 12]]]

output = random_bright(image, training=True)

tf.Tensor([[[26.5, 27.5, 28.5]
            [29.5, 30.5, 31.5]]
           [[32.5, 33.5, 34.5]
            [35.5, 36.5, 37.5]]],
          shape=(2, 2, 3), dtype=int64)
"
"tf.keras.layers.RandomZoom(
    height_factor,
    width_factor=None,
    fill_mode='reflect',
    interpolation='bilinear',
    seed=None,
    fill_value=0.0,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RandomZoom
input_img = np.random.random((32, 224, 224, 3))
layer = tf.keras.layers.RandomZoom(.5, .2)
out_img = layer(input_img)
out_img.shape
TensorShape([32, 224, 224, 3])"
"tf.keras.layers.RepeatVector(
    n, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RepeatVector
model = Sequential()
model.add(Dense(32, input_dim=32))

model.add(RepeatVector(3))
"
"tf.keras.layers.Reshape(
    target_shape, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Reshape
model = tf.keras.Sequential()
model.add(tf.keras.layers.Reshape((3, 4), input_shape=(12,)))
model.output_shape
(None, 3, 4)"
"tf.keras.layers.SimpleRNN(
    units,
    activation='tanh',
    use_bias=True,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros',
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    dropout=0.0,
    recurrent_dropout=0.0,
    return_sequences=False,
    return_state=False,
    go_backwards=False,
    stateful=False,
    unroll=False,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SimpleRNN
inputs = np.random.random([32, 10, 8]).astype(np.float32)
simple_rnn = tf.keras.layers.SimpleRNN(4)


simple_rnn = tf.keras.layers.SimpleRNN(
    4, return_sequences=True, return_state=True)

whole_sequence_output, final_state = simple_rnn(inputs)
"
"tf.keras.layers.SimpleRNNCell(
    units,
    activation='tanh',
    use_bias=True,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros',
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    dropout=0.0,
    recurrent_dropout=0.0,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SimpleRNNCell
inputs = np.random.random([32, 10, 8]).astype(np.float32)
rnn = tf.keras.layers.RNN(tf.keras.layers.SimpleRNNCell(4))


rnn = tf.keras.layers.RNN(
    tf.keras.layers.SimpleRNNCell(4),
    return_sequences=True,
    return_state=True)

whole_sequence_output, final_state = rnn(inputs)
"
"tf.keras.layers.Softmax(
    axis=-1, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Softmax
inp = np.asarray([1., 2., 1.])
layer = tf.keras.layers.Softmax()
layer(inp).numpy()
array([0.21194157, 0.5761169 , 0.21194157], dtype=float32)
mask = np.asarray([True, False, True], dtype=bool)
layer(inp, mask).numpy()
array([0.5, 0. , 0.5], dtype=float32)"
"tf.keras.layers.StackedRNNCells(
    cells, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import StackedRNNCells
batch_size = 3
sentence_max_length = 5
n_features = 2
new_shape = (batch_size, sentence_max_length, n_features)
x = tf.constant(np.reshape(np.arange(30), new_shape), dtype = tf.float32)

rnn_cells = [tf.keras.layers.LSTMCell(128) for _ in range(2)]
stacked_lstm = tf.keras.layers.StackedRNNCells(rnn_cells)
lstm_layer = tf.keras.layers.RNN(stacked_lstm)

result = lstm_layer(x)
"
"tf.keras.layers.StringLookup(
    max_tokens=None,
    num_oov_indices=1,
    mask_token=None,
    oov_token='[UNK]',
    vocabulary=None,
    idf_weights=None,
    encoding='utf-8',
    invert=False,
    output_mode='int',
    sparse=False,
    pad_to_max_tokens=False,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import StringLookup
vocab = [""a"", ""b"", ""c"", ""d""]
data = tf.constant([[""a"", ""c"", ""d""], [""d"", ""z"", ""b""]])
layer = tf.keras.layers.StringLookup(vocabulary=vocab)
layer(data)
<tf.Tensor: shape=(2, 3), dtype=int64, numpy=
<tf.Tensor: shape=(2, 3), dtype=int64, numpy=
array([[1, 3, 4],
       [4, 0, 2]])>"
"tf.keras.layers.Subtract(
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Subtract
    import keras

    input1 = keras.layers.Input(shape=(16,))
    x1 = keras.layers.Dense(8, activation='relu')(input1)
    input2 = keras.layers.Input(shape=(32,))
    x2 = keras.layers.Dense(8, activation='relu')(input2)
    subtracted = keras.layers.Subtract()([x1, x2])

    out = keras.layers.Dense(4)(subtracted)
    model = keras.models.Model(inputs=[input1, input2], outputs=out)
"
"tf.keras.layers.TextVectorization(
    max_tokens=None,
    standardize='lower_and_strip_punctuation',
    split='whitespace',
    ngrams=None,
    output_mode='int',
    output_sequence_length=None,
    pad_to_max_tokens=False,
    vocabulary=None,
    idf_weights=None,
    sparse=False,
    ragged=False,
    encoding='utf-8',
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TextVectorization
text_dataset = tf.data.Dataset.from_tensor_slices([""foo"", ""bar"", ""baz""])
vectorize_layer = tf.keras.layers.TextVectorization(
 max_tokens=max_features,
 output_mode='int',
 output_sequence_length=max_len)
vectorize_layer.adapt(text_dataset.batch(64))
model = tf.keras.models.Sequential()
model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
model.add(vectorize_layer)
input_data = [[""foo qux bar""], [""qux baz""]]
model.predict(input_data)
array([[2, 1, 4, 0],
       [1, 3, 0, 0]])"
"tf.keras.layers.TimeDistributed(
    layer, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TimeDistributed
inputs = tf.keras.Input(shape=(10, 128, 128, 3))
conv_2d_layer = tf.keras.layers.Conv2D(64, (3, 3))
outputs = tf.keras.layers.TimeDistributed(conv_2d_layer)(inputs)
outputs.shape
TensorShape([None, 10, 126, 126, 64])"
"tf.keras.layers.UnitNormalization(
    axis=-1, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import UnitNormalization
data = tf.constant(np.arange(6).reshape(2, 3), dtype=tf.float32)
normalized_data = tf.keras.layers.UnitNormalization()(data)
print(tf.reduce_sum(normalized_data[0, :] ** 2).numpy())
1.0"
"tf.keras.layers.UpSampling1D(
    size=2, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import UpSampling1D
input_shape = (2, 2, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
print(x)
[[[ 0  1  2]
  [ 3  4  5]]
 [[ 6  7  8]
  [ 9 10 11]]]
y = tf.keras.layers.UpSampling1D(size=2)(x)
print(y)
tf.Tensor(
  [[[ 0  1  2]
    [ 0  1  2]
    [ 3  4  5]
    [ 3  4  5]]
   [[ 6  7  8]
    [ 6  7  8]
    [ 9 10 11]
    [ 9 10 11]]], shape=(2, 4, 3), dtype=int64)"
"tf.keras.layers.UpSampling2D(
    size=(2, 2), data_format=None, interpolation='nearest', **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import UpSampling2D
input_shape = (2, 2, 1, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
print(x)
[[[[ 0  1  2]]
  [[ 3  4  5]]]
 [[[ 6  7  8]]
  [[ 9 10 11]]]]
y = tf.keras.layers.UpSampling2D(size=(1, 2))(x)
print(y)
tf.Tensor(
  [[[[ 0  1  2]
     [ 0  1  2]]
    [[ 3  4  5]
     [ 3  4  5]]]
   [[[ 6  7  8]
     [ 6  7  8]]
    [[ 9 10 11]
     [ 9 10 11]]]], shape=(2, 2, 2, 3), dtype=int64)"
"tf.keras.layers.UpSampling3D(
    size=(2, 2, 2), data_format=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import UpSampling3D
input_shape = (2, 1, 2, 1, 3)
x = tf.constant(1, shape=input_shape)
y = tf.keras.layers.UpSampling3D(size=2)(x)
print(y.shape)
(2, 2, 4, 2, 3)"
"tf.keras.layers.ZeroPadding1D(
    padding=1, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ZeroPadding1D
input_shape = (2, 2, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
print(x)
[[[ 0  1  2]
  [ 3  4  5]]
 [[ 6  7  8]
  [ 9 10 11]]]
y = tf.keras.layers.ZeroPadding1D(padding=2)(x)
print(y)
tf.Tensor(
  [[[ 0  0  0]
    [ 0  0  0]
    [ 0  1  2]
    [ 3  4  5]
    [ 0  0  0]
    [ 0  0  0]]
   [[ 0  0  0]
    [ 0  0  0]
    [ 6  7  8]
    [ 9 10 11]
    [ 0  0  0]
    [ 0  0  0]]], shape=(2, 6, 3), dtype=int64)"
"tf.keras.layers.ZeroPadding2D(
    padding=(1, 1), data_format=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ZeroPadding2D
input_shape = (1, 1, 2, 2)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
print(x)
[[[[0 1]
   [2 3]]]]
y = tf.keras.layers.ZeroPadding2D(padding=1)(x)
print(y)
tf.Tensor(
  [[[[0 0]
     [0 0]
     [0 0]
     [0 0]]
    [[0 0]
     [0 1]
     [2 3]
     [0 0]]
    [[0 0]
     [0 0]
     [0 0]
     [0 0]]]], shape=(1, 3, 4, 2), dtype=int64)"
"tf.keras.layers.ZeroPadding3D(
    padding=(1, 1, 1), data_format=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ZeroPadding3D
input_shape = (1, 1, 2, 2, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
y = tf.keras.layers.ZeroPadding3D(padding=2)(x)
print(y.shape)
(1, 5, 6, 6, 3)"
"tf.keras.layers.add(
    inputs, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import add
input_shape = (2, 3, 4)
x1 = tf.random.normal(input_shape)
x2 = tf.random.normal(input_shape)
y = tf.keras.layers.add([x1, x2])
print(y.shape)
(2, 3, 4)"
"tf.keras.layers.concatenate(
    inputs, axis=-1, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import concatenate
x = np.arange(20).reshape(2, 2, 5)
print(x)
[[[ 0  1  2  3  4]
  [ 5  6  7  8  9]]
 [[10 11 12 13 14]
  [15 16 17 18 19]]]
y = np.arange(20, 30).reshape(2, 1, 5)
print(y)
[[[20 21 22 23 24]]
 [[25 26 27 28 29]]]
tf.keras.layers.concatenate([x, y],
                            axis=1)
<tf.Tensor: shape=(2, 3, 5), dtype=int64, numpy=
<tf.Tensor: shape=(2, 3, 5), dtype=int64, numpy=
array([[[ 0,  1,  2,  3,  4],
      [ 5,  6,  7,  8,  9],
      [20, 21, 22, 23, 24]],
     [[10, 11, 12, 13, 14],
      [15, 16, 17, 18, 19],
      [25, 26, 27, 28, 29]]])>"
"tf.keras.layers.deserialize(
    config, custom_objects=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import deserialize
config = {
  'class_name': 'Dense',
  'config': {
    'activation': 'relu',
    'activity_regularizer': None,
    'bias_constraint': None,
    'bias_initializer': {'class_name': 'Zeros', 'config': {} },
    'bias_regularizer': None,
    'dtype': 'float32',
    'kernel_constraint': None,
    'kernel_initializer': {'class_name': 'GlorotUniform',
                           'config': {'seed': None} },
    'kernel_regularizer': None,
    'name': 'dense',
    'trainable': True,
    'units': 32,
    'use_bias': True
  }
}
dense_layer = tf.keras.layers.deserialize(config)
"
"tf.keras.layers.EinsumDense(
    equation,
    output_shape,
    activation=None,
    bias_axes=None,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import EinsumDense
layer = tf.keras.layers.EinsumDense(""ab,bc->ac"",
                                    output_shape=64,
                                    bias_axes=""c"")
input_tensor = tf.keras.Input(shape=[32])
output_tensor = layer(input_tensor)
output_tensor
<... shape=(None, 64) dtype=...>
<... shape=(None, 64) dtype=...>"
"tf.keras.layers.experimental.RandomFourierFeatures(
    output_dim,
    kernel_initializer='gaussian',
    scale=None,
    trainable=False,
    name=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RandomFourierFeatures
model = keras.Sequential([
  keras.Input(shape=(784,)),
  RandomFourierFeatures(
      output_dim=4096,
      scale=10.,
      kernel_initializer='gaussian'),
  layers.Dense(units=10, activation='softmax'),
])
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['categorical_accuracy']
)
"
"tf.keras.layers.experimental.SyncBatchNormalization(
    axis=-1,
    momentum=0.99,
    epsilon=0.001,
    center=True,
    scale=True,
    beta_initializer='zeros',
    gamma_initializer='ones',
    moving_mean_initializer='zeros',
    moving_variance_initializer='ones',
    beta_regularizer=None,
    gamma_regularizer=None,
    beta_constraint=None,
    gamma_constraint=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SyncBatchNormalization
strategy = tf.distribute.MirroredStrategy()

with strategy.scope():
  model = tf.keras.Sequential()
  model.add(tf.keras.layers.Dense(16))
  model.add(tf.keras.layers.experimental.SyncBatchNormalization())
"
"tf.keras.layers.CategoryEncoding(
    num_tokens=None, output_mode='multi_hot', sparse=False, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CategoryEncoding
layer = tf.keras.layers.CategoryEncoding(
          num_tokens=4, output_mode=""one_hot"")
layer([3, 2, 0, 1])
<tf.Tensor: shape=(4, 4), dtype=float32, numpy=
<tf.Tensor: shape=(4, 4), dtype=float32, numpy=
  array([[0., 0., 0., 1.],
         [0., 0., 1., 0.],
         [1., 0., 0., 0.],
         [0., 1., 0., 0.]], dtype=float32)>"
"tf.keras.layers.Discretization(
    bin_boundaries=None,
    num_bins=None,
    epsilon=0.01,
    output_mode='int',
    sparse=False,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Discretization
>>> input = np.array([[-1.5, 1.0, 3.4, .5], [0.0, 3.0, 1.3, 0.0]])
>>> layer = tf.keras.layers.Discretization(bin_boundaries=[0., 1., 2.])
>>> layer(input)
<tf.Tensor: shape=(2, 4), dtype=int64, numpy=
<tf.Tensor: shape=(2, 4), dtype=int64, numpy=
array([[0, 2, 3, 1],
       [1, 3, 2, 1]])>
"
"tf.keras.layers.experimental.preprocessing.HashedCrossing(
    num_bins, output_mode='int', sparse=False, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import HashedCrossing
layer = tf.keras.layers.experimental.preprocessing.HashedCrossing(
    num_bins=5)
feat1 = tf.constant(['A', 'B', 'A', 'B', 'A'])
feat2 = tf.constant([101, 101, 101, 102, 102])
layer((feat1, feat2))
<tf.Tensor: shape=(5,), dtype=int64, numpy=array([1, 4, 1, 1, 3])>
<tf.Tensor: shape=(5,), dtype=int64, numpy=array([1, 4, 1, 1, 3])>"
"tf.keras.layers.Hashing(
    num_bins,
    mask_value=None,
    salt=None,
    output_mode='int',
    sparse=False,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Hashing
layer = tf.keras.layers.Hashing(num_bins=3)
inp = [['A'], ['B'], ['C'], ['D'], ['E']]
layer(inp)
<tf.Tensor: shape=(5, 1), dtype=int64, numpy=
<tf.Tensor: shape=(5, 1), dtype=int64, numpy=
  array([[1],
         [0],
         [1],
         [1],
         [2]])>"
"tf.keras.layers.IntegerLookup(
    max_tokens=None,
    num_oov_indices=1,
    mask_token=None,
    oov_token=-1,
    vocabulary=None,
    vocabulary_dtype='int64',
    idf_weights=None,
    invert=False,
    output_mode='int',
    sparse=False,
    pad_to_max_tokens=False,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import IntegerLookup
vocab = [12, 36, 1138, 42]
layer = tf.keras.layers.IntegerLookup(vocabulary=vocab)
layer(data)
<tf.Tensor: shape=(2, 3), dtype=int64, numpy=
<tf.Tensor: shape=(2, 3), dtype=int64, numpy=
array([[1, 3, 4],
       [4, 0, 2]])>"
"tf.keras.layers.Normalization(
    axis=-1, mean=None, variance=None, invert=False, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Normalization
adapt_data = np.array([1., 2., 3., 4., 5.], dtype='float32')
input_data = np.array([1., 2., 3.], dtype='float32')
layer = tf.keras.layers.Normalization(axis=None)
layer.adapt(adapt_data)
layer(input_data)
<tf.Tensor: shape=(3,), dtype=float32, numpy=
<tf.Tensor: shape=(3,), dtype=float32, numpy=
array([-1.4142135, -0.70710677, 0.], dtype=float32)>"
"tf.keras.layers.RandomZoom(
    height_factor,
    width_factor=None,
    fill_mode='reflect',
    interpolation='bilinear',
    seed=None,
    fill_value=0.0,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RandomZoom
input_img = np.random.random((32, 224, 224, 3))
layer = tf.keras.layers.RandomZoom(.5, .2)
out_img = layer(input_img)
out_img.shape
TensorShape([32, 224, 224, 3])"
"tf.keras.layers.StringLookup(
    max_tokens=None,
    num_oov_indices=1,
    mask_token=None,
    oov_token='[UNK]',
    vocabulary=None,
    idf_weights=None,
    encoding='utf-8',
    invert=False,
    output_mode='int',
    sparse=False,
    pad_to_max_tokens=False,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import StringLookup
vocab = [""a"", ""b"", ""c"", ""d""]
data = tf.constant([[""a"", ""c"", ""d""], [""d"", ""z"", ""b""]])
layer = tf.keras.layers.StringLookup(vocabulary=vocab)
layer(data)
<tf.Tensor: shape=(2, 3), dtype=int64, numpy=
<tf.Tensor: shape=(2, 3), dtype=int64, numpy=
array([[1, 3, 4],
       [4, 0, 2]])>"
"tf.keras.layers.TextVectorization(
    max_tokens=None,
    standardize='lower_and_strip_punctuation',
    split='whitespace',
    ngrams=None,
    output_mode='int',
    output_sequence_length=None,
    pad_to_max_tokens=False,
    vocabulary=None,
    idf_weights=None,
    sparse=False,
    ragged=False,
    encoding='utf-8',
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TextVectorization
text_dataset = tf.data.Dataset.from_tensor_slices([""foo"", ""bar"", ""baz""])
vectorize_layer = tf.keras.layers.TextVectorization(
 max_tokens=max_features,
 output_mode='int',
 output_sequence_length=max_len)
vectorize_layer.adapt(text_dataset.batch(64))
model = tf.keras.models.Sequential()
model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
model.add(vectorize_layer)
input_data = [[""foo qux bar""], [""qux baz""]]
model.predict(input_data)
array([[2, 1, 4, 0],
       [1, 3, 0, 0]])"
"tf.keras.layers.maximum(
    inputs, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import maximum
input1 = tf.keras.layers.Input(shape=(16,))
x1 = tf.keras.layers.Dense(8, activation='relu')(input1) #shape=(None, 8)
input2 = tf.keras.layers.Input(shape=(32,))
x2 = tf.keras.layers.Dense(8, activation='relu')(input2) #shape=(None, 8)
max_inp=tf.keras.layers.maximum([x1,x2]) #shape=(None, 8)
out = tf.keras.layers.Dense(4)(max_inp)
model = tf.keras.models.Model(inputs=[input1, input2], outputs=out)
"
"tf.keras.layers.multiply(
    inputs, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import multiply
x1 = np.arange(3.0)
x2 = np.arange(3.0)
tf.keras.layers.multiply([x1, x2])
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 1., 4.], ...)>
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 1., 4.], ...)>"
"tf.keras.layers.serialize(
    layer
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import serialize
from pprint import pprint
model = tf.keras.models.Sequential()
model.add(tf.keras.Input(shape=(16,)))
model.add(tf.keras.layers.Dense(32, activation='relu'))

pprint(tf.keras.layers.serialize(model))
"
"tf.keras.layers.subtract(
    inputs, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import subtract
    import keras

    input1 = keras.layers.Input(shape=(16,))
    x1 = keras.layers.Dense(8, activation='relu')(input1)
    input2 = keras.layers.Input(shape=(32,))
    x2 = keras.layers.Dense(8, activation='relu')(input2)
    subtracted = keras.layers.subtract([x1, x2])

    out = keras.layers.Dense(4)(subtracted)
    model = keras.models.Model(inputs=[input1, input2], outputs=out)
"
"tf.keras.losses.BinaryCrossentropy(
    from_logits=False,
    label_smoothing=0.0,
    axis=-1,
    reduction=losses_utils.ReductionV2.AUTO,
    name='binary_crossentropy'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import BinaryCrossentropy
model.compile(
  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
  ....
)
"
"tf.keras.losses.BinaryFocalCrossentropy(
    apply_class_balancing=False,
    alpha=0.25,
    gamma=2.0,
    from_logits=False,
    label_smoothing=0.0,
    axis=-1,
    reduction=losses_utils.ReductionV2.AUTO,
    name='binary_focal_crossentropy'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import BinaryFocalCrossentropy
model.compile(
  loss=tf.keras.losses.BinaryFocalCrossentropy(gamma=2.0, from_logits=True),
  ....
)
"
"tf.keras.losses.CategoricalCrossentropy(
    from_logits=False,
    label_smoothing=0.0,
    axis=-1,
    reduction=losses_utils.ReductionV2.AUTO,
    name='categorical_crossentropy'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CategoricalCrossentropy
y_true = [[0, 1, 0], [0, 0, 1]]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
cce = tf.keras.losses.CategoricalCrossentropy()
cce(y_true, y_pred).numpy()
1.177"
"tf.keras.losses.CategoricalHinge(
    reduction=losses_utils.ReductionV2.AUTO, name='categorical_hinge'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CategoricalHinge
y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
h = tf.keras.losses.CategoricalHinge()
h(y_true, y_pred).numpy()
1.4"
"tf.keras.losses.CosineSimilarity(
    axis=-1,
    reduction=losses_utils.ReductionV2.AUTO,
    name='cosine_similarity'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CosineSimilarity
y_true = [[0., 1.], [1., 1.]]
y_pred = [[1., 0.], [1., 1.]]
cosine_loss = tf.keras.losses.CosineSimilarity(axis=1)
cosine_loss(y_true, y_pred).numpy()
-0.5"
"tf.keras.losses.Hinge(
    reduction=losses_utils.ReductionV2.AUTO, name='hinge'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Hinge
y_true = [[0., 1.], [0., 0.]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
h = tf.keras.losses.Hinge()
h(y_true, y_pred).numpy()
1.3"
"tf.keras.losses.KLDivergence(
    reduction=losses_utils.ReductionV2.AUTO, name='kl_divergence'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import KLDivergence
y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
kl = tf.keras.losses.KLDivergence()
kl(y_true, y_pred).numpy()
0.458"
"tf.keras.losses.LogCosh(
    reduction=losses_utils.ReductionV2.AUTO, name='log_cosh'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LogCosh
y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [0., 0.]]
l = tf.keras.losses.LogCosh()
l(y_true, y_pred).numpy()
0.108"
"tf.keras.losses.Loss(
    reduction=losses_utils.ReductionV2.AUTO, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
class MeanSquaredError(Loss):

  def call(self, y_true, y_pred):
    return tf.reduce_mean(tf.math.square(y_pred - y_true), axis=-1)
"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_percentage_error
y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_logarithmic_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.losses.MeanAbsoluteError(
    reduction=losses_utils.ReductionV2.AUTO,
    name='mean_absolute_error'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanAbsoluteError
y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [1., 0.]]
mae = tf.keras.losses.MeanAbsoluteError()
mae(y_true, y_pred).numpy()
0.5"
"tf.keras.losses.MeanAbsolutePercentageError(
    reduction=losses_utils.ReductionV2.AUTO,
    name='mean_absolute_percentage_error'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanAbsolutePercentageError
y_true = [[2., 1.], [2., 3.]]
y_pred = [[1., 1.], [1., 0.]]
mape = tf.keras.losses.MeanAbsolutePercentageError()
mape(y_true, y_pred).numpy()
50."
"tf.keras.losses.MeanSquaredError(
    reduction=losses_utils.ReductionV2.AUTO,
    name='mean_squared_error'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanSquaredError
y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [1., 0.]]
mse = tf.keras.losses.MeanSquaredError()
mse(y_true, y_pred).numpy()
0.5"
"tf.keras.losses.MeanSquaredLogarithmicError(
    reduction=losses_utils.ReductionV2.AUTO,
    name='mean_squared_logarithmic_error'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanSquaredLogarithmicError
y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [1., 0.]]
msle = tf.keras.losses.MeanSquaredLogarithmicError()
msle(y_true, y_pred).numpy()
0.240"
"tf.keras.losses.Poisson(
    reduction=losses_utils.ReductionV2.AUTO, name='poisson'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Poisson
y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [0., 0.]]
p = tf.keras.losses.Poisson()
p(y_true, y_pred).numpy()
0.5"
"tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=False,
    ignore_class=None,
    reduction=losses_utils.ReductionV2.AUTO,
    name='sparse_categorical_crossentropy'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SparseCategoricalCrossentropy
y_true = [1, 2]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
scce = tf.keras.losses.SparseCategoricalCrossentropy()
scce(y_true, y_pred).numpy()
1.177"
"tf.keras.losses.SquaredHinge(
    reduction=losses_utils.ReductionV2.AUTO, name='squared_hinge'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SquaredHinge
y_true = [[0., 1.], [0., 0.]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
h = tf.keras.losses.SquaredHinge()
h(y_true, y_pred).numpy()
1.86"
"tf.keras.metrics.binary_crossentropy(
    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import binary_crossentropy
y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.916 , 0.714], dtype=float32)"
"tf.keras.metrics.binary_focal_crossentropy(
    y_true,
    y_pred,
    apply_class_balancing=False,
    alpha=0.25,
    gamma=2.0,
    from_logits=False,
    label_smoothing=0.0,
    axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import binary_focal_crossentropy
y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
loss = tf.keras.losses.binary_focal_crossentropy(y_true, y_pred,
                                                 gamma=2)
assert loss.shape == (2,)
loss.numpy()
array([0.330, 0.206], dtype=float32)"
"tf.keras.metrics.categorical_crossentropy(
    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import categorical_crossentropy
y_true = [[0, 1, 0], [0, 0, 1]]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.0513, 2.303], dtype=float32)"
"tf.keras.losses.categorical_hinge(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import categorical_hinge
y_true = np.random.randint(0, 3, size=(2,))
y_true = tf.keras.utils.to_categorical(y_true, num_classes=3)
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.categorical_hinge(y_true, y_pred)
assert loss.shape == (2,)
pos = np.sum(y_true * y_pred, axis=-1)
neg = np.amax((1. - y_true) * y_pred, axis=-1)
assert np.array_equal(loss.numpy(), np.maximum(0., neg - pos + 1.))"
"tf.keras.losses.cosine_similarity(
    y_true, y_pred, axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import cosine_similarity
y_true = [[0., 1.], [1., 1.], [1., 1.]]
y_pred = [[1., 0.], [1., 1.], [-1., -1.]]
loss = tf.keras.losses.cosine_similarity(y_true, y_pred, axis=1)
loss.numpy()
array([-0., -0.999, 0.999], dtype=float32)"
"tf.keras.losses.get(
    identifier
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get
loss = tf.keras.losses.get(""categorical_crossentropy"")
type(loss)
<class 'function'>
<class 'function'>
loss = tf.keras.losses.get(""CategoricalCrossentropy"")
type(loss)
<class '...keras.losses.CategoricalCrossentropy'>
<class '...keras.losses.CategoricalCrossentropy'>"
"tf.keras.metrics.hinge(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import hinge
y_true = np.random.choice([-1, 1], size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.hinge(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    np.mean(np.maximum(1. - y_true * y_pred, 0.), axis=-1))"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_percentage_error
y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_percentage_error
y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_logarithmic_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_logarithmic_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.poisson(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import poisson
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.poisson(y_true, y_pred)
assert loss.shape == (2,)
y_pred = y_pred + 1e-7
assert np.allclose(
    loss.numpy(), np.mean(y_pred - y_true * np.log(y_pred), axis=-1),
    atol=1e-5)"
"tf.keras.metrics.sparse_categorical_crossentropy(
    y_true, y_pred, from_logits=False, axis=-1, ignore_class=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sparse_categorical_crossentropy
y_true = [1, 2]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.0513, 2.303], dtype=float32)"
"tf.keras.metrics.squared_hinge(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import squared_hinge
y_true = np.random.choice([-1, 1], size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.squared_hinge(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    np.mean(np.square(np.maximum(1. - y_true * y_pred, 0.)), axis=-1))"
"tf.keras.metrics.AUC(
    num_thresholds=200,
    curve='ROC',
    summation_method='interpolation',
    name=None,
    dtype=None,
    thresholds=None,
    multi_label=False,
    num_labels=None,
    label_weights=None,
    from_logits=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import AUC
m = tf.keras.metrics.AUC(num_thresholds=3)
m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])
m.result().numpy()
0.75"
"tf.keras.metrics.Accuracy(
    name='accuracy', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Accuracy
m = tf.keras.metrics.Accuracy()
m.update_state([[1], [2], [3], [4]], [[0], [2], [3], [4]])
m.result().numpy()
0.75"
"tf.keras.metrics.BinaryAccuracy(
    name='binary_accuracy', dtype=None, threshold=0.5
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import BinaryAccuracy
m = tf.keras.metrics.BinaryAccuracy()
m.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]])
m.result().numpy()
0.75"
"tf.keras.metrics.BinaryCrossentropy(
    name='binary_crossentropy',
    dtype=None,
    from_logits=False,
    label_smoothing=0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import BinaryCrossentropy
m = tf.keras.metrics.BinaryCrossentropy()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
0.81492424"
"tf.keras.metrics.CategoricalAccuracy(
    name='categorical_accuracy', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CategoricalAccuracy
m = tf.keras.metrics.CategoricalAccuracy()
m.update_state([[0, 0, 1], [0, 1, 0]], [[0.1, 0.9, 0.8],
                [0.05, 0.95, 0]])
m.result().numpy()
0.5"
"tf.keras.metrics.CategoricalCrossentropy(
    name='categorical_crossentropy',
    dtype=None,
    from_logits=False,
    label_smoothing=0,
    axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CategoricalCrossentropy
m = tf.keras.metrics.CategoricalCrossentropy()
m.update_state([[0, 1, 0], [0, 0, 1]],
               [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])
m.result().numpy()
1.1769392"
"tf.keras.metrics.CategoricalHinge(
    name='categorical_hinge', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CategoricalHinge
m = tf.keras.metrics.CategoricalHinge()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
1.4000001"
"tf.keras.metrics.CosineSimilarity(
    name='cosine_similarity', dtype=None, axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CosineSimilarity
m = tf.keras.metrics.CosineSimilarity(axis=1)
m.update_state([[0., 1.], [1., 1.]], [[1., 0.], [1., 1.]])
m.result().numpy()
0.49999997"
"tf.keras.metrics.FalseNegatives(
    thresholds=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import FalseNegatives
m = tf.keras.metrics.FalseNegatives()
m.update_state([0, 1, 1, 1], [0, 1, 0, 0])
m.result().numpy()
2.0"
"tf.keras.metrics.FalsePositives(
    thresholds=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import FalsePositives
m = tf.keras.metrics.FalsePositives()
m.update_state([0, 1, 0, 0], [0, 0, 1, 1])
m.result().numpy()
2.0"
"tf.keras.metrics.Hinge(
    name='hinge', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Hinge
m = tf.keras.metrics.Hinge()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
1.3"
"tf.keras.metrics.KLDivergence(
    name='kullback_leibler_divergence', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import KLDivergence
m = tf.keras.metrics.KLDivergence()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
0.45814306"
"tf.keras.metrics.LogCoshError(
    name='logcosh', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LogCoshError
m = tf.keras.metrics.LogCoshError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.10844523"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_percentage_error
y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_logarithmic_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.Mean(
    name='mean', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Mean
m = tf.keras.metrics.Mean()
m.update_state([1, 3, 5, 7])
m.result().numpy()
4.0
m.reset_state()
m.update_state([1, 3, 5, 7], sample_weight=[1, 1, 0, 0])
m.result().numpy()
2.0"
"tf.keras.metrics.MeanAbsoluteError(
    name='mean_absolute_error', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanAbsoluteError
m = tf.keras.metrics.MeanAbsoluteError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.25"
"tf.keras.metrics.MeanAbsolutePercentageError(
    name='mean_absolute_percentage_error', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanAbsolutePercentageError
m = tf.keras.metrics.MeanAbsolutePercentageError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
250000000.0"
"tf.keras.metrics.MeanMetricWrapper(
    fn, name=None, dtype=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanMetricWrapper
def accuracy(y_true, y_pred):
  return tf.cast(tf.math.equal(y_true, y_pred), tf.float32)

accuracy_metric = tf.keras.metrics.MeanMetricWrapper(fn=accuracy)

keras_model.compile(..., metrics=accuracy_metric)
"
"tf.keras.metrics.MeanRelativeError(
    normalizer, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanRelativeError
m = tf.keras.metrics.MeanRelativeError(normalizer=[1, 3, 2, 3])
m.update_state([1, 3, 2, 3], [2, 4, 6, 8])"
"tf.keras.metrics.MeanSquaredError(
    name='mean_squared_error', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanSquaredError
m = tf.keras.metrics.MeanSquaredError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.25"
"tf.keras.metrics.MeanSquaredLogarithmicError(
    name='mean_squared_logarithmic_error', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanSquaredLogarithmicError
m = tf.keras.metrics.MeanSquaredLogarithmicError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.12011322"
"tf.keras.metrics.MeanTensor(
    name='mean_tensor', dtype=None, shape=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanTensor
m = tf.keras.metrics.MeanTensor()
m.update_state([0, 1, 2, 3])
m.update_state([4, 5, 6, 7])
m.result().numpy()
array([2., 3., 4., 5.], dtype=float32)"
"tf.keras.metrics.Metric(
    name=None, dtype=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Metric
m = SomeMetric(...)
for input in ...:
  m.update_state(input)
print('Final result: ', m.result().numpy())
"
"tf.keras.metrics.Poisson(
    name='poisson', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Poisson
m = tf.keras.metrics.Poisson()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.49999997"
"tf.keras.metrics.Precision(
    thresholds=None, top_k=None, class_id=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Precision
m = tf.keras.metrics.Precision()
m.update_state([0, 1, 1, 1], [1, 0, 1, 1])
m.result().numpy()
0.6666667"
"tf.keras.metrics.PrecisionAtRecall(
    recall, num_thresholds=200, class_id=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import PrecisionAtRecall
m = tf.keras.metrics.PrecisionAtRecall(0.5)
m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])
m.result().numpy()
0.5"
"tf.keras.metrics.Recall(
    thresholds=None, top_k=None, class_id=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Recall
m = tf.keras.metrics.Recall()
m.update_state([0, 1, 1, 1], [1, 0, 1, 1])
m.result().numpy()
0.6666667"
"tf.keras.metrics.RecallAtPrecision(
    precision, num_thresholds=200, class_id=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RecallAtPrecision
m = tf.keras.metrics.RecallAtPrecision(0.8)
m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])
m.result().numpy()
0.5"
"tf.keras.metrics.RootMeanSquaredError(
    name='root_mean_squared_error', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RootMeanSquaredError
m = tf.keras.metrics.RootMeanSquaredError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.5"
"tf.keras.metrics.SensitivityAtSpecificity(
    specificity, num_thresholds=200, class_id=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SensitivityAtSpecificity
m = tf.keras.metrics.SensitivityAtSpecificity(0.5)
m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])
m.result().numpy()
0.5"
"tf.keras.metrics.SparseCategoricalCrossentropy(
    name: str = 'sparse_categorical_crossentropy',
    dtype: Optional[Union[str, tf.dtypes.DType]] = None,
    from_logits: bool = False,
    ignore_class: Optional[int] = None,
    axis: int = -1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SparseCategoricalCrossentropy
m = tf.keras.metrics.SparseCategoricalCrossentropy()
m.update_state([1, 2],
               [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])
m.result().numpy()
1.1769392"
"tf.keras.metrics.SparseTopKCategoricalAccuracy(
    k=5, name='sparse_top_k_categorical_accuracy', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SparseTopKCategoricalAccuracy
m = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)
m.update_state([2, 1], [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])
m.result().numpy()
0.5"
"tf.keras.metrics.SpecificityAtSensitivity(
    sensitivity, num_thresholds=200, class_id=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SpecificityAtSensitivity
m = tf.keras.metrics.SpecificityAtSensitivity(0.5)
m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])
m.result().numpy()
0.66666667"
"tf.keras.metrics.SquaredHinge(
    name='squared_hinge', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SquaredHinge
m = tf.keras.metrics.SquaredHinge()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
1.86"
"tf.keras.metrics.Sum(
    name='sum', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Sum
m = tf.keras.metrics.Sum()
m.update_state([1, 3, 5, 7])
m.result().numpy()
16.0"
"tf.keras.metrics.TopKCategoricalAccuracy(
    k=5, name='top_k_categorical_accuracy', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TopKCategoricalAccuracy
m = tf.keras.metrics.TopKCategoricalAccuracy(k=1)
m.update_state([[0, 0, 1], [0, 1, 0]],
               [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])
m.result().numpy()
0.5"
"tf.keras.metrics.TrueNegatives(
    thresholds=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TrueNegatives
m = tf.keras.metrics.TrueNegatives()
m.update_state([0, 1, 0, 0], [1, 1, 0, 0])
m.result().numpy()
2.0"
"tf.keras.metrics.TruePositives(
    thresholds=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TruePositives
m = tf.keras.metrics.TruePositives()
m.update_state([0, 1, 1, 1], [1, 0, 1, 1])
m.result().numpy()
2.0"
"tf.keras.metrics.binary_accuracy(
    y_true, y_pred, threshold=0.5
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import binary_accuracy
y_true = [[1], [1], [0], [0]]
y_pred = [[1], [1], [0], [0]]
m = tf.keras.metrics.binary_accuracy(y_true, y_pred)
assert m.shape == (4,)
m.numpy()
array([1., 1., 1., 1.], dtype=float32)"
"tf.keras.metrics.binary_crossentropy(
    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import binary_crossentropy
y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.916 , 0.714], dtype=float32)"
"tf.keras.metrics.binary_focal_crossentropy(
    y_true,
    y_pred,
    apply_class_balancing=False,
    alpha=0.25,
    gamma=2.0,
    from_logits=False,
    label_smoothing=0.0,
    axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import binary_focal_crossentropy
y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
loss = tf.keras.losses.binary_focal_crossentropy(y_true, y_pred,
                                                 gamma=2)
assert loss.shape == (2,)
loss.numpy()
array([0.330, 0.206], dtype=float32)"
"tf.keras.metrics.categorical_accuracy(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import categorical_accuracy
y_true = [[0, 0, 1], [0, 1, 0]]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.categorical_accuracy(y_true, y_pred)
assert m.shape == (2,)
m.numpy()
array([0., 1.], dtype=float32)"
"tf.keras.metrics.categorical_crossentropy(
    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import categorical_crossentropy
y_true = [[0, 1, 0], [0, 0, 1]]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.0513, 2.303], dtype=float32)"
"tf.keras.metrics.get(
    identifier
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get
metric = tf.keras.metrics.get(""categorical_crossentropy"")
type(metric)
<class 'function'>
<class 'function'>
metric = tf.keras.metrics.get(""CategoricalCrossentropy"")
type(metric)
<class '...metrics.CategoricalCrossentropy'>
<class '...metrics.CategoricalCrossentropy'>"
"tf.keras.metrics.hinge(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import hinge
y_true = np.random.choice([-1, 1], size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.hinge(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    np.mean(np.maximum(1. - y_true * y_pred, 0.), axis=-1))"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_percentage_error
y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_percentage_error
y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_logarithmic_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_logarithmic_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.poisson(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import poisson
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.poisson(y_true, y_pred)
assert loss.shape == (2,)
y_pred = y_pred + 1e-7
assert np.allclose(
    loss.numpy(), np.mean(y_pred - y_true * np.log(y_pred), axis=-1),
    atol=1e-5)"
"tf.keras.metrics.sparse_categorical_accuracy(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sparse_categorical_accuracy
y_true = [2, 1]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)
assert m.shape == (2,)
m.numpy()
array([0., 1.], dtype=float32)"
"tf.keras.metrics.sparse_categorical_crossentropy(
    y_true, y_pred, from_logits=False, axis=-1, ignore_class=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sparse_categorical_crossentropy
y_true = [1, 2]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.0513, 2.303], dtype=float32)"
"tf.keras.metrics.sparse_top_k_categorical_accuracy(
    y_true, y_pred, k=5
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sparse_top_k_categorical_accuracy
y_true = [2, 1]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.sparse_top_k_categorical_accuracy(
    y_true, y_pred, k=3)
assert m.shape == (2,)
m.numpy()
array([1., 1.], dtype=float32)"
"tf.keras.metrics.squared_hinge(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import squared_hinge
y_true = np.random.choice([-1, 1], size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.squared_hinge(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    np.mean(np.square(np.maximum(1. - y_true * y_pred, 0.)), axis=-1))"
"tf.keras.metrics.top_k_categorical_accuracy(
    y_true, y_pred, k=5
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import top_k_categorical_accuracy
y_true = [[0, 0, 1], [0, 1, 0]]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.top_k_categorical_accuracy(y_true, y_pred, k=3)
assert m.shape == (2,)
m.numpy()
array([1., 1.], dtype=float32)"
"tf.keras.mixed_precision.Policy(
    name
)
","import pandas as pd
import numpy as np
import tensorflow as tf
tf.keras.mixed_precision.set_global_policy('mixed_float16')
layer1 = tf.keras.layers.Dense(10)
<Policy ""mixed_float16"">
<Policy ""mixed_float16"">
layer2 = tf.keras.layers.Dense(10, dtype='float32')
layer2.dtype_policy
<Policy ""float32"">
<Policy ""float32"">
tf.keras.mixed_precision.set_global_policy('float32')"
"tf.keras.mixed_precision.set_global_policy(
    policy
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import set_global_policy
tf.keras.mixed_precision.set_global_policy('mixed_float16')
tf.keras.mixed_precision.global_policy()
<Policy ""mixed_float16"">
<Policy ""mixed_float16"">
tf.keras.layers.Dense(10).dtype_policy
<Policy ""mixed_float16"">
<Policy ""mixed_float16"">
tf.keras.layers.Dense(10, dtype='float64').dtype_policy
<Policy ""float64"">
<Policy ""float64"">
tf.keras.mixed_precision.set_global_policy('float32')"
"tf.keras.Model(
    *args, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Model
import tensorflow as tf

inputs = tf.keras.Input(shape=(3,))
x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)
outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)
"
"tf.keras.Sequential(
    layers=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Sequential
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8, input_shape=(16,)))
model.add(tf.keras.layers.Dense(4))

model = tf.keras.Sequential()
model.add(tf.keras.Input(shape=(16,)))
model.add(tf.keras.layers.Dense(8))

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8))
model.add(tf.keras.layers.Dense(4))

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8, input_shape=(16,)))
model.add(tf.keras.layers.Dense(4))
len(model.weights)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8))
model.add(tf.keras.layers.Dense(4))
model.build((None, 16))
len(model.weights)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8))
model.add(tf.keras.layers.Dense(1))
model.compile(optimizer='sgd', loss='mse')
model.fit(x, y, batch_size=32, epochs=10)
"
"tf.keras.models.clone_model(
    model, input_tensors=None, clone_function=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import clone_model
model = keras.Sequential([
    keras.Input(shape=(728,)),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid'),
])
new_model = clone_model(model)
"
"tf.keras.models.load_model(
    filepath, custom_objects=None, compile=True, options=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import load_model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(5, input_shape=(3,)),
    tf.keras.layers.Softmax()])
model.save('/tmp/model')
loaded_model = tf.keras.models.load_model('/tmp/model')
x = tf.random.uniform((10, 3))
assert np.allclose(model.predict(x), loaded_model.predict(x))"
"tf.keras.models.model_from_json(
    json_string, custom_objects=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import model_from_json
model = tf.keras.Sequential([
    tf.keras.layers.Dense(5, input_shape=(3,)),
    tf.keras.layers.Softmax()])
config = model.to_json()
loaded_model = tf.keras.models.model_from_json(config)"
"tf.keras.optimizers.experimental.RMSprop(
    learning_rate=0.001,
    rho=0.9,
    momentum=0.0,
    epsilon=1e-07,
    centered=False,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=100,
    jit_compile=True,
    name='RMSprop',
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RMSprop
opt = tf.keras.optimizers.experimental.RMSprop(learning_rate=0.1)
var1 = tf.Variable(10.0)
opt.minimize(loss, [var1])
var1.numpy()
9.683772"
"tf.keras.optimizers.experimental.RMSprop(
    learning_rate=0.001,
    rho=0.9,
    momentum=0.0,
    epsilon=1e-07,
    centered=False,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=100,
    jit_compile=True,
    name='RMSprop',
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RMSprop
opt = tf.keras.optimizers.experimental.RMSprop(learning_rate=0.1)
var1 = tf.Variable(10.0)
opt.minimize(loss, [var1])
var1.numpy()
9.683772"
"tf.keras.optimizers.legacy.Adam(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    amsgrad=False,
    name='Adam',
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Adam
opt = tf.keras.optimizers.legacy.Adam(learning_rate=0.1)
var1 = tf.Variable(10.0)
step_count = opt.minimize(loss, [var1]).numpy()
var1.numpy()
9.9"
"tf.keras.optimizers.legacy.Nadam(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    name='Nadam',
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Nadam
>>> opt = tf.keras.optimizers.legacy.Nadam(learning_rate=0.2)
>>> var1 = tf.Variable(10.0)
>>> loss = lambda: (var1 ** 2) / 2.0
>>> step_count = opt.minimize(loss, [var1]).numpy()
>>> ""{:.1f}"".format(var1.numpy())
9.8
"
"tf.keras.optimizers.legacy.RMSprop(
    learning_rate=0.001,
    rho=0.9,
    momentum=0.0,
    epsilon=1e-07,
    centered=False,
    name='RMSprop',
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RMSprop
opt = tf.keras.optimizers.legacy.RMSprop(learning_rate=0.1)
var1 = tf.Variable(10.0)
step_count = opt.minimize(loss, [var1]).numpy()
var1.numpy()
9.683772"
"tf.keras.optimizers.schedules.CosineDecayRestarts(
    initial_learning_rate,
    first_decay_steps,
    t_mul=2.0,
    m_mul=1.0,
    alpha=0.0,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CosineDecayRestarts
first_decay_steps = 1000
lr_decayed_fn = (
  tf.keras.optimizers.schedules.CosineDecayRestarts(
      initial_learning_rate,
      first_decay_steps))
"
"tf.keras.optimizers.schedules.PiecewiseConstantDecay(
    boundaries, values, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import PiecewiseConstantDecay
step = tf.Variable(0, trainable=False)
boundaries = [100000, 110000]
values = [1.0, 0.5, 0.1]
learning_rate_fn = keras.optimizers.schedules.PiecewiseConstantDecay(
    boundaries, values)

learning_rate = learning_rate_fn(step)
"
"tf.keras.optimizers.schedules.deserialize(
    config, custom_objects=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import deserialize
config = {
  'class_name': 'PolynomialDecay',
  'config': {'cycle': False,
    'decay_steps': 10000,
    'end_learning_rate': 0.01,
    'initial_learning_rate': 0.1,
    'name': None,
    'power': 0.5} }
lr_schedule = tf.keras.optimizers.schedules.deserialize(config)
"
"tf.keras.optimizers.schedules.serialize(
    learning_rate_schedule
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import serialize
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
  0.1, decay_steps=100000, decay_rate=0.96, staircase=True)
tf.keras.optimizers.schedules.serialize(lr_schedule)
{'class_name': 'ExponentialDecay', 'config': {...} }"
"tf.keras.optimizers.serialize(
    optimizer
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import serialize
tf.keras.optimizers.serialize(tf.keras.optimizers.legacy.SGD())
{'class_name': 'SGD', 'config': {'name': 'SGD', 'learning_rate': 0.01,
                                 'decay': 0.0, 'momentum': 0.0,
                                 'nesterov': False} }"
"tf.keras.preprocessing.image.ImageDataGenerator(
    featurewise_center=False,
    samplewise_center=False,
    featurewise_std_normalization=False,
    samplewise_std_normalization=False,
    zca_whitening=False,
    zca_epsilon=1e-06,
    rotation_range=0,
    width_shift_range=0.0,
    height_shift_range=0.0,
    brightness_range=None,
    shear_range=0.0,
    zoom_range=0.0,
    channel_shift_range=0.0,
    fill_mode='nearest',
    cval=0.0,
    horizontal_flip=False,
    vertical_flip=False,
    rescale=None,
    preprocessing_function=None,
    data_format=None,
    validation_split=0.0,
    interpolation_order=1,
    dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ImageDataGenerator
(x_train, y_train), (x_test, y_test) = cifar10.load_data()
y_train = utils.to_categorical(y_train, num_classes)
y_test = utils.to_categorical(y_test, num_classes)
datagen = ImageDataGenerator(
    featurewise_center=True,
    featurewise_std_normalization=True,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True,
    validation_split=0.2)
datagen.fit(x_train)
model.fit(datagen.flow(x_train, y_train, batch_size=32,
         subset='training'),
         validation_data=datagen.flow(x_train, y_train,
         batch_size=8, subset='validation'),
         steps_per_epoch=len(x_train) / 32, epochs=epochs)
for e in range(epochs):
    print('Epoch', e)
    batches = 0
    for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=32):
        model.fit(x_batch, y_batch)
        batches += 1
        if batches >= len(x_train) / 32:
            break
"
"tf.keras.utils.array_to_img(
    x, data_format=None, scale=True, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import array_to_img
from PIL import Image
img = np.random.random(size=(100, 100, 3))
pil_img = tf.keras.utils.array_to_img(img)
"
"tf.keras.utils.img_to_array(
    img, data_format=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import img_to_array
from PIL import Image
img_data = np.random.random(size=(100, 100, 3))
img = tf.keras.utils.array_to_img(img_data)
array = tf.keras.utils.image.img_to_array(img)
"
"tf.keras.utils.load_img(
    path,
    grayscale=False,
    color_mode='rgb',
    target_size=None,
    interpolation='nearest',
    keep_aspect_ratio=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import load_img
image = tf.keras.utils.load_img(image_path)
input_arr = tf.keras.utils.img_to_array(image)
predictions = model.predict(input_arr)
"
"tf.keras.preprocessing.sequence.TimeseriesGenerator(
    data,
    targets,
    length,
    sampling_rate=1,
    stride=1,
    start_index=0,
    end_index=None,
    shuffle=False,
    reverse=False,
    batch_size=128
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TimeseriesGenerator
from keras.preprocessing.sequence import TimeseriesGenerator
import numpy as np
data = np.array([[i] for i in range(50)])
targets = np.array([[i] for i in range(50)])
data_gen = TimeseriesGenerator(data, targets,
                               length=10, sampling_rate=2,
                               batch_size=2)
assert len(data_gen) == 20
batch_0 = data_gen[0]
x, y = batch_0
assert np.array_equal(x,
                      np.array([[[0], [2], [4], [6], [8]],
                                [[1], [3], [5], [7], [9]]]))
assert np.array_equal(y,
                      np.array([[10], [11]]))
"
"tf.keras.utils.pad_sequences(
    sequences,
    maxlen=None,
    dtype='int32',
    padding='pre',
    truncating='pre',
    value=0.0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import pad_sequences
sequence = [[1], [2, 3], [4, 5, 6]]
tf.keras.preprocessing.sequence.pad_sequences(sequence)
array([[0, 0, 1],
       [0, 2, 3],
       [4, 5, 6]], dtype=int32)"
"tf.keras.preprocessing.text.text_to_word_sequence(
    input_text,
    filters='!""#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n',
    lower=True,
    split=' '
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import text_to_word_sequence
sample_text = 'This is a sample sentence.'
tf.keras.preprocessing.text.text_to_word_sequence(sample_text)
['this', 'is', 'a', 'sample', 'sentence']"
"tf.keras.regularizers.OrthogonalRegularizer(
    factor=0.01, mode='rows'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import OrthogonalRegularizer
regularizer = tf.keras.regularizers.OrthogonalRegularizer(factor=0.01)
layer = tf.keras.layers.Dense(units=4, kernel_regularizer=regularizer)"
"tf.keras.regularizers.OrthogonalRegularizer(
    factor=0.01, mode='rows'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import OrthogonalRegularizer
regularizer = tf.keras.regularizers.OrthogonalRegularizer(factor=0.01)
layer = tf.keras.layers.Dense(units=4, kernel_regularizer=regularizer)"
"tf.keras.utils.custom_object_scope(
    *args
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import custom_object_scope
layer = Dense(3, kernel_regularizer=my_regularizer)
config = layer.get_config()
...
with custom_object_scope({'my_regularizer': my_regularizer}):
  layer = Dense.from_config(config)
"
"tf.keras.utils.SequenceEnqueuer(
    sequence, use_multiprocessing=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SequenceEnqueuer
    enqueuer = SequenceEnqueuer(...)
    enqueuer.start()
    datas = enqueuer.get()
    for data in datas:
    enqueuer.stop()
"
"tf.keras.utils.SidecarEvaluator(
    model,
    data,
    checkpoint_dir,
    steps=None,
    max_evaluations=None,
    callbacks=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SidecarEvaluator
model = tf.keras.models.Sequential(...)
model.compile(metrics=tf.keras.metrics.SparseCategoricalAccuracy(
    name=""eval_metrics""))
data = tf.data.Dataset.from_tensor_slices(...)

tf.keras.SidecarEvaluator(
    model=model,
    data=data,
    checkpoint_dir='/tmp/checkpoint_dir',
    callbacks=[tf.keras.callbacks.TensorBoard(log_dir='/tmp/log_dir')]
).start()
"
"tf.keras.utils.array_to_img(
    x, data_format=None, scale=True, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import array_to_img
from PIL import Image
img = np.random.random(size=(100, 100, 3))
pil_img = tf.keras.utils.array_to_img(img)
"
"tf.keras.utils.custom_object_scope(
    *args
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import custom_object_scope
layer = Dense(3, kernel_regularizer=my_regularizer)
config = layer.get_config()
...
with custom_object_scope({'my_regularizer': my_regularizer}):
  layer = Dense.from_config(config)
"
"tf.keras.utils.deserialize_keras_object(
    identifier,
    module_objects=None,
    custom_objects=None,
    printable_module_name='object'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import deserialize_keras_object
def deserialize(config, custom_objects=None):
   return deserialize_keras_object(
     identifier,
     module_objects=globals(),
     custom_objects=custom_objects,
     name=""MyObjectType"",
   )
"
"tf.keras.utils.experimental.DatasetCreator(
    dataset_fn, input_options=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import DatasetCreator
model = tf.keras.Sequential([tf.keras.layers.Dense(10)])
model.compile(tf.keras.optimizers.SGD(), loss=""mse"")

def dataset_fn(input_context):
  global_batch_size = 64
  batch_size = input_context.get_per_replica_batch_size(global_batch_size)
  dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat()
  dataset = dataset.shard(
      input_context.num_input_pipelines, input_context.input_pipeline_id)
  dataset = dataset.batch(batch_size)
  dataset = dataset.prefetch(2)
  return dataset

input_options = tf.distribute.InputOptions(
    experimental_fetch_to_device=True,
    experimental_per_replica_buffer_size=2)
model.fit(tf.keras.utils.experimental.DatasetCreator(
    dataset_fn, input_options=input_options), epochs=10, steps_per_epoch=10)
"
"tf.keras.utils.get_file(
    fname=None,
    origin=None,
    untar=False,
    md5_hash=None,
    file_hash=None,
    cache_subdir='datasets',
    hash_algorithm='auto',
    extract=False,
    archive_format='auto',
    cache_dir=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get_file
path_to_downloaded_file = tf.keras.utils.get_file(
    ""flower_photos"",
    ""https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"",
    untar=True)
"
"tf.keras.utils.get_registered_object(
    name, custom_objects=None, module_objects=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get_registered_object
def from_config(cls, config, custom_objects=None):
  if 'my_custom_object_name' in config:
    config['hidden_cls'] = tf.keras.utils.get_registered_object(
        config['my_custom_object_name'], custom_objects=custom_objects)
"
"tf.keras.utils.img_to_array(
    img, data_format=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import img_to_array
from PIL import Image
img_data = np.random.random(size=(100, 100, 3))
img = tf.keras.utils.array_to_img(img_data)
array = tf.keras.utils.image.img_to_array(img)
"
"tf.keras.utils.load_img(
    path,
    grayscale=False,
    color_mode='rgb',
    target_size=None,
    interpolation='nearest',
    keep_aspect_ratio=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import load_img
image = tf.keras.utils.load_img(image_path)
input_arr = tf.keras.utils.img_to_array(image)
predictions = model.predict(input_arr)
"
"tf.keras.utils.pack_x_y_sample_weight(
    x, y=None, sample_weight=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import pack_x_y_sample_weight
x = tf.ones((10, 1))
data = tf.keras.utils.pack_x_y_sample_weight(x)
isinstance(data, tf.Tensor)
True
y = tf.ones((10, 1))
data = tf.keras.utils.pack_x_y_sample_weight(x, y)
isinstance(data, tuple)
True
x, y = data"
"tf.keras.utils.pad_sequences(
    sequences,
    maxlen=None,
    dtype='int32',
    padding='pre',
    truncating='pre',
    value=0.0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import pad_sequences
sequence = [[1], [2, 3], [4, 5, 6]]
tf.keras.preprocessing.sequence.pad_sequences(sequence)
array([[0, 0, 1],
       [0, 2, 3],
       [4, 5, 6]], dtype=int32)"
"tf.keras.utils.plot_model(
    model,
    to_file='model.png',
    show_shapes=False,
    show_dtype=False,
    show_layer_names=True,
    rankdir='TB',
    expand_nested=False,
    dpi=96,
    layer_range=None,
    show_layer_activations=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import plot_model
input = tf.keras.Input(shape=(100,), dtype='int32', name='input')
x = tf.keras.layers.Embedding(
    output_dim=512, input_dim=10000, input_length=100)(input)
x = tf.keras.layers.LSTM(32)(x)
x = tf.keras.layers.Dense(64, activation='relu')(x)
x = tf.keras.layers.Dense(64, activation='relu')(x)
x = tf.keras.layers.Dense(64, activation='relu')(x)
output = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(x)
model = tf.keras.Model(inputs=[input], outputs=[output])
dot_img_file = '/tmp/model_1.png'
tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)
"
"tf.keras.utils.register_keras_serializable(
    package='Custom', name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import register_keras_serializable
@keras.utils.register_keras_serializable('my_package')
class MyDense(keras.layers.Dense):
  pass

assert keras.utils.get_registered_object('my_package>MyDense') == MyDense
assert keras.utils.get_registered_name(MyDense) == 'my_package>MyDense'
"
"tf.keras.utils.split_dataset(
    dataset, left_size=None, right_size=None, shuffle=False, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import split_dataset
data = np.random.random(size=(1000, 4))
left_ds, right_ds = tf.keras.utils.split_dataset(data, left_size=0.8)
int(left_ds.cardinality())
800
int(right_ds.cardinality())
200"
"tf.keras.utils.to_categorical(
    y, num_classes=None, dtype='float32'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import to_categorical
a = tf.keras.utils.to_categorical([0, 1, 2, 3], num_classes=4)
a = tf.constant(a, shape=[4, 4])
print(a)
tf.Tensor(
  [[1. 0. 0. 0.]
   [0. 1. 0. 0.]
   [0. 0. 1. 0.]
   [0. 0. 0. 1.]], shape=(4, 4), dtype=float32)"
"tf.keras.utils.unpack_x_y_sample_weight(
    data
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import unpack_x_y_sample_weight
features_batch = tf.ones((10, 5))
labels_batch = tf.zeros((10, 5))
data = (features_batch, labels_batch)
x, y, sample_weight = tf.keras.utils.unpack_x_y_sample_weight(data)
sample_weight is None
True"
"tf.keras.utils.warmstart_embedding_matrix(
    base_vocabulary,
    new_vocabulary,
    base_embeddings,
    new_embeddings_initializer='uniform'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import warmstart_embedding_matrix
>>> import keras
>>> vocab_base = tf.convert_to_tensor([""unk"", ""a"", ""b"", ""c""])
>>> vocab_new = tf.convert_to_tensor(
...        [""unk"", ""unk"", ""a"", ""b"", ""c"", ""d"", ""e""])
>>> vectorized_vocab_base = np.random.rand(vocab_base.shape[0], 3)
>>> vectorized_vocab_new = np.random.rand(vocab_new.shape[0], 3)
>>> warmstarted_embedding_matrix = warmstart_embedding_matrix(
...       base_vocabulary=vocab_base,
...       new_vocabulary=vocab_new,
...       base_embeddings=vectorized_vocab_base,
...       new_embeddings_initializer=keras.initializers.Constant(
...         vectorized_vocab_new))
"
"tf.math.less(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import less
x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.less(x, y) ==> [False, True, False]

x = tf.constant([5, 4, 6])
y = tf.constant([5, 6, 7])
tf.math.less(x, y) ==> [False, True, True]
"
"tf.math.less_equal(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import less_equal
x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.less_equal(x, y) ==> [True, True, False]

x = tf.constant([5, 4, 6])
y = tf.constant([5, 6, 6])
tf.math.less_equal(x, y) ==> [True, True, True]
"
"tf.linalg.LinearOperatorAdjoint(
    operator,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearOperatorAdjoint
operator = LinearOperatorFullMatrix([[1 - i., 3.], [0., 1. + i]])
operator_adjoint = LinearOperatorAdjoint(operator)

operator_adjoint.to_dense()
==> [[1. + i, 0.]
     [3., 1 - i]]

operator_adjoint.shape
==> [2, 2]

operator_adjoint.log_abs_determinant()
==> - log(2)

x = ... Shape [2, 4] Tensor
operator_adjoint.matmul(x)
==> Shape [2, 4] Tensor, equal to operator.matmul(x, adjoint=True)
"
"tf.linalg.LinearOperatorBlockDiag(
    operators,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=True,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearOperatorBlockDiag
operator_1 = LinearOperatorFullMatrix([[1., 2.], [3., 4.]])
operator_2 = LinearOperatorFullMatrix([[1., 0.], [0., 1.]])
operator = LinearOperatorBlockDiag([operator_1, operator_2])

operator.to_dense()
==> [[1., 2., 0., 0.],
     [3., 4., 0., 0.],
     [0., 0., 1., 0.],
     [0., 0., 0., 1.]]

operator.shape
==> [4, 4]

operator.log_abs_determinant()
==> scalar Tensor

operator.matmul(x)
==> tf.concat([operator_1.matmul(x1), operator_2.matmul(x2)])

operator_1 = LinearOperatorFullMatrix([[1.], [3.]])
operator_2 = LinearOperatorFullMatrix([[1., 6.]])
operator_3 = LinearOperatorFullMatrix([[2.], [7.]])
operator = LinearOperatorBlockDiag([operator_1, operator_2, operator_3])

operator.to_dense()
==> [[1., 0., 0., 0.],
     [3., 0., 0., 0.],
     [0., 1., 6., 0.],
     [0., 0., 0., 2.]]
     [0., 0., 0., 7.]]

operator.shape
==> [5, 4]


matrix_44 = tf.random.normal(shape=[2, 3, 4, 4])
operator_44 = LinearOperatorFullMatrix(matrix)

matrix_55 = tf.random.normal(shape=[1, 3, 5, 5])
operator_55 = LinearOperatorFullMatrix(matrix_55)

operator_99 = LinearOperatorBlockDiag([operator_44, operator_55])

x = tf.random.normal(shape=[2, 3, 9])
operator_99.matmul(x)
==> Shape [2, 3, 9] Tensor

x = [tf.random.normal(shape=[2, 3, 4]), tf.random.normal(shape=[2, 3, 5])]
operator_99.matmul(x)
==> [Shape [2, 3, 4] Tensor, Shape [2, 3, 5] Tensor]
"
"tf.linalg.LinearOperatorBlockLowerTriangular(
    operators,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name='LinearOperatorBlockLowerTriangular'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearOperatorBlockLowerTriangular
>>> operator_0 = tf.linalg.LinearOperatorFullMatrix([[1., 2.], [3., 4.]])
>>> operator_1 = tf.linalg.LinearOperatorFullMatrix([[1., 0.], [0., 1.]])
>>> operator_2 = tf.linalg.LinearOperatorLowerTriangular([[5., 6.], [7., 8]])
>>> operator = LinearOperatorBlockLowerTriangular(
...   [[operator_0], [operator_1, operator_2]])
"
"tf.linalg.LinearOperatorDiag(
    diag,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name='LinearOperatorDiag'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearOperatorDiag
diag = [1., -1.]
operator = LinearOperatorDiag(diag)

operator.to_dense()
==> [[1.,  0.]
     [0., -1.]]

operator.shape
==> [2, 2]

operator.log_abs_determinant()
==> scalar Tensor

x = ... Shape [2, 4] Tensor
operator.matmul(x)
==> Shape [2, 4] Tensor

diag = tf.random.normal(shape=[2, 3, 4])
operator = LinearOperatorDiag(diag)

y = tf.random.normal(shape=[2, 1, 4, 2])
x = operator.solve(y)
==> operator.matmul(x) = y
"
"tf.linalg.LinearOperatorFullMatrix(
    matrix,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name='LinearOperatorFullMatrix'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearOperatorFullMatrix
matrix = [[1., 2.], [3., 4.]]
operator = LinearOperatorFullMatrix(matrix)

operator.to_dense()
==> [[1., 2.]
     [3., 4.]]

operator.shape
==> [2, 2]

operator.log_abs_determinant()
==> scalar Tensor

x = ... Shape [2, 4] Tensor
operator.matmul(x)
==> Shape [2, 4] Tensor

matrix = tf.random.normal(shape=[2, 3, 4, 4])
operator = LinearOperatorFullMatrix(matrix)
"
"tf.linalg.LinearOperatorHouseholder(
    reflection_axis,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name='LinearOperatorHouseholder'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearOperatorHouseholder
vec = [1 / np.sqrt(2), 1. / np.sqrt(2)]
operator = LinearOperatorHouseholder(vec)

operator.to_dense()
==> [[0.,  -1.]
     [-1., -0.]]

operator.shape
==> [2, 2]

operator.log_abs_determinant()
==> scalar Tensor

x = ... Shape [2, 4] Tensor
operator.matmul(x)
==> Shape [2, 4] Tensor
"
"tf.linalg.LinearOperatorIdentity(
    num_rows,
    batch_shape=None,
    dtype=None,
    is_non_singular=True,
    is_self_adjoint=True,
    is_positive_definite=True,
    is_square=True,
    assert_proper_shapes=False,
    name='LinearOperatorIdentity'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearOperatorIdentity
operator = LinearOperatorIdentity(num_rows=2, dtype=tf.float32)

operator.to_dense()
==> [[1., 0.]
     [0., 1.]]

operator.shape
==> [2, 2]

operator.log_abs_determinant()
==> 0.

x = ... Shape [2, 4] Tensor
operator.matmul(x)
==> Shape [2, 4] Tensor, same as x.

y = tf.random.normal(shape=[3, 2, 4])
x = operator.solve(y)
==> Shape [3, 2, 4] Tensor, same as y.

operator = LinearOperatorIdentity(num_rows=2, batch_shape=[2])
operator.to_dense()
==> [[[1., 0.]
      [0., 1.]],
     [[1., 0.]
      [0., 1.]]]

x = ... Shape [2, 2, 3]
operator.matmul(x)
==> Shape [2, 2, 3] Tensor, same as x

x = ... Shape [1, 2, 3]
operator.matmul(x)
==> Shape [2, 2, 3] Tensor, equal to [x, x]
"
"tf.linalg.LinearOperatorInversion(
    operator,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearOperatorInversion
operator = LinearOperatorFullMatrix([[1., 0.], [0., 2.]])
operator_inv = LinearOperatorInversion(operator)

operator_inv.to_dense()
==> [[1., 0.]
     [0., 0.5]]

operator_inv.shape
==> [2, 2]

operator_inv.log_abs_determinant()
==> - log(2)

x = ... Shape [2, 4] Tensor
operator_inv.matmul(x)
==> Shape [2, 4] Tensor, equal to operator.solve(x)
"
"tf.linalg.LinearOperatorKronecker(
    operators,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearOperatorKronecker
operator_1 = LinearOperatorFullMatrix([[1., 2.], [3., 4.]])
operator_2 = LinearOperatorFullMatrix([[1., 0.], [2., 1.]])
operator = LinearOperatorKronecker([operator_1, operator_2])

operator.to_dense()
==> [[1., 0., 2., 0.],
     [2., 1., 4., 2.],
     [3., 0., 4., 0.],
     [6., 3., 8., 4.]]

operator.shape
==> [4, 4]

operator.log_abs_determinant()
==> scalar Tensor

x = ... Shape [4, 2] Tensor
operator.matmul(x)
==> Shape [4, 2] Tensor

matrix_45 = tf.random.normal(shape=[2, 3, 4, 5])
operator_45 = LinearOperatorFullMatrix(matrix)

matrix_56 = tf.random.normal(shape=[2, 3, 5, 6])
operator_56 = LinearOperatorFullMatrix(matrix_56)

operator_large = LinearOperatorKronecker([operator_45, operator_56])

x = tf.random.normal(shape=[2, 3, 6, 2])
operator_large.matmul(x)
==> Shape [2, 3, 30, 2] Tensor
"
"tf.linalg.LinearOperatorLowerTriangular(
    tril,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name='LinearOperatorLowerTriangular'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearOperatorLowerTriangular
tril = [[1., 2.], [3., 4.]]
operator = LinearOperatorLowerTriangular(tril)

operator.to_dense()
==> [[1., 0.]
     [3., 4.]]

operator.shape
==> [2, 2]

operator.log_abs_determinant()
==> scalar Tensor

x = ... Shape [2, 4] Tensor
operator.matmul(x)
==> Shape [2, 4] Tensor

tril = tf.random.normal(shape=[2, 3, 4, 4])
operator = LinearOperatorLowerTriangular(tril)
"
"tf.linalg.LinearOperatorPermutation(
    perm,
    dtype=tf.dtypes.float32,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name='LinearOperatorPermutation'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearOperatorPermutation
vec = [0, 2, 1]
operator = LinearOperatorPermutation(vec)

operator.to_dense()
==> [[1., 0., 0.]
     [0., 0., 1.]
     [0., 1., 0.]]

operator.shape
==> [3, 3]

operator.log_abs_determinant()
==> scalar Tensor

x = ... Shape [3, 4] Tensor
operator.matmul(x)
==> Shape [3, 4] Tensor
"
"tf.linalg.LinearOperatorTridiag(
    diagonals,
    diagonals_format=_COMPACT,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name='LinearOperatorTridiag'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearOperatorTridiag
superdiag = [3., 4., 5.]
diag = [1., -1., 2.]
subdiag = [6., 7., 8]
operator = tf.linalg.LinearOperatorTridiag(
   [superdiag, diag, subdiag],
   diagonals_format='sequence')
operator.to_dense()
<tf.Tensor: shape=(3, 3), dtype=float32, numpy=
<tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[ 1.,  3.,  0.],
       [ 7., -1.,  4.],
       [ 0.,  8.,  2.]], dtype=float32)>
operator.shape
TensorShape([3, 3])"
"tf.linalg.LinearOperatorZeros(
    num_rows,
    num_columns=None,
    batch_shape=None,
    dtype=None,
    is_non_singular=False,
    is_self_adjoint=True,
    is_positive_definite=False,
    is_square=True,
    assert_proper_shapes=False,
    name='LinearOperatorZeros'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearOperatorZeros
operator = LinearOperatorZero(num_rows=2, dtype=tf.float32)

operator.to_dense()
==> [[0., 0.]
     [0., 0.]]

operator.shape
==> [2, 2]

operator.determinant()
==> 0.

x = ... Shape [2, 4] Tensor
operator.matmul(x)
==> Shape [2, 4] Tensor, same as x.

operator = LinearOperatorZeros(num_rows=2, batch_shape=[2])
operator.to_dense()
==> [[[0., 0.]
      [0., 0.]],
     [[0., 0.]
      [0., 0.]]]

x = ... Shape [2, 2, 3]
operator.matmul(x)
==> Shape [2, 2, 3] Tensor, same as tf.zeros_like(x)

x = ... Shape [1, 2, 3]
operator.matmul(x)
==> Shape [2, 2, 3] Tensor, equal to tf.zeros_like([x, x])
"
"tf.linalg.adjoint(
    matrix, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import adjoint
x = tf.constant([[1 + 1j, 2 + 2j, 3 + 3j],
                 [4 + 4j, 5 + 5j, 6 + 6j]])
"
"tf.linalg.band_part(
    input, num_lower, num_upper, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import band_part

tf.linalg.band_part(input, 1, -1) ==> [[ 0,  1,  2, 3]
                                       [-1,  0,  1, 2]
                                       [ 0, -1,  0, 1]
                                       [ 0,  0, -1, 0]],

tf.linalg.band_part(input, 2, 1) ==> [[ 0,  1,  0, 0]
                                      [-1,  0,  1, 0]
                                      [-2, -1,  0, 1]
                                      [ 0, -2, -1, 0]]
"
"tf.linalg.banded_triangular_solve(
    bands, rhs, lower=True, adjoint=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import banded_triangular_solve
x = [[2., 3., 4.], [1., 2., 3.]]
x2 = [[2., 3., 4.], [10000., 2., 3.]]
y = tf.zeros([3, 3])
z = tf.linalg.set_diag(y, x, align='LEFT_RIGHT', k=(-1, 0))
z
<tf.Tensor: shape=(3, 3), dtype=float32, numpy=
<tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[2., 0., 0.],
       [2., 3., 0.],
       [0., 3., 4.]], dtype=float32)>
soln = tf.linalg.banded_triangular_solve(x, tf.ones([3, 1]))
soln
<tf.Tensor: shape=(3, 1), dtype=float32, numpy=
<tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[0.5 ],
       [0.  ],
       [0.25]], dtype=float32)>
are_equal = soln == tf.linalg.banded_triangular_solve(x2, tf.ones([3, 1]))
tf.reduce_all(are_equal).numpy()
True
are_equal = soln == tf.linalg.triangular_solve(z, tf.ones([3, 1]))
tf.reduce_all(are_equal).numpy()
True"
"tf.linalg.cholesky_solve(
    chol, rhs, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import cholesky_solve

...
"
"tf.linalg.diag(
    diagonal,
    name='diag',
    k=0,
    num_rows=-1,
    num_cols=-1,
    padding_value=0,
    align='RIGHT_LEFT'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
output[i, j, ..., l, m, n]
  = diagonal[i, j, ..., l, n-max(d_upper, 0)] ; if n - m == d_upper
    padding_value                             ; otherwise
"
"tf.linalg.eigh_tridiagonal(
    alpha,
    beta,
    eigvals_only=True,
    select='a',
    select_range=None,
    tol=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import eigh_tridiagonal
import numpy
eigvals = tf.linalg.eigh_tridiagonal([0.0, 0.0, 0.0], [1.0, 1.0])
eigvals_expected = [-numpy.sqrt(2.0), 0.0, numpy.sqrt(2.0)]
tf.assert_near(eigvals_expected, eigvals)
"
"tf.eye(
    num_rows,
    num_columns=None,
    batch_shape=None,
    dtype=tf.dtypes.float32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import eye
tf.eye(2)
==> [[1., 0.],
     [0., 1.]]

batch_identity = tf.eye(2, batch_shape=[3])

tf.eye(2, num_columns=3)
==> [[ 1.,  0.,  0.],
     [ 0.,  1.,  0.]]
"
"tf.linalg.logdet(
    matrix, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import logdet
underflow:
"
"tf.linalg.lu_matrix_inverse(
    lower_upper, perm, validate_args=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import lu_matrix_inverse
inv_X = tf.lu_matrix_inverse(*tf.linalg.lu(X))
tf.assert_near(tf.matrix_inverse(X), inv_X)
"
"tf.linalg.lu_reconstruct(
    lower_upper, perm, validate_args=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import lu_reconstruct
import numpy as np
import tensorflow as tf
import tensorflow_probability as tfp

x = [[[3., 4], [1, 2]],
     [[7., 8], [3, 4]]]
x_reconstructed = tf.linalg.lu_reconstruct(*tf.linalg.lu(x))
tf.assert_near(x, x_reconstructed)
"
"tf.linalg.lu_solve(
    lower_upper, perm, rhs, validate_args=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import lu_solve
import numpy as np
import tensorflow as tf
import tensorflow_probability as tfp

x = [[[1., 2],
      [3, 4]],
     [[7, 8],
      [3, 4]]]
inv_x = tf.linalg.lu_solve(*tf.linalg.lu(x), rhs=tf.eye(2))
tf.assert_near(tf.matrix_inverse(x), inv_x)
"
"tf.linalg.matmul(
    a,
    b,
    transpose_a=False,
    transpose_b=False,
    adjoint_a=False,
    adjoint_b=False,
    a_is_sparse=False,
    b_is_sparse=False,
    output_type=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import matmul
a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
array([[1, 2, 3],
       [4, 5, 6]], dtype=int32)>
b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
array([[ 7,  8],
       [ 9, 10],
       [11, 12]], dtype=int32)>
c = tf.matmul(a, b)
<tf.Tensor: shape=(2, 2), dtype=int32, numpy=
<tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[ 58,  64],
       [139, 154]], dtype=int32)>"
"tf.linalg.matrix_transpose(
    a, name='matrix_transpose', conjugate=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import matrix_transpose
x = tf.constant([[1, 2, 3], [4, 5, 6]])

x = tf.constant([[1 + 1j, 2 + 2j, 3 + 3j],
                 [4 + 4j, 5 + 5j, 6 + 6j]])

"
"tf.linalg.matvec(
    a,
    b,
    transpose_a=False,
    adjoint_a=False,
    a_is_sparse=False,
    b_is_sparse=False,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import matvec
a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])

b = tf.constant([7, 9, 11], shape=[3])

c = tf.linalg.matvec(a, b)


a = tf.constant(np.arange(1, 13, dtype=np.int32),
                shape=[2, 2, 3])

b = tf.constant(np.arange(13, 19, dtype=np.int32),
                shape=[2, 3])

c = tf.linalg.matvec(a, b)
"
"tf.linalg.pinv(
    a, rcond=None, validate_args=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import pinv
import tensorflow as tf
import tensorflow_probability as tfp

a = tf.constant([[1.,  0.4,  0.5],
                 [0.4, 0.2,  0.25],
                 [0.5, 0.25, 0.35]])
tf.matmul(tf.linalg.pinv(a), a)
             [0., 1., 0.],
             [0., 0., 1.]], dtype=float32)

a = tf.constant([[1.,  0.4,  0.5,  1.],
                 [0.4, 0.2,  0.25, 2.],
                 [0.5, 0.25, 0.35, 3.]])
tf.matmul(tf.linalg.pinv(a), a)
             [ 0.37,  0.43, -0.33,  0.02],
             [ 0.21, -0.33,  0.81,  0.01],
             [-0.02,  0.02,  0.01,  1.  ]], dtype=float32)
"
"tf.linalg.qr(
    input, full_matrices=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import qr
q, r = qr(a)
q_full, r_full = qr(a, full_matrices=True)
"
"tf.linalg.svd(
    tensor, full_matrices=False, compute_uv=True, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import svd
s, u, v = svd(a)
s = svd(a, compute_uv=False)
"
"tf.linalg.tensor_diag_part(
    input, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import tensor_diag_part
x = [[[[1111,1112],[1121,1122]],
      [[1211,1212],[1221,1222]]],
     [[[2111, 2112], [2121, 2122]],
      [[2211, 2212], [2221, 2222]]]
     ]
tf.linalg.tensor_diag_part(x)
<tf.Tensor: shape=(2, 2), dtype=int32, numpy=
<tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[1111, 1212],
       [2121, 2222]], dtype=int32)>
tf.linalg.diag_part(x).shape
TensorShape([2, 2, 2])"
"tf.linalg.trace(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import trace
x = tf.constant([[1, 2], [3, 4]])

x = tf.constant([[1, 2, 3],
                 [4, 5, 6],
                 [7, 8, 9]])

x = tf.constant([[[1, 2, 3],
                  [4, 5, 6],
                  [7, 8, 9]],
                 [[-1, -2, -3],
                  [-4, -5, -6],
                  [-7, -8, -9]]])
"
"tf.linalg.tridiagonal_matmul(
    diagonals, rhs, diagonals_format='compact', name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import tridiagonal_matmul
superdiag = tf.constant([-1, -1, 0], dtype=tf.float64)
maindiag = tf.constant([2, 2, 2], dtype=tf.float64)
subdiag = tf.constant([0, -1, -1], dtype=tf.float64)
diagonals = [superdiag, maindiag, subdiag]
rhs = tf.constant([[1, 1], [1, 1], [1, 1]], dtype=tf.float64)
x = tf.linalg.tridiagonal_matmul(diagonals, rhs, diagonals_format='sequence')
"
"tf.linalg.tridiagonal_solve(
    diagonals,
    rhs,
    diagonals_format='compact',
    transpose_rhs=False,
    conjugate_rhs=False,
    name=None,
    partial_pivoting=True,
    perturb_singular=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import tridiagonal_solve
rhs = tf.constant([...])
matrix = tf.constant([[...]])
m = matrix.shape[0]
diagonals=tf.gather_nd(matrix, indices)
x = tf.linalg.tridiagonal_solve(diagonals, rhs)
"
"tf.linspace(
    start, stop, num, name=None, axis=0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import linspace
tf.linspace(10.0, 12.0, 3, name=""linspace"") => [ 10.0  11.0  12.0]
"
"tf.lite.TFLiteConverter(
    funcs, trackable_obj=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
  converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
  tflite_model = converter.convert()

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

converter = tf.lite.TFLiteConverter.from_concrete_functions([func], model)
tflite_model = converter.convert()

converter = tf.lite.TFLiteConverter.experimental_from_jax([func], [[
    ('input1', input1), ('input2', input2)]])
tflite_model = converter.convert()
"
"tf.lite.experimental.authoring.compatible(
    target=None, converter_target_spec=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
@tf.lite.experimental.authoring.compatible
@tf.function(input_signature=[
    tf.TensorSpec(shape=[None], dtype=tf.float32)
])
def f(x):
    return tf.cosh(x)

result = f(tf.constant([0.0]))
#   - <stdin>:6
"
"tf.lite.experimental.load_delegate(
    library, options=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import load_delegate
import tensorflow as tf

try:
  delegate = tf.lite.experimental.load_delegate('delegate.so')
except ValueError:
  // Fallback to CPU

if delegate:
  interpreter = tf.lite.Interpreter(
      model_path='model.tflite',
      experimental_delegates=[delegate])
else:
  interpreter = tf.lite.Interpreter(model_path='model.tflite')
"
"tf.math.logical_and(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import logical_and
>>> a = tf.constant([True])
>>> b = tf.constant([False])
>>> tf.math.logical_and(a, b)
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>
>>> a & b
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>
"
"tf.math.logical_not(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import logical_not
tf.math.logical_not(tf.constant([True, False]))
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>"
"tf.math.logical_or(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import logical_or
>>> a = tf.constant([True])
>>> b = tf.constant([False])
>>> tf.math.logical_or(a, b)
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>
>>> a | b
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>
"
"tf.lookup.KeyValueTensorInitializer(
    keys, values, key_dtype=None, value_dtype=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import KeyValueTensorInitializer
keys_tensor = tf.constant(['a', 'b', 'c'])
vals_tensor = tf.constant([7, 8, 9])
input_tensor = tf.constant(['a', 'f'])
init = tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor)
table = tf.lookup.StaticHashTable(
    init,
    default_value=-1)
table.lookup(input_tensor).numpy()
array([ 7, -1], dtype=int32)"
"tf.lookup.StaticHashTable(
    initializer, default_value, name=None, experimental_is_anonymous=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import StaticHashTable
keys_tensor = tf.constant(['a', 'b', 'c'])
vals_tensor = tf.constant([7, 8, 9])
input_tensor = tf.constant(['a', 'f'])
table = tf.lookup.StaticHashTable(
    tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor),
    default_value=-1)
table.lookup(input_tensor).numpy()
array([ 7, -1], dtype=int32)"
"tf.lookup.StaticVocabularyTable(
    initializer,
    num_oov_buckets,
    lookup_key_dtype=None,
    name=None,
    experimental_is_anonymous=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import StaticVocabularyTable
init = tf.lookup.KeyValueTensorInitializer(
    keys=tf.constant(['emerson', 'lake', 'palmer']),
    values=tf.constant([0, 1, 2], dtype=tf.int64))
table = tf.lookup.StaticVocabularyTable(
   init,
   num_oov_buckets=5)"
"tf.lookup.experimental.DenseHashTable(
    key_dtype,
    value_dtype,
    default_value,
    empty_key,
    deleted_key,
    initial_num_buckets=None,
    name='MutableDenseHashTable',
    checkpoint=True,
    experimental_is_anonymous=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import DenseHashTable
table = tf.lookup.experimental.DenseHashTable(
    key_dtype=tf.string,
    value_dtype=tf.int64,
    default_value=-1,
    empty_key='',
    deleted_key='$')
keys = tf.constant(['a', 'b', 'c'])
values = tf.constant([0, 1, 2], dtype=tf.int64)
table.insert(keys, values)
table.remove(tf.constant(['c']))
table.lookup(tf.constant(['a', 'b', 'c','d'])).numpy()
array([ 0,  1, -1, -1])"
"tf.lookup.experimental.MutableHashTable(
    key_dtype,
    value_dtype,
    default_value,
    name='MutableHashTable',
    checkpoint=True,
    experimental_is_anonymous=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MutableHashTable
table = tf.lookup.experimental.MutableHashTable(key_dtype=tf.string,
                                                value_dtype=tf.int64,
                                                default_value=-1)
keys_tensor = tf.constant(['a', 'b', 'c'])
vals_tensor = tf.constant([7, 8, 9], dtype=tf.int64)
input_tensor = tf.constant(['a', 'f'])
table.insert(keys_tensor, vals_tensor)
table.lookup(input_tensor).numpy()
array([ 7, -1])
table.remove(tf.constant(['c']))
table.lookup(keys_tensor).numpy()
array([ 7, 8, -1])
sorted(table.export()[0].numpy())
[b'a', b'b']
sorted(table.export()[1].numpy())
[7, 8]"
"tf.keras.losses.BinaryCrossentropy(
    from_logits=False,
    label_smoothing=0.0,
    axis=-1,
    reduction=losses_utils.ReductionV2.AUTO,
    name='binary_crossentropy'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import BinaryCrossentropy
model.compile(
  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
  ....
)
"
"tf.keras.losses.BinaryFocalCrossentropy(
    apply_class_balancing=False,
    alpha=0.25,
    gamma=2.0,
    from_logits=False,
    label_smoothing=0.0,
    axis=-1,
    reduction=losses_utils.ReductionV2.AUTO,
    name='binary_focal_crossentropy'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import BinaryFocalCrossentropy
model.compile(
  loss=tf.keras.losses.BinaryFocalCrossentropy(gamma=2.0, from_logits=True),
  ....
)
"
"tf.keras.losses.CategoricalCrossentropy(
    from_logits=False,
    label_smoothing=0.0,
    axis=-1,
    reduction=losses_utils.ReductionV2.AUTO,
    name='categorical_crossentropy'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CategoricalCrossentropy
y_true = [[0, 1, 0], [0, 0, 1]]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
cce = tf.keras.losses.CategoricalCrossentropy()
cce(y_true, y_pred).numpy()
1.177"
"tf.keras.losses.CategoricalHinge(
    reduction=losses_utils.ReductionV2.AUTO, name='categorical_hinge'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CategoricalHinge
y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
h = tf.keras.losses.CategoricalHinge()
h(y_true, y_pred).numpy()
1.4"
"tf.keras.losses.CosineSimilarity(
    axis=-1,
    reduction=losses_utils.ReductionV2.AUTO,
    name='cosine_similarity'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CosineSimilarity
y_true = [[0., 1.], [1., 1.]]
y_pred = [[1., 0.], [1., 1.]]
cosine_loss = tf.keras.losses.CosineSimilarity(axis=1)
cosine_loss(y_true, y_pred).numpy()
-0.5"
"tf.keras.losses.Hinge(
    reduction=losses_utils.ReductionV2.AUTO, name='hinge'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Hinge
y_true = [[0., 1.], [0., 0.]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
h = tf.keras.losses.Hinge()
h(y_true, y_pred).numpy()
1.3"
"tf.keras.losses.KLDivergence(
    reduction=losses_utils.ReductionV2.AUTO, name='kl_divergence'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import KLDivergence
y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
kl = tf.keras.losses.KLDivergence()
kl(y_true, y_pred).numpy()
0.458"
"tf.keras.losses.LogCosh(
    reduction=losses_utils.ReductionV2.AUTO, name='log_cosh'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LogCosh
y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [0., 0.]]
l = tf.keras.losses.LogCosh()
l(y_true, y_pred).numpy()
0.108"
"tf.keras.losses.Loss(
    reduction=losses_utils.ReductionV2.AUTO, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
class MeanSquaredError(Loss):

  def call(self, y_true, y_pred):
    return tf.reduce_mean(tf.math.square(y_pred - y_true), axis=-1)
"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_percentage_error
y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_logarithmic_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.losses.MeanAbsoluteError(
    reduction=losses_utils.ReductionV2.AUTO,
    name='mean_absolute_error'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanAbsoluteError
y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [1., 0.]]
mae = tf.keras.losses.MeanAbsoluteError()
mae(y_true, y_pred).numpy()
0.5"
"tf.keras.losses.MeanAbsolutePercentageError(
    reduction=losses_utils.ReductionV2.AUTO,
    name='mean_absolute_percentage_error'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanAbsolutePercentageError
y_true = [[2., 1.], [2., 3.]]
y_pred = [[1., 1.], [1., 0.]]
mape = tf.keras.losses.MeanAbsolutePercentageError()
mape(y_true, y_pred).numpy()
50."
"tf.keras.losses.MeanSquaredError(
    reduction=losses_utils.ReductionV2.AUTO,
    name='mean_squared_error'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanSquaredError
y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [1., 0.]]
mse = tf.keras.losses.MeanSquaredError()
mse(y_true, y_pred).numpy()
0.5"
"tf.keras.losses.MeanSquaredLogarithmicError(
    reduction=losses_utils.ReductionV2.AUTO,
    name='mean_squared_logarithmic_error'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanSquaredLogarithmicError
y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [1., 0.]]
msle = tf.keras.losses.MeanSquaredLogarithmicError()
msle(y_true, y_pred).numpy()
0.240"
"tf.keras.losses.Poisson(
    reduction=losses_utils.ReductionV2.AUTO, name='poisson'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Poisson
y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [0., 0.]]
p = tf.keras.losses.Poisson()
p(y_true, y_pred).numpy()
0.5"
"tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=False,
    ignore_class=None,
    reduction=losses_utils.ReductionV2.AUTO,
    name='sparse_categorical_crossentropy'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SparseCategoricalCrossentropy
y_true = [1, 2]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
scce = tf.keras.losses.SparseCategoricalCrossentropy()
scce(y_true, y_pred).numpy()
1.177"
"tf.keras.losses.SquaredHinge(
    reduction=losses_utils.ReductionV2.AUTO, name='squared_hinge'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SquaredHinge
y_true = [[0., 1.], [0., 0.]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
h = tf.keras.losses.SquaredHinge()
h(y_true, y_pred).numpy()
1.86"
"tf.keras.metrics.binary_crossentropy(
    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import binary_crossentropy
y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.916 , 0.714], dtype=float32)"
"tf.keras.metrics.binary_focal_crossentropy(
    y_true,
    y_pred,
    apply_class_balancing=False,
    alpha=0.25,
    gamma=2.0,
    from_logits=False,
    label_smoothing=0.0,
    axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import binary_focal_crossentropy
y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
loss = tf.keras.losses.binary_focal_crossentropy(y_true, y_pred,
                                                 gamma=2)
assert loss.shape == (2,)
loss.numpy()
array([0.330, 0.206], dtype=float32)"
"tf.keras.metrics.categorical_crossentropy(
    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import categorical_crossentropy
y_true = [[0, 1, 0], [0, 0, 1]]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.0513, 2.303], dtype=float32)"
"tf.keras.losses.categorical_hinge(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import categorical_hinge
y_true = np.random.randint(0, 3, size=(2,))
y_true = tf.keras.utils.to_categorical(y_true, num_classes=3)
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.categorical_hinge(y_true, y_pred)
assert loss.shape == (2,)
pos = np.sum(y_true * y_pred, axis=-1)
neg = np.amax((1. - y_true) * y_pred, axis=-1)
assert np.array_equal(loss.numpy(), np.maximum(0., neg - pos + 1.))"
"tf.keras.losses.cosine_similarity(
    y_true, y_pred, axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import cosine_similarity
y_true = [[0., 1.], [1., 1.], [1., 1.]]
y_pred = [[1., 0.], [1., 1.], [-1., -1.]]
loss = tf.keras.losses.cosine_similarity(y_true, y_pred, axis=1)
loss.numpy()
array([-0., -0.999, 0.999], dtype=float32)"
"tf.keras.losses.get(
    identifier
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get
loss = tf.keras.losses.get(""categorical_crossentropy"")
type(loss)
<class 'function'>
<class 'function'>
loss = tf.keras.losses.get(""CategoricalCrossentropy"")
type(loss)
<class '...keras.losses.CategoricalCrossentropy'>
<class '...keras.losses.CategoricalCrossentropy'>"
"tf.keras.metrics.hinge(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import hinge
y_true = np.random.choice([-1, 1], size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.hinge(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    np.mean(np.maximum(1. - y_true * y_pred, 0.), axis=-1))"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_percentage_error
y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_percentage_error
y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_logarithmic_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_logarithmic_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.poisson(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import poisson
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.poisson(y_true, y_pred)
assert loss.shape == (2,)
y_pred = y_pred + 1e-7
assert np.allclose(
    loss.numpy(), np.mean(y_pred - y_true * np.log(y_pred), axis=-1),
    atol=1e-5)"
"tf.keras.metrics.sparse_categorical_crossentropy(
    y_true, y_pred, from_logits=False, axis=-1, ignore_class=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sparse_categorical_crossentropy
y_true = [1, 2]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.0513, 2.303], dtype=float32)"
"tf.keras.metrics.squared_hinge(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import squared_hinge
y_true = np.random.choice([-1, 1], size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.squared_hinge(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    np.mean(np.square(np.maximum(1. - y_true * y_pred, 0.)), axis=-1))"
"tf.make_ndarray(
    tensor
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import make_ndarray
a = tf.constant([[1,2,3],[4,5,6]])
"
"tf.make_tensor_proto(
    values, dtype=None, shape=None, verify_shape=False, allow_broadcast=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import make_tensor_proto
  request = tensorflow_serving.apis.predict_pb2.PredictRequest()
  request.model_spec.name = ""my_model""
  request.model_spec.signature_name = ""serving_default""
  request.inputs[""images""].CopyFrom(tf.make_tensor_proto(X_new))
"
"tf.map_fn(
    fn,
    elems,
    dtype=None,
    parallel_iterations=None,
    back_prop=True,
    swap_memory=False,
    infer_shape=True,
    name=None,
    fn_output_signature=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import map_fn
tf.map_fn(fn=lambda t: tf.range(t, t + 3), elems=tf.constant([3, 5, 2]))
<tf.Tensor: shape=(3, 3), dtype=int32, numpy=
<tf.Tensor: shape=(3, 3), dtype=int32, numpy=
  array([[3, 4, 5],
         [5, 6, 7],
         [2, 3, 4]], dtype=int32)>"
"tf.math.abs(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import abs
x = tf.constant([-2.25, 3.25])
tf.abs(x)
<tf.Tensor: shape=(2,), dtype=float32,
<tf.Tensor: shape=(2,), dtype=float32,
numpy=array([2.25, 3.25], dtype=float32)>"
"tf.math.accumulate_n(
    inputs, shape=None, tensor_dtype=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import accumulate_n
a = tf.constant([[1, 2], [3, 4]])
b = tf.constant([[5, 0], [0, 6]])
tf.math.accumulate_n([a, b, a]).numpy()
array([[ 7, 4],
       [ 6, 14]], dtype=int32)"
"tf.math.acos(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import acos
x = tf.constant([1.0, -0.5, 3.4, 0.2, 0.0, -2], dtype = tf.float32)
tf.math.acos(x)
<tf.Tensor: shape=(6,), dtype=float32,
<tf.Tensor: shape=(6,), dtype=float32,
numpy= array([0. , 2.0943952, nan, 1.3694383, 1.5707964, nan],
dtype=float32)>"
"tf.math.acosh(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import acosh
x = tf.constant([-2, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
tf.math.acosh(x) ==> [nan nan 0. 0.62236255 5.9914584 9.903487 inf]
"
"tf.math.add(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import add
x = [1, 2, 3, 4, 5]
y = 1
tf.add(x, y)
<tf.Tensor: shape=(5,), dtype=int32, numpy=array([2, 3, 4, 5, 6],
<tf.Tensor: shape=(5,), dtype=int32, numpy=array([2, 3, 4, 5, 6],
dtype=int32)>"
"tf.math.add_n(
    inputs, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import add_n
a = tf.constant([[3, 5], [4, 8]])
b = tf.constant([[1, 6], [2, 9]])
tf.math.add_n([a, b, a]).numpy()
array([[ 7, 16],
       [10, 25]], dtype=int32)"
"tf.math.angle(
    input, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import angle
input = tf.constant([-2.25 + 4.75j, 3.25 + 5.75j], dtype=tf.complex64)
tf.math.angle(input).numpy()
"
"tf.math.approx_max_k(
    operand,
    k,
    reduction_dimension=-1,
    recall_target=0.95,
    reduction_input_size_override=-1,
    aggregate_to_topk=True,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import approx_max_k
import tensorflow as tf
@tf.function(jit_compile=True)
def mips(qy, db, k=10, recall_target=0.95):
  dists = tf.einsum('ik,jk->ij', qy, db)
  return tf.nn.approx_max_k(dists, k=k, recall_target=recall_target)
qy = tf.random.uniform((256,128))
db = tf.random.uniform((2048,128))
dot_products, neighbors = mips(qy, db, k=20)"
"tf.math.approx_min_k(
    operand,
    k,
    reduction_dimension=-1,
    recall_target=0.95,
    reduction_input_size_override=-1,
    aggregate_to_topk=True,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import approx_min_k
import tensorflow as tf
@tf.function(jit_compile=True)
def l2_ann(qy, db, half_db_norms, k=10, recall_target=0.95):
  dists = half_db_norms - tf.einsum('ik,jk->ij', qy, db)
  return tf.nn.approx_min_k(dists, k=k, recall_target=recall_target)
qy = tf.random.uniform((256,128))
db = tf.random.uniform((2048,128))
half_db_norms = tf.norm(db, axis=1) / 2
dists, neighbors = l2_ann(qy, db, half_db_norms)"
"tf.math.argmax(
    input,
    axis=None,
    output_type=tf.dtypes.int64,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import argmax
A = tf.constant([2, 20, 30, 3, 6])
<tf.Tensor: shape=(), dtype=int64, numpy=2>
<tf.Tensor: shape=(), dtype=int64, numpy=2>
B = tf.constant([[2, 20, 30, 3, 6], [3, 11, 16, 1, 8],
                 [14, 45, 23, 5, 27]])
tf.math.argmax(B, 0)
<tf.Tensor: shape=(5,), dtype=int64, numpy=array([2, 2, 0, 2, 2])>
<tf.Tensor: shape=(5,), dtype=int64, numpy=array([2, 2, 0, 2, 2])>
tf.math.argmax(B, 1)
<tf.Tensor: shape=(3,), dtype=int64, numpy=array([2, 2, 1])>
<tf.Tensor: shape=(3,), dtype=int64, numpy=array([2, 2, 1])>
C = tf.constant([0, 0, 0, 0])
<tf.Tensor: shape=(), dtype=int64, numpy=0>
<tf.Tensor: shape=(), dtype=int64, numpy=0>"
"tf.math.argmin(
    input,
    axis=None,
    output_type=tf.dtypes.int64,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import argmin
import tensorflow as tf
a = [1, 10, 26.9, 2.8, 166.32, 62.3]
b = tf.math.argmin(input = a)
c = tf.keras.backend.eval(b)
"
"tf.math.asin(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import asin
x = tf.constant([1.047, 0.785])

"
"tf.math.asinh(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import asinh
  x = tf.constant([-float(""inf""), -2, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
  tf.math.asinh(x) ==> [-inf -1.4436355 -0.4812118 0.8813736 1.0159732 5.991471 9.903487 inf]
"
"tf.math.atan(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import atan
x = tf.constant([1.047, 0.785])

"
"tf.math.atan2(
    y, x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import atan2
x = [1., 1.]
y = [1., -1.]
print((tf.math.atan2(y,x) * (180 / np.pi)).numpy())
[ 45. -45.]"
"tf.math.atanh(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import atanh
  x = tf.constant([-float(""inf""), -1, -0.5, 1, 0, 0.5, 10, float(""inf"")])
  tf.math.atanh(x) ==> [nan -inf -0.54930615 inf  0. 0.54930615 nan nan]
"
"tf.math.bessel_i0(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bessel_i0
tf.math.special.bessel_i0([-1., -0.5, 0.5, 1.]).numpy()
array([1.26606588, 1.06348337, 1.06348337, 1.26606588], dtype=float32)"
"tf.math.bessel_i0e(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bessel_i0e
tf.math.special.bessel_i0e([-1., -0.5, 0.5, 1.]).numpy()
array([0.46575961, 0.64503527, 0.64503527, 0.46575961], dtype=float32)"
"tf.math.bessel_i1(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bessel_i1
tf.math.special.bessel_i1([-1., -0.5, 0.5, 1.]).numpy()
array([-0.5651591 , -0.25789431,  0.25789431,  0.5651591 ], dtype=float32)"
"tf.math.bessel_i1e(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bessel_i1e
tf.math.special.bessel_i1e([-1., -0.5, 0.5, 1.]).numpy()
array([-0.20791042, -0.15642083,  0.15642083,  0.20791042], dtype=float32)"
"tf.math.bincount(
    arr,
    weights=None,
    minlength=None,
    maxlength=None,
    dtype=tf.dtypes.int32,
    name=None,
    axis=None,
    binary_output=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bincount
values = tf.constant([1,1,2,3,2,4,4,5])
tf.math.bincount(values) #[0 2 2 1 2 1]
"
"tf.math.ceil(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ceil
tf.math.ceil([-1.7, -1.5, -0.2, 0.2, 1.5, 1.7, 2.0])
<tf.Tensor: shape=(7,), dtype=float32,
<tf.Tensor: shape=(7,), dtype=float32,
numpy=array([-1., -1., -0.,  1.,  2.,  2.,  2.], dtype=float32)>"
"tf.math.confusion_matrix(
    labels,
    predictions,
    num_classes=None,
    weights=None,
    dtype=tf.dtypes.int32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import confusion_matrix
  tf.math.confusion_matrix([1, 2, 4], [2, 2, 4]) ==>
      [[0 0 0 0 0]
       [0 0 1 0 0]
       [0 0 1 0 0]
       [0 0 0 0 0]
       [0 0 0 0 1]]
"
"tf.math.conj(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import conj
x = tf.constant([-2.25 + 4.75j, 3.25 + 5.75j])
tf.math.conj(x)
<tf.Tensor: shape=(2,), dtype=complex128,
<tf.Tensor: shape=(2,), dtype=complex128,
numpy=array([-2.25-4.75j,  3.25-5.75j])>"
"tf.math.cos(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import cos
  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
  tf.math.cos(x) ==> [nan -0.91113025 0.87758255 0.5403023 0.36235774 0.48718765 -0.95215535 nan]
"
"tf.math.cosh(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import cosh
  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 2, 10, float(""inf"")])
  tf.math.cosh(x) ==> [inf 4.0515420e+03 1.1276259e+00 1.5430807e+00 1.8106556e+00 3.7621956e+00 1.1013233e+04 inf]
"
"tf.math.count_nonzero(
    input,
    axis=None,
    keepdims=None,
    dtype=tf.dtypes.int64,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import count_nonzero
x = tf.constant([[0, 1, 0], [1, 1, 0]])
"
"tf.math.cumprod(
    x, axis=0, exclusive=False, reverse=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import cumprod
"
"tf.math.cumsum(
    x, axis=0, exclusive=False, reverse=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import cumsum
x = tf.constant([2, 4, 6, 8])
tf.cumsum(x)
<tf.Tensor: shape=(4,), dtype=int32,
<tf.Tensor: shape=(4,), dtype=int32,
numpy=array([ 2,  6, 12, 20], dtype=int32)>"
"tf.math.divide(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import divide
x = tf.constant([16, 12, 11])
y = tf.constant([4, 6, 2])
tf.divide(x,y)
<tf.Tensor: shape=(3,), dtype=float64,
<tf.Tensor: shape=(3,), dtype=float64,
numpy=array([4. , 2. , 5.5])>"
"tf.math.divide_no_nan(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import divide_no_nan
tf.constant(3.0) / 0.0
<tf.Tensor: shape=(), dtype=float32, numpy=inf>
<tf.Tensor: shape=(), dtype=float32, numpy=inf>
tf.math.divide_no_nan(3.0, 0.0)
<tf.Tensor: shape=(), dtype=float32, numpy=0.0>
<tf.Tensor: shape=(), dtype=float32, numpy=0.0>"
"tf.math.equal(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import equal
x = tf.constant([2, 4])
y = tf.constant(2)
tf.math.equal(x, y)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  False])>
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  False])>"
"tf.math.erf(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import erf
tf.math.erf([[1.0, 2.0, 3.0], [0.0, -1.0, -2.0]])
<tf.Tensor: shape=(2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 3), dtype=float32, numpy=
array([[ 0.8427007,  0.9953223,  0.999978 ],
       [ 0.       , -0.8427007, -0.9953223]], dtype=float32)>"
"tf.math.erfcinv(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import erfcinv
tf.math.erfcinv([0., 0.2, 1., 1.5, 2.])
<tf.Tensor: shape=(5,), dtype=float32, numpy=
<tf.Tensor: shape=(5,), dtype=float32, numpy=
array([       inf,  0.9061935, -0.       , -0.4769363,       -inf],
      dtype=float32)>"
"tf.math.exp(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import exp
x = tf.constant(2.0)
tf.math.exp(x)
<tf.Tensor: shape=(), dtype=float32, numpy=7.389056>
<tf.Tensor: shape=(), dtype=float32, numpy=7.389056>"
"tf.math.expm1(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import expm1
  x = tf.constant(2.0)
  tf.math.expm1(x) ==> 6.389056

  x = tf.constant([2.0, 8.0])
  tf.math.expm1(x) ==> array([6.389056, 2979.958], dtype=float32)

  x = tf.constant(1 + 1j)
  tf.math.expm1(x) ==> (0.46869393991588515+2.2873552871788423j)
"
"tf.math.floor(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import floor
x = tf.constant([1.3324, -1.5, 5.555, -2.532, 0.99, float(""inf"")])
tf.floor(x).numpy()
array([ 1., -2.,  5., -3.,  0., inf], dtype=float32)"
"tf.math.greater(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import greater
x = tf.constant([5, 4, 6])
y = tf.constant([5, 2, 5])
tf.math.greater(x, y) ==> [False, True, True]

x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.greater(x, y) ==> [False, False, True]
"
"tf.math.greater_equal(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import greater_equal
x = tf.constant([5, 4, 6, 7])
y = tf.constant([5, 2, 5, 10])
tf.math.greater_equal(x, y) ==> [True, True, True, False]

x = tf.constant([5, 4, 6, 7])
y = tf.constant([5])
tf.math.greater_equal(x, y) ==> [True, False, True, True]
"
"tf.math.imag(
    input, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import imag
x = tf.constant([-2.25 + 4.75j, 3.25 + 5.75j])
"
"tf.math.invert_permutation(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import invert_permutation
invert_permutation(x) ==> [2, 4, 3, 0, 1]
"
"tf.math.is_finite(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import is_finite
x = tf.constant([5.0, 4.8, 6.8, np.inf, np.nan])
tf.math.is_finite(x) ==> [True, True, True, False, False]
"
"tf.math.is_inf(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import is_inf
x = tf.constant([5.0, np.inf, 6.8, np.inf])
tf.math.is_inf(x) ==> [False, True, False, True]
"
"tf.math.is_nan(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import is_nan
x = tf.constant([5.0, np.nan, 6.8, np.nan, np.inf])
tf.math.is_nan(x) ==> [False, True, False, True, False]
"
"tf.math.is_non_decreasing(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import is_non_decreasing
x1 = tf.constant([1.0, 1.0, 3.0])
tf.math.is_non_decreasing(x1)
<tf.Tensor: shape=(), dtype=bool, numpy=True>
<tf.Tensor: shape=(), dtype=bool, numpy=True>
x2 = tf.constant([3.0, 1.0, 2.0])
tf.math.is_non_decreasing(x2)
<tf.Tensor: shape=(), dtype=bool, numpy=False>
<tf.Tensor: shape=(), dtype=bool, numpy=False>"
"tf.math.is_strictly_increasing(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import is_strictly_increasing
x1 = tf.constant([1.0, 2.0, 3.0])
tf.math.is_strictly_increasing(x1)
<tf.Tensor: shape=(), dtype=bool, numpy=True>
<tf.Tensor: shape=(), dtype=bool, numpy=True>
x2 = tf.constant([3.0, 1.0, 2.0])
tf.math.is_strictly_increasing(x2)
<tf.Tensor: shape=(), dtype=bool, numpy=False>
<tf.Tensor: shape=(), dtype=bool, numpy=False>"
"tf.math.less(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import less
x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.less(x, y) ==> [False, True, False]

x = tf.constant([5, 4, 6])
y = tf.constant([5, 6, 7])
tf.math.less(x, y) ==> [False, True, True]
"
"tf.math.less_equal(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import less_equal
x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.less_equal(x, y) ==> [True, True, False]

x = tf.constant([5, 4, 6])
y = tf.constant([5, 6, 6])
tf.math.less_equal(x, y) ==> [True, True, True]
"
"tf.math.lgamma(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import lgamma
x = tf.constant([0, 0.5, 1, 4.5, -4, -5.6])
tf.math.lgamma(x) ==> [inf, 0.5723649, 0., 2.4537368, inf, -4.6477685]
"
"tf.math.log(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import log
x = tf.constant([0, 0.5, 1, 5])
tf.math.log(x)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([      -inf, -0.6931472,  0.       ,  1.609438 ], dtype=float32)>
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([      -inf, -0.6931472,  0.       ,  1.609438 ], dtype=float32)>"
"tf.math.log1p(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import log1p
x = tf.constant([0, 0.5, 1, 5])
tf.math.log1p(x)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.       , 0.4054651, 0.6931472, 1.7917595], dtype=float32)>
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.       , 0.4054651, 0.6931472, 1.7917595], dtype=float32)>"
"tf.math.log_sigmoid(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import log_sigmoid
x = tf.constant([0.0, 1.0, 50.0, 100.0])
tf.math.log_sigmoid(x)
<tf.Tensor: shape=(4,), dtype=float32, numpy=
<tf.Tensor: shape=(4,), dtype=float32, numpy=
array([-6.9314718e-01, -3.1326169e-01, -1.9287499e-22, -0.0000000e+00],
      dtype=float32)>"
"tf.math.logical_and(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import logical_and
>>> a = tf.constant([True])
>>> b = tf.constant([False])
>>> tf.math.logical_and(a, b)
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>
>>> a & b
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>
"
"tf.math.logical_not(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import logical_not
tf.math.logical_not(tf.constant([True, False]))
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>"
"tf.math.logical_or(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import logical_or
>>> a = tf.constant([True])
>>> b = tf.constant([False])
>>> tf.math.logical_or(a, b)
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>
>>> a | b
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>
"
"tf.math.logical_xor(
    x, y, name='LogicalXor'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import logical_xor
a = tf.constant([True])
b = tf.constant([False])
tf.math.logical_xor(a, b)
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>"
"tf.math.maximum(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import maximum
x = tf.constant([0., 0., 0., 0.])
y = tf.constant([-2., 0., 2., 5.])
tf.math.maximum(x, y)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 2., 5.], dtype=float32)>
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 2., 5.], dtype=float32)>"
"tf.math.minimum(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import minimum
x = tf.constant([0., 0., 0., 0.])
y = tf.constant([-5., -2., 0., 3.])
tf.math.minimum(x, y)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([-5., -2., 0., 0.], dtype=float32)>
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([-5., -2., 0., 0.], dtype=float32)>"
"tf.math.multiply(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import multiply
x = tf.constant(([1, 2, 3, 4]))
tf.math.multiply(x, x)
<tf.Tensor: shape=(4,), dtype=..., numpy=array([ 1,  4,  9, 16], dtype=int32)>
<tf.Tensor: shape=(4,), dtype=..., numpy=array([ 1,  4,  9, 16], dtype=int32)>"
"tf.math.not_equal(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import not_equal
x = tf.constant([2, 4])
y = tf.constant(2)
tf.math.not_equal(x, y)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>"
"tf.math.pow(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import pow
x = tf.constant([[2, 2], [3, 3]])
y = tf.constant([[8, 16], [2, 3]])
"
"tf.math.real(
    input, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import real
x = tf.constant([-2.25 + 4.75j, 3.25 + 5.75j])
"
"tf.math.reciprocal_no_nan(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reciprocal_no_nan
x = tf.constant([2.0, 0.5, 0, 1], dtype=tf.float32)
"
"tf.math.reduce_all(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_all
>>> x = tf.constant([[True,  True], [False, False]])
>>> tf.math.reduce_all(x)
<tf.Tensor: shape=(), dtype=bool, numpy=False>
<tf.Tensor: shape=(), dtype=bool, numpy=False>
>>> tf.math.reduce_all(x, 0)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False, False])>
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False, False])>
>>> tf.math.reduce_all(x, 1)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>
"
"tf.math.reduce_any(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_any
>>> x = tf.constant([[True,  True], [False, False]])
>>> tf.reduce_any(x)
<tf.Tensor: shape=(), dtype=bool, numpy=True>
<tf.Tensor: shape=(), dtype=bool, numpy=True>
>>> tf.reduce_any(x, 0)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  True])>
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  True])>
>>> tf.reduce_any(x, 1)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>
"
"tf.math.reduce_euclidean_norm(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_euclidean_norm
y = tf.constant([[1, 2, 3], [1, 1, 1]], dtype = tf.float32)
"
"tf.math.reduce_logsumexp(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_logsumexp
x = tf.constant([[0., 0., 0.], [0., 0., 0.]])
"
"tf.math.reduce_max(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_max
>>> x = tf.constant([5, 1, 2, 4])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=int32, numpy=5>
<tf.Tensor: shape=(), dtype=int32, numpy=5>
>>> x = tf.constant([-5, -1, -2, -4])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=int32, numpy=-1>
<tf.Tensor: shape=(), dtype=int32, numpy=-1>
>>> x = tf.constant([4, float('nan')])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=float32, numpy=nan>
<tf.Tensor: shape=(), dtype=float32, numpy=nan>
>>> x = tf.constant([float('nan'), float('nan')])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=float32, numpy=nan>
<tf.Tensor: shape=(), dtype=float32, numpy=nan>
>>> x = tf.constant([float('-inf'), float('inf')])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=float32, numpy=inf>
<tf.Tensor: shape=(), dtype=float32, numpy=inf>
"
"tf.math.reduce_mean(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_mean
x = tf.constant([[1., 1.], [2., 2.]])
tf.reduce_mean(x)
<tf.Tensor: shape=(), dtype=float32, numpy=1.5>
<tf.Tensor: shape=(), dtype=float32, numpy=1.5>
tf.reduce_mean(x, 0)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.5, 1.5], dtype=float32)>
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.5, 1.5], dtype=float32)>
tf.reduce_mean(x, 1)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 2.], dtype=float32)>
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 2.], dtype=float32)>"
"tf.math.reduce_min(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_min
a = tf.constant([
  [[1, 2], [3, 4]],
  [[1, 2], [3, 4]]
])
tf.reduce_min(a)
<tf.Tensor: shape=(), dtype=int32, numpy=1>
<tf.Tensor: shape=(), dtype=int32, numpy=1>"
"tf.math.reduce_prod(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_prod
>>> x = tf.constant([[1., 2.], [3., 4.]])
>>> tf.math.reduce_prod(x)
<tf.Tensor: shape=(), dtype=float32, numpy=24.>
<tf.Tensor: shape=(), dtype=float32, numpy=24.>
>>> tf.math.reduce_prod(x, 0)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([3., 8.], dtype=float32)>
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([3., 8.], dtype=float32)>
>>> tf.math.reduce_prod(x, 1)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 12.],
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 12.],
dtype=float32)>
"
"tf.math.reduce_std(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_std
x = tf.constant([[1., 2.], [3., 4.]])
tf.math.reduce_std(x)
<tf.Tensor: shape=(), dtype=float32, numpy=1.118034>
<tf.Tensor: shape=(), dtype=float32, numpy=1.118034>
tf.math.reduce_std(x, 0)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)>
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)>
tf.math.reduce_std(x, 1)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.5, 0.5], dtype=float32)>
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.5, 0.5], dtype=float32)>"
"tf.math.reduce_sum(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_sum
>>> x = tf.constant([[1, 1, 1], [1, 1, 1]])
>>> x.numpy()
array([[1, 1, 1],
       [1, 1, 1]], dtype=int32)
>>> tf.reduce_sum(x).numpy()
6
>>> tf.reduce_sum(x, 0).numpy()
array([2, 2, 2], dtype=int32)
>>> tf.reduce_sum(x, 1).numpy()
array([3, 3], dtype=int32)
>>> tf.reduce_sum(x, 1, keepdims=True).numpy()
array([[3],
       [3]], dtype=int32)
>>> tf.reduce_sum(x, [0, 1]).numpy()
6
"
"tf.math.reduce_variance(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_variance
x = tf.constant([[1., 2.], [3., 4.]])
tf.math.reduce_variance(x)
<tf.Tensor: shape=(), dtype=float32, numpy=1.25>
<tf.Tensor: shape=(), dtype=float32, numpy=1.25>
tf.math.reduce_variance(x, 0)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], ...)>
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], ...)>
tf.math.reduce_variance(x, 1)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.25, 0.25], ...)>
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.25, 0.25], ...)>"
"tf.math.rint(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import rint
rint(-1.5) ==> -2.0
rint(0.5000001) ==> 1.0
rint([-1.7, -1.5, -0.2, 0.2, 1.5, 1.7, 2.0]) ==> [-2., -2., -0., 0., 2., 2., 2.]
"
"tf.math.round(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import round
x = tf.constant([0.9, 2.5, 2.3, 1.5, -4.5])
"
"tf.math.rsqrt(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import rsqrt
x = tf.constant([2., 0., -2.])
tf.math.rsqrt(x)
<tf.Tensor: shape=(3,), dtype=float32,
<tf.Tensor: shape=(3,), dtype=float32,
numpy=array([0.707, inf, nan], dtype=float32)>"
"tf.math.scalar_mul(
    scalar, x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import scalar_mul
x = tf.reshape(tf.range(30, dtype=tf.float32), [10, 3])
with tf.GradientTape() as g:
  g.watch(x)
  z = tf.math.scalar_mul(10.0, y)"
"tf.math.segment_max(
    data, segment_ids, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import segment_max
c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
tf.math.segment_max(c, tf.constant([0, 0, 1])).numpy()
array([[4, 3, 3, 4],
       [5, 6, 7, 8]], dtype=int32)"
"tf.math.segment_mean(
    data, segment_ids, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import segment_mean
c = tf.constant([[1.0,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
tf.math.segment_mean(c, tf.constant([0, 0, 1])).numpy()
array([[2.5, 2.5, 2.5, 2.5],
       [5., 6., 7., 8.]], dtype=float32)"
"tf.math.segment_min(
    data, segment_ids, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import segment_min
c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
tf.math.segment_min(c, tf.constant([0, 0, 1])).numpy()
array([[1, 2, 2, 1],
       [5, 6, 7, 8]], dtype=int32)"
"tf.math.segment_prod(
    data, segment_ids, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import segment_prod
c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
tf.math.segment_prod(c, tf.constant([0, 0, 1])).numpy()
array([[4, 6, 6, 4],
       [5, 6, 7, 8]], dtype=int32)"
"tf.math.segment_sum(
    data, segment_ids, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import segment_sum
c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
tf.math.segment_sum(c, tf.constant([0, 0, 1])).numpy()
array([[5, 5, 5, 5],
       [5, 6, 7, 8]], dtype=int32)"
"tf.math.sigmoid(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sigmoid
x = tf.constant([0.0, 1.0, 50.0, 100.0])
tf.math.sigmoid(x)
<tf.Tensor: shape=(4,), dtype=float32,
<tf.Tensor: shape=(4,), dtype=float32,
numpy=array([0.5, 0.7310586, 1.0, 1.0], dtype=float32)>"
"tf.math.sign(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sign
tf.math.sign([0., 2., -3.])
<tf.Tensor: shape=(3,), dtype=float32,
<tf.Tensor: shape=(3,), dtype=float32,
numpy=array([ 0.,  1., -1.], dtype=float32)>"
"tf.math.sin(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sin
  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 200, 10, float(""inf"")])
  tf.math.sin(x) ==> [nan -0.4121185 -0.47942555 0.84147096 0.9320391 -0.87329733 -0.54402107 nan]
"
"tf.math.sinh(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sinh
  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 2, 10, float(""inf"")])
  tf.math.sinh(x) ==> [-inf -4.0515420e+03 -5.2109528e-01 1.1752012e+00 1.5094614e+00 3.6268604e+00 1.1013232e+04 inf]
"
"tf.nn.softmax(
    logits, axis=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis, keepdims=True)
"
"tf.math.softplus(
    features, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import softplus
import tensorflow as tf
tf.math.softplus(tf.range(0, 2, dtype=tf.float32)).numpy()
array([0.6931472, 1.3132616], dtype=float32)"
"tf.math.bessel_i0(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bessel_i0
tf.math.special.bessel_i0([-1., -0.5, 0.5, 1.]).numpy()
array([1.26606588, 1.06348337, 1.06348337, 1.26606588], dtype=float32)"
"tf.math.bessel_i0e(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bessel_i0e
tf.math.special.bessel_i0e([-1., -0.5, 0.5, 1.]).numpy()
array([0.46575961, 0.64503527, 0.64503527, 0.46575961], dtype=float32)"
"tf.math.bessel_i1(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bessel_i1
tf.math.special.bessel_i1([-1., -0.5, 0.5, 1.]).numpy()
array([-0.5651591 , -0.25789431,  0.25789431,  0.5651591 ], dtype=float32)"
"tf.math.bessel_i1e(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bessel_i1e
tf.math.special.bessel_i1e([-1., -0.5, 0.5, 1.]).numpy()
array([-0.20791042, -0.15642083,  0.15642083,  0.20791042], dtype=float32)"
"tf.math.special.bessel_j0(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bessel_j0
tf.math.special.bessel_j0([0.5, 1., 2., 4.]).numpy()
array([ 0.93846981,  0.76519769,  0.22389078, -0.39714981], dtype=float32)"
"tf.math.special.bessel_j1(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bessel_j1
tf.math.special.bessel_j1([0.5, 1., 2., 4.]).numpy()
array([ 0.24226846,  0.44005059,  0.57672481, -0.06604333], dtype=float32)"
"tf.math.special.bessel_k0(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bessel_k0
tf.math.special.bessel_k0([0.5, 1., 2., 4.]).numpy()
array([0.92441907, 0.42102444, 0.11389387, 0.01115968], dtype=float32)"
"tf.math.special.bessel_k0e(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bessel_k0e
tf.math.special.bessel_k0e([0.5, 1., 2., 4.]).numpy()
array([1.52410939, 1.14446308, 0.84156822, 0.60929767], dtype=float32)"
"tf.math.special.bessel_k1(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bessel_k1
tf.math.special.bessel_k1([0.5, 1., 2., 4.]).numpy()
array([1.65644112, 0.60190723, 0.13986588, 0.0124835 ], dtype=float32)"
"tf.math.special.bessel_k1e(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bessel_k1e
tf.math.special.bessel_k1e([0.5, 1., 2., 4.]).numpy()
array([2.73100971, 1.63615349, 1.03347685, 0.68157595], dtype=float32)"
"tf.math.special.bessel_y0(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bessel_y0
tf.math.special.bessel_y0([0.5, 1., 2., 4.]).numpy()
array([-0.44451873,  0.08825696,  0.51037567, -0.01694074], dtype=float32)"
"tf.math.special.bessel_y1(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bessel_y1
tf.math.special.bessel_y1([0.5, 1., 2., 4.]).numpy()
array([-1.47147239, -0.78121282, -0.10703243,  0.39792571], dtype=float32)"
"tf.math.special.dawsn(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import dawsn
>>> tf.math.special.dawsn([-1., -0.5, 0.5, 1.]).numpy()
array([-0.5380795, -0.4244364, 0.4244364,  0.5380795], dtype=float32)
"
"tf.math.special.expint(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import expint
tf.math.special.expint([1., 1.1, 2.1, 4.1]).numpy()
array([ 1.8951179,  2.1673784,  5.3332353, 21.048464], dtype=float32)"
"tf.math.special.fresnel_cos(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import fresnel_cos
>>> tf.math.special.fresnel_cos([-1., -0.1, 0.1, 1.]).numpy()
array([-0.7798934 , -0.09999753,  0.09999753,  0.7798934 ], dtype=float32)
"
"tf.math.special.fresnel_sin(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import fresnel_sin
tf.math.special.fresnel_sin([-1., -0.1, 0.1, 1.]).numpy()
array([-0.43825912, -0.00052359,  0.00052359,  0.43825912], dtype=float32)"
"tf.math.special.spence(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import spence
tf.math.special.spence([0.5, 1., 2., 3.]).numpy()
array([ 0.58224034,  0.        , -0.82246685, -1.4367464], dtype=float32)"
"tf.math.sqrt(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sqrt
x = tf.constant([[4.0], [16.0]])
tf.sqrt(x)
<tf.Tensor: shape=(2, 1), dtype=float32, numpy=
<tf.Tensor: shape=(2, 1), dtype=float32, numpy=
  array([[2.],
         [4.]], dtype=float32)>
y = tf.constant([[-4.0], [16.0]])
tf.sqrt(y)
<tf.Tensor: shape=(2, 1), dtype=float32, numpy=
<tf.Tensor: shape=(2, 1), dtype=float32, numpy=
  array([[nan],
         [ 4.]], dtype=float32)>
z = tf.constant([[-1.0], [16.0]], dtype=tf.complex128)
tf.sqrt(z)
<tf.Tensor: shape=(2, 1), dtype=complex128, numpy=
<tf.Tensor: shape=(2, 1), dtype=complex128, numpy=
  array([[0.0+1.j],
         [4.0+0.j]])>"
"tf.math.square(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import square
tf.math.square([-2., 0., 3.])
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([4., 0., 9.], dtype=float32)>
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([4., 0., 9.], dtype=float32)>"
"tf.math.subtract(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import subtract
x = [1, 2, 3, 4, 5]
y = 1
tf.subtract(x, y)
<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 1, 2, 3, 4], dtype=int32)>
<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 1, 2, 3, 4], dtype=int32)>
tf.subtract(y, x)
<tf.Tensor: shape=(5,), dtype=int32,
<tf.Tensor: shape=(5,), dtype=int32,
numpy=array([ 0, -1, -2, -3, -4], dtype=int32)>"
"tf.math.tan(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import tan
  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
  tf.math.tan(x) ==> [nan 0.45231566 -0.5463025 1.5574077 2.572152 -1.7925274 0.32097113 nan]
"
"tf.math.top_k(
    input, k=1, sorted=True, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import top_k
result = tf.math.top_k([1, 2, 98, 1, 1, 99, 3, 1, 3, 96, 4, 1],
                        k=3)
result.values.numpy()
array([99, 98, 96], dtype=int32)
result.indices.numpy()
array([5, 2, 9], dtype=int32)"
"tf.math.unsorted_segment_max(
    data, segment_ids, num_segments, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import unsorted_segment_max
c = tf.constant([[1,2,3,4], [5,6,7,8], [4,3,2,1]])
tf.math.unsorted_segment_max(c, tf.constant([0, 1, 0]), num_segments=2).numpy()
array([[4, 3, 3, 4],
       [5,  6, 7, 8]], dtype=int32)"
"tf.math.unsorted_segment_min(
    data, segment_ids, num_segments, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import unsorted_segment_min
c = tf.constant([[1,2,3,4], [5,6,7,8], [4,3,2,1]])
tf.math.unsorted_segment_min(c, tf.constant([0, 1, 0]), num_segments=2).numpy()
array([[1, 2, 2, 1],
       [5, 6, 7, 8]], dtype=int32)"
"tf.math.unsorted_segment_prod(
    data, segment_ids, num_segments, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import unsorted_segment_prod
c = tf.constant([[1,2,3,4], [5,6,7,8], [4,3,2,1]])
tf.math.unsorted_segment_prod(c, tf.constant([0, 1, 0]), num_segments=2).numpy()
array([[4, 6, 6, 4],
       [5, 6, 7, 8]], dtype=int32)"
"tf.math.unsorted_segment_sum(
    data, segment_ids, num_segments, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import unsorted_segment_sum
c = [[1,2,3,4], [5,6,7,8], [4,3,2,1]]
tf.math.unsorted_segment_sum(c, [0, 1, 0], num_segments=2).numpy()
array([[5, 5, 5, 5],
       [5, 6, 7, 8]], dtype=int32)"
"tf.math.xlog1py(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import xlog1py
tf.math.xlog1py(0., 1.)
<tf.Tensor: shape=(), dtype=float32, numpy=0.>
<tf.Tensor: shape=(), dtype=float32, numpy=0.>
tf.math.xlog1py(1., 1.)
<tf.Tensor: shape=(), dtype=float32, numpy=0.6931472>
<tf.Tensor: shape=(), dtype=float32, numpy=0.6931472>
tf.math.xlog1py(2., 2.)
<tf.Tensor: shape=(), dtype=float32, numpy=2.1972246>
<tf.Tensor: shape=(), dtype=float32, numpy=2.1972246>
tf.math.xlog1py(0., -1.)
<tf.Tensor: shape=(), dtype=float32, numpy=0.>
<tf.Tensor: shape=(), dtype=float32, numpy=0.>"
"tf.math.zero_fraction(
    value, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import zero_fraction
    z = tf.nn.relu(...)
    summ = tf.compat.v1.summary.scalar('sparsity', tf.nn.zero_fraction(z))
"
"tf.linalg.matmul(
    a,
    b,
    transpose_a=False,
    transpose_b=False,
    adjoint_a=False,
    adjoint_b=False,
    a_is_sparse=False,
    b_is_sparse=False,
    output_type=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import matmul
a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
array([[1, 2, 3],
       [4, 5, 6]], dtype=int32)>
b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
array([[ 7,  8],
       [ 9, 10],
       [11, 12]], dtype=int32)>
c = tf.matmul(a, b)
<tf.Tensor: shape=(2, 2), dtype=int32, numpy=
<tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[ 58,  64],
       [139, 154]], dtype=int32)>"
"tf.math.maximum(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import maximum
x = tf.constant([0., 0., 0., 0.])
y = tf.constant([-2., 0., 2., 5.])
tf.math.maximum(x, y)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 2., 5.], dtype=float32)>
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 2., 5.], dtype=float32)>"
"tf.meshgrid(
    *args, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import meshgrid
x = [1, 2, 3]
y = [4, 5, 6]
X, Y = tf.meshgrid(x, y)
"
"tf.keras.metrics.AUC(
    num_thresholds=200,
    curve='ROC',
    summation_method='interpolation',
    name=None,
    dtype=None,
    thresholds=None,
    multi_label=False,
    num_labels=None,
    label_weights=None,
    from_logits=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import AUC
m = tf.keras.metrics.AUC(num_thresholds=3)
m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])
m.result().numpy()
0.75"
"tf.keras.metrics.Accuracy(
    name='accuracy', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Accuracy
m = tf.keras.metrics.Accuracy()
m.update_state([[1], [2], [3], [4]], [[0], [2], [3], [4]])
m.result().numpy()
0.75"
"tf.keras.metrics.BinaryAccuracy(
    name='binary_accuracy', dtype=None, threshold=0.5
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import BinaryAccuracy
m = tf.keras.metrics.BinaryAccuracy()
m.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]])
m.result().numpy()
0.75"
"tf.keras.metrics.BinaryCrossentropy(
    name='binary_crossentropy',
    dtype=None,
    from_logits=False,
    label_smoothing=0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import BinaryCrossentropy
m = tf.keras.metrics.BinaryCrossentropy()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
0.81492424"
"tf.keras.metrics.CategoricalAccuracy(
    name='categorical_accuracy', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CategoricalAccuracy
m = tf.keras.metrics.CategoricalAccuracy()
m.update_state([[0, 0, 1], [0, 1, 0]], [[0.1, 0.9, 0.8],
                [0.05, 0.95, 0]])
m.result().numpy()
0.5"
"tf.keras.metrics.CategoricalCrossentropy(
    name='categorical_crossentropy',
    dtype=None,
    from_logits=False,
    label_smoothing=0,
    axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CategoricalCrossentropy
m = tf.keras.metrics.CategoricalCrossentropy()
m.update_state([[0, 1, 0], [0, 0, 1]],
               [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])
m.result().numpy()
1.1769392"
"tf.keras.metrics.CategoricalHinge(
    name='categorical_hinge', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CategoricalHinge
m = tf.keras.metrics.CategoricalHinge()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
1.4000001"
"tf.keras.metrics.CosineSimilarity(
    name='cosine_similarity', dtype=None, axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CosineSimilarity
m = tf.keras.metrics.CosineSimilarity(axis=1)
m.update_state([[0., 1.], [1., 1.]], [[1., 0.], [1., 1.]])
m.result().numpy()
0.49999997"
"tf.keras.metrics.FalseNegatives(
    thresholds=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import FalseNegatives
m = tf.keras.metrics.FalseNegatives()
m.update_state([0, 1, 1, 1], [0, 1, 0, 0])
m.result().numpy()
2.0"
"tf.keras.metrics.FalsePositives(
    thresholds=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import FalsePositives
m = tf.keras.metrics.FalsePositives()
m.update_state([0, 1, 0, 0], [0, 0, 1, 1])
m.result().numpy()
2.0"
"tf.keras.metrics.Hinge(
    name='hinge', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Hinge
m = tf.keras.metrics.Hinge()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
1.3"
"tf.keras.metrics.KLDivergence(
    name='kullback_leibler_divergence', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import KLDivergence
m = tf.keras.metrics.KLDivergence()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
0.45814306"
"tf.keras.metrics.LogCoshError(
    name='logcosh', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LogCoshError
m = tf.keras.metrics.LogCoshError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.10844523"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_percentage_error
y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_logarithmic_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.Mean(
    name='mean', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Mean
m = tf.keras.metrics.Mean()
m.update_state([1, 3, 5, 7])
m.result().numpy()
4.0
m.reset_state()
m.update_state([1, 3, 5, 7], sample_weight=[1, 1, 0, 0])
m.result().numpy()
2.0"
"tf.keras.metrics.MeanAbsoluteError(
    name='mean_absolute_error', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanAbsoluteError
m = tf.keras.metrics.MeanAbsoluteError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.25"
"tf.keras.metrics.MeanAbsolutePercentageError(
    name='mean_absolute_percentage_error', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanAbsolutePercentageError
m = tf.keras.metrics.MeanAbsolutePercentageError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
250000000.0"
"tf.keras.metrics.MeanMetricWrapper(
    fn, name=None, dtype=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanMetricWrapper
def accuracy(y_true, y_pred):
  return tf.cast(tf.math.equal(y_true, y_pred), tf.float32)

accuracy_metric = tf.keras.metrics.MeanMetricWrapper(fn=accuracy)

keras_model.compile(..., metrics=accuracy_metric)
"
"tf.keras.metrics.MeanRelativeError(
    normalizer, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanRelativeError
m = tf.keras.metrics.MeanRelativeError(normalizer=[1, 3, 2, 3])
m.update_state([1, 3, 2, 3], [2, 4, 6, 8])"
"tf.keras.metrics.MeanSquaredError(
    name='mean_squared_error', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanSquaredError
m = tf.keras.metrics.MeanSquaredError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.25"
"tf.keras.metrics.MeanSquaredLogarithmicError(
    name='mean_squared_logarithmic_error', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanSquaredLogarithmicError
m = tf.keras.metrics.MeanSquaredLogarithmicError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.12011322"
"tf.keras.metrics.MeanTensor(
    name='mean_tensor', dtype=None, shape=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanTensor
m = tf.keras.metrics.MeanTensor()
m.update_state([0, 1, 2, 3])
m.update_state([4, 5, 6, 7])
m.result().numpy()
array([2., 3., 4., 5.], dtype=float32)"
"tf.keras.metrics.Metric(
    name=None, dtype=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Metric
m = SomeMetric(...)
for input in ...:
  m.update_state(input)
print('Final result: ', m.result().numpy())
"
"tf.keras.metrics.Poisson(
    name='poisson', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Poisson
m = tf.keras.metrics.Poisson()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.49999997"
"tf.keras.metrics.Precision(
    thresholds=None, top_k=None, class_id=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Precision
m = tf.keras.metrics.Precision()
m.update_state([0, 1, 1, 1], [1, 0, 1, 1])
m.result().numpy()
0.6666667"
"tf.keras.metrics.PrecisionAtRecall(
    recall, num_thresholds=200, class_id=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import PrecisionAtRecall
m = tf.keras.metrics.PrecisionAtRecall(0.5)
m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])
m.result().numpy()
0.5"
"tf.keras.metrics.Recall(
    thresholds=None, top_k=None, class_id=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Recall
m = tf.keras.metrics.Recall()
m.update_state([0, 1, 1, 1], [1, 0, 1, 1])
m.result().numpy()
0.6666667"
"tf.keras.metrics.RecallAtPrecision(
    precision, num_thresholds=200, class_id=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RecallAtPrecision
m = tf.keras.metrics.RecallAtPrecision(0.8)
m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])
m.result().numpy()
0.5"
"tf.keras.metrics.RootMeanSquaredError(
    name='root_mean_squared_error', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RootMeanSquaredError
m = tf.keras.metrics.RootMeanSquaredError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.5"
"tf.keras.metrics.SensitivityAtSpecificity(
    specificity, num_thresholds=200, class_id=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SensitivityAtSpecificity
m = tf.keras.metrics.SensitivityAtSpecificity(0.5)
m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])
m.result().numpy()
0.5"
"tf.keras.metrics.SparseCategoricalCrossentropy(
    name: str = 'sparse_categorical_crossentropy',
    dtype: Optional[Union[str, tf.dtypes.DType]] = None,
    from_logits: bool = False,
    ignore_class: Optional[int] = None,
    axis: int = -1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SparseCategoricalCrossentropy
m = tf.keras.metrics.SparseCategoricalCrossentropy()
m.update_state([1, 2],
               [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])
m.result().numpy()
1.1769392"
"tf.keras.metrics.SparseTopKCategoricalAccuracy(
    k=5, name='sparse_top_k_categorical_accuracy', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SparseTopKCategoricalAccuracy
m = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)
m.update_state([2, 1], [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])
m.result().numpy()
0.5"
"tf.keras.metrics.SpecificityAtSensitivity(
    sensitivity, num_thresholds=200, class_id=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SpecificityAtSensitivity
m = tf.keras.metrics.SpecificityAtSensitivity(0.5)
m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])
m.result().numpy()
0.66666667"
"tf.keras.metrics.SquaredHinge(
    name='squared_hinge', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SquaredHinge
m = tf.keras.metrics.SquaredHinge()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
1.86"
"tf.keras.metrics.Sum(
    name='sum', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Sum
m = tf.keras.metrics.Sum()
m.update_state([1, 3, 5, 7])
m.result().numpy()
16.0"
"tf.keras.metrics.TopKCategoricalAccuracy(
    k=5, name='top_k_categorical_accuracy', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TopKCategoricalAccuracy
m = tf.keras.metrics.TopKCategoricalAccuracy(k=1)
m.update_state([[0, 0, 1], [0, 1, 0]],
               [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])
m.result().numpy()
0.5"
"tf.keras.metrics.TrueNegatives(
    thresholds=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TrueNegatives
m = tf.keras.metrics.TrueNegatives()
m.update_state([0, 1, 0, 0], [1, 1, 0, 0])
m.result().numpy()
2.0"
"tf.keras.metrics.TruePositives(
    thresholds=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TruePositives
m = tf.keras.metrics.TruePositives()
m.update_state([0, 1, 1, 1], [1, 0, 1, 1])
m.result().numpy()
2.0"
"tf.keras.metrics.binary_accuracy(
    y_true, y_pred, threshold=0.5
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import binary_accuracy
y_true = [[1], [1], [0], [0]]
y_pred = [[1], [1], [0], [0]]
m = tf.keras.metrics.binary_accuracy(y_true, y_pred)
assert m.shape == (4,)
m.numpy()
array([1., 1., 1., 1.], dtype=float32)"
"tf.keras.metrics.binary_crossentropy(
    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import binary_crossentropy
y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.916 , 0.714], dtype=float32)"
"tf.keras.metrics.binary_focal_crossentropy(
    y_true,
    y_pred,
    apply_class_balancing=False,
    alpha=0.25,
    gamma=2.0,
    from_logits=False,
    label_smoothing=0.0,
    axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import binary_focal_crossentropy
y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
loss = tf.keras.losses.binary_focal_crossentropy(y_true, y_pred,
                                                 gamma=2)
assert loss.shape == (2,)
loss.numpy()
array([0.330, 0.206], dtype=float32)"
"tf.keras.metrics.categorical_accuracy(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import categorical_accuracy
y_true = [[0, 0, 1], [0, 1, 0]]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.categorical_accuracy(y_true, y_pred)
assert m.shape == (2,)
m.numpy()
array([0., 1.], dtype=float32)"
"tf.keras.metrics.categorical_crossentropy(
    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import categorical_crossentropy
y_true = [[0, 1, 0], [0, 0, 1]]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.0513, 2.303], dtype=float32)"
"tf.keras.metrics.get(
    identifier
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get
metric = tf.keras.metrics.get(""categorical_crossentropy"")
type(metric)
<class 'function'>
<class 'function'>
metric = tf.keras.metrics.get(""CategoricalCrossentropy"")
type(metric)
<class '...metrics.CategoricalCrossentropy'>
<class '...metrics.CategoricalCrossentropy'>"
"tf.keras.metrics.hinge(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import hinge
y_true = np.random.choice([-1, 1], size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.hinge(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    np.mean(np.maximum(1. - y_true * y_pred, 0.), axis=-1))"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_percentage_error
y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_percentage_error
y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_logarithmic_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_logarithmic_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.poisson(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import poisson
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.poisson(y_true, y_pred)
assert loss.shape == (2,)
y_pred = y_pred + 1e-7
assert np.allclose(
    loss.numpy(), np.mean(y_pred - y_true * np.log(y_pred), axis=-1),
    atol=1e-5)"
"tf.keras.metrics.sparse_categorical_accuracy(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sparse_categorical_accuracy
y_true = [2, 1]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)
assert m.shape == (2,)
m.numpy()
array([0., 1.], dtype=float32)"
"tf.keras.metrics.sparse_categorical_crossentropy(
    y_true, y_pred, from_logits=False, axis=-1, ignore_class=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sparse_categorical_crossentropy
y_true = [1, 2]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.0513, 2.303], dtype=float32)"
"tf.keras.metrics.sparse_top_k_categorical_accuracy(
    y_true, y_pred, k=5
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sparse_top_k_categorical_accuracy
y_true = [2, 1]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.sparse_top_k_categorical_accuracy(
    y_true, y_pred, k=3)
assert m.shape == (2,)
m.numpy()
array([1., 1.], dtype=float32)"
"tf.keras.metrics.squared_hinge(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import squared_hinge
y_true = np.random.choice([-1, 1], size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.squared_hinge(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    np.mean(np.square(np.maximum(1. - y_true * y_pred, 0.)), axis=-1))"
"tf.keras.metrics.top_k_categorical_accuracy(
    y_true, y_pred, k=5
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import top_k_categorical_accuracy
y_true = [[0, 0, 1], [0, 1, 0]]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.top_k_categorical_accuracy(y_true, y_pred, k=3)
assert m.shape == (2,)
m.numpy()
array([1., 1.], dtype=float32)"
"tf.math.minimum(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import minimum
x = tf.constant([0., 0., 0., 0.])
y = tf.constant([-5., -2., 0., 3.])
tf.math.minimum(x, y)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([-5., -2., 0., 0.], dtype=float32)>
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([-5., -2., 0., 0.], dtype=float32)>"
"tf.math.multiply(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import multiply
x = tf.constant(([1, 2, 3, 4]))
tf.math.multiply(x, x)
<tf.Tensor: shape=(4,), dtype=..., numpy=array([ 1,  4,  9, 16], dtype=int32)>
<tf.Tensor: shape=(4,), dtype=..., numpy=array([ 1,  4,  9, 16], dtype=int32)>"
"tf.name_scope(
    name
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import name_scope
def my_op(a, b, c, name=None):
  with tf.name_scope(""MyOp"") as scope:
    a = tf.convert_to_tensor(a, name=""a"")
    b = tf.convert_to_tensor(b, name=""b"")
    c = tf.convert_to_tensor(c, name=""c"")
    return foo_op(..., name=scope)
"
"tf.nest.map_structure(
    func, *structure, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import map_structure
a = {""hello"": 24, ""world"": 76}
tf.nest.map_structure(lambda p: p * 2, a)
{'hello': 48, 'world': 152}"
"tf.math.approx_max_k(
    operand,
    k,
    reduction_dimension=-1,
    recall_target=0.95,
    reduction_input_size_override=-1,
    aggregate_to_topk=True,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import approx_max_k
import tensorflow as tf
@tf.function(jit_compile=True)
def mips(qy, db, k=10, recall_target=0.95):
  dists = tf.einsum('ik,jk->ij', qy, db)
  return tf.nn.approx_max_k(dists, k=k, recall_target=recall_target)
qy = tf.random.uniform((256,128))
db = tf.random.uniform((2048,128))
dot_products, neighbors = mips(qy, db, k=20)"
"tf.math.approx_min_k(
    operand,
    k,
    reduction_dimension=-1,
    recall_target=0.95,
    reduction_input_size_override=-1,
    aggregate_to_topk=True,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import approx_min_k
import tensorflow as tf
@tf.function(jit_compile=True)
def l2_ann(qy, db, half_db_norms, k=10, recall_target=0.95):
  dists = half_db_norms - tf.einsum('ik,jk->ij', qy, db)
  return tf.nn.approx_min_k(dists, k=k, recall_target=recall_target)
qy = tf.random.uniform((256,128))
db = tf.random.uniform((2048,128))
half_db_norms = tf.norm(db, axis=1) / 2
dists, neighbors = l2_ann(qy, db, half_db_norms)"
"tf.nn.compute_average_loss(
    per_example_loss, sample_weight=None, global_batch_size=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import compute_average_loss
with strategy.scope():
  def compute_loss(labels, predictions, sample_weight=None):

    per_example_loss = tf.keras.losses.sparse_categorical_crossentropy(
        labels, predictions)

    return tf.nn.compute_average_loss(
        per_example_loss,
        sample_weight=sample_weight,
        global_batch_size=GLOBAL_BATCH_SIZE)
"
"tf.nn.ctc_greedy_decoder(
    inputs, sequence_length, merge_repeated=True, blank_index=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ctc_greedy_decoder
inf = float(""inf"")
logits = tf.constant([[[   0., -inf, -inf],
                       [ -2.3, -inf, -0.1]],
                      [[ -inf, -0.5, -inf],
                       [ -inf, -inf, -0.1]],
                      [[ -inf, -inf, -inf],
                       [ -0.1, -inf, -2.3]]])
seq_lens = tf.constant([2, 3])
outputs = tf.nn.ctc_greedy_decoder(
    logits,
    seq_lens,
    blank_index=1)"
"tf.nn.dropout(
    x, rate, noise_shape=None, seed=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import dropout
tf.random.set_seed(0)
x = tf.ones([3,5])
tf.nn.dropout(x, rate = 0.5, seed = 1).numpy()
array([[2., 0., 0., 2., 2.],
     [2., 2., 2., 2., 2.],
     [2., 0., 2., 0., 2.]], dtype=float32)"
"tf.nn.elu(
    features, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import elu
tf.nn.elu(1.0)
<tf.Tensor: shape=(), dtype=float32, numpy=1.0>
<tf.Tensor: shape=(), dtype=float32, numpy=1.0>
tf.nn.elu(0.0)
<tf.Tensor: shape=(), dtype=float32, numpy=0.0>
<tf.Tensor: shape=(), dtype=float32, numpy=0.0>
tf.nn.elu(-1000.0)
<tf.Tensor: shape=(), dtype=float32, numpy=-1.0>
<tf.Tensor: shape=(), dtype=float32, numpy=-1.0>"
"tf.nn.experimental.stateless_dropout(
    x, rate, seed, rng_alg=None, noise_shape=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_dropout
x = tf.ones([3,5])
tf.nn.experimental.stateless_dropout(x, rate=0.5, seed=[1, 0])
<tf.Tensor: shape=(3, 5), dtype=float32, numpy=
<tf.Tensor: shape=(3, 5), dtype=float32, numpy=
array([[2., 0., 2., 0., 0.],
       [0., 0., 2., 0., 2.],
       [0., 0., 0., 0., 2.]], dtype=float32)>"
"tf.nn.gelu(
    features, approximate=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import gelu
x = tf.constant([-3.0, -1.0, 0.0, 1.0, 3.0], dtype=tf.float32)
y = tf.nn.gelu(x)
y.numpy()
array([-0.00404951, -0.15865529,  0.        ,  0.8413447 ,  2.9959507 ],
    dtype=float32)
y = tf.nn.gelu(x, approximate=True)
y.numpy()
array([-0.00363752, -0.15880796,  0.        ,  0.841192  ,  2.9963627 ],
    dtype=float32)"
"tf.nn.isotonic_regression(
    inputs, decreasing=True, axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import isotonic_regression
>>> x = tf.constant([[3, 1, 2], [1, 3, 4]], dtype=tf.float32)
>>> y, segments = tf.nn.isotonic_regression(x, axis=1)
<tf.Tensor: shape=(2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 3), dtype=float32, numpy=
array([[3.       , 1.5      , 1.5      ],
       [2.6666667, 2.6666667, 2.6666667]], dtype=float32)>
"
"tf.nn.max_pool(
    input, ksize, strides, padding, data_format=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import max_pool
matrix = tf.constant([
    [0, 0, 1, 7],
    [0, 2, 0, 0],
    [5, 2, 0, 0],
    [0, 0, 9, 8],
])
reshaped = tf.reshape(matrix, (1, 4, 4, 1))
tf.nn.max_pool(reshaped, ksize=2, strides=2, padding=""SAME"")
<tf.Tensor: shape=(1, 2, 2, 1), dtype=int32, numpy=
<tf.Tensor: shape=(1, 2, 2, 1), dtype=int32, numpy=
array([[[[2],
         [7]],
        [[5],
         [9]]]], dtype=int32)>"
"tf.nn.max_pool2d(
    input, ksize, strides, padding, data_format='NHWC', name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import max_pool2d
x = tf.constant([[1., 2., 3., 4.],
                 [5., 6., 7., 8.],
                 [9., 10., 11., 12.]])
x = x[tf.newaxis, :, :, tf.newaxis]
result = tf.nn.max_pool2d(x, ksize=(2, 2), strides=(2, 2),
                          padding=""VALID"")
result[0, :, :, 0]
<tf.Tensor: shape=(1, 2), dtype=float32, numpy=
<tf.Tensor: shape=(1, 2), dtype=float32, numpy=
array([[6., 8.]], dtype=float32)>"
"tf.nn.nce_loss(
    weights,
    biases,
    labels,
    inputs,
    num_sampled,
    num_classes,
    num_true=1,
    sampled_values=None,
    remove_accidental_hits=False,
    name='nce_loss'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import nce_loss
if mode == ""train"":
  loss = tf.nn.nce_loss(
      weights=weights,
      biases=biases,
      labels=labels,
      inputs=inputs,
      ...)
elif mode == ""eval"":
  logits = tf.matmul(inputs, tf.transpose(weights))
  logits = tf.nn.bias_add(logits, biases)
  labels_one_hot = tf.one_hot(labels, n_classes)
  loss = tf.nn.sigmoid_cross_entropy_with_logits(
      labels=labels_one_hot,
      logits=logits)
  loss = tf.reduce_sum(loss, axis=1)
"
"tf.nn.relu(
    features, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import relu
>>> tf.nn.relu([-2., 0., 3.]).numpy()
array([0., 0., 3.], dtype=float32)
"
"tf.nn.relu6(
    features, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import relu6
x = tf.constant([-3.0, -1.0, 0.0, 6.0, 10.0], dtype=tf.float32)
y = tf.nn.relu6(x)
y.numpy()
array([0., 0., 0., 6., 6.], dtype=float32)"
"tf.nn.sampled_softmax_loss(
    weights,
    biases,
    labels,
    inputs,
    num_sampled,
    num_classes,
    num_true=1,
    sampled_values=None,
    remove_accidental_hits=True,
    seed=None,
    name='sampled_softmax_loss'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sampled_softmax_loss
if mode == ""train"":
  loss = tf.nn.sampled_softmax_loss(
      weights=weights,
      biases=biases,
      labels=labels,
      inputs=inputs,
      ...)
elif mode == ""eval"":
  logits = tf.matmul(inputs, tf.transpose(weights))
  logits = tf.nn.bias_add(logits, biases)
  labels_one_hot = tf.one_hot(labels, n_classes)
  loss = tf.nn.softmax_cross_entropy_with_logits(
      labels=labels_one_hot,
      logits=logits)
"
"tf.nn.scale_regularization_loss(
    regularization_loss
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import scale_regularization_loss
with strategy.scope():
  def compute_loss(self, label, predictions):
    per_example_loss = tf.keras.losses.sparse_categorical_crossentropy(
        labels, predictions)

    loss = tf.nn.compute_average_loss(
        per_example_loss,
        sample_weight=sample_weight,
        global_batch_size=GLOBAL_BATCH_SIZE)

    loss += tf.nn.scale_regularization_loss(tf.nn.l2_loss(weights))
    return loss
"
"tf.math.sigmoid(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sigmoid
x = tf.constant([0.0, 1.0, 50.0, 100.0])
tf.math.sigmoid(x)
<tf.Tensor: shape=(4,), dtype=float32,
<tf.Tensor: shape=(4,), dtype=float32,
numpy=array([0.5, 0.7310586, 1.0, 1.0], dtype=float32)>"
"tf.nn.softmax(
    logits, axis=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis, keepdims=True)
"
"tf.nn.softmax_cross_entropy_with_logits(
    labels, logits, axis=-1, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import softmax_cross_entropy_with_logits
logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]
labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]
tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)
<tf.Tensor: shape=(2,), dtype=float32,
<tf.Tensor: shape=(2,), dtype=float32,
numpy=array([0.16984604, 0.82474494], dtype=float32)>"
"tf.math.softplus(
    features, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import softplus
import tensorflow as tf
tf.math.softplus(tf.range(0, 2, dtype=tf.float32)).numpy()
array([0.6931472, 1.3132616], dtype=float32)"
"tf.nn.sparse_softmax_cross_entropy_with_logits(
    labels, logits, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sparse_softmax_cross_entropy_with_logits
logits = tf.constant([[2., -5., .5, -.1],
                      [0., 0., 1.9, 1.4],
                      [-100., 100., -100., -100.]])
labels = tf.constant([0, 3, 1])
tf.nn.sparse_softmax_cross_entropy_with_logits(
    labels=labels, logits=logits).numpy()
array([0.29750752, 1.1448325 , 0.        ], dtype=float32)"
"tf.math.top_k(
    input, k=1, sorted=True, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import top_k
result = tf.math.top_k([1, 2, 98, 1, 1, 99, 3, 1, 3, 96, 4, 1],
                        k=3)
result.values.numpy()
array([99, 98, 96], dtype=int32)
result.indices.numpy()
array([5, 2, 9], dtype=int32)"
"tf.math.zero_fraction(
    value, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import zero_fraction
    z = tf.nn.relu(...)
    summ = tf.compat.v1.summary.scalar('sparsity', tf.nn.zero_fraction(z))
"
"tf.no_gradient(
    op_type
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import no_gradient
tf.no_gradient(""Size"")
"
"tf.math.not_equal(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import not_equal
x = tf.constant([2, 4])
y = tf.constant(2)
tf.math.not_equal(x, y)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>"
"tf.numpy_function(
    func, inp, Tout, stateful=True, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import numpy_function
def my_numpy_func(x):
  return np.sinh(x)
@tf.function(input_signature=[tf.TensorSpec(None, tf.float32)])
def tf_function(input):
  y = tf.numpy_function(my_numpy_func, [input], tf.float32)
  return y * y
tf_function(tf.constant(1.))
<tf.Tensor: shape=(), dtype=float32, numpy=1.3810978>
<tf.Tensor: shape=(), dtype=float32, numpy=1.3810978>"
"tf.ones(
    shape,
    dtype=tf.dtypes.float32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ones
tf.ones([3, 4], tf.int32)
<tf.Tensor: shape=(3, 4), dtype=int32, numpy=
<tf.Tensor: shape=(3, 4), dtype=int32, numpy=
array([[1, 1, 1, 1],
       [1, 1, 1, 1],
       [1, 1, 1, 1]], dtype=int32)>"
"tf.ones_like(
    input, dtype=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ones_like
tensor = tf.constant([[1, 2, 3], [4, 5, 6]])
tf.ones_like(tensor)
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
  array([[1, 1, 1],
         [1, 1, 1]], dtype=int32)>"
"tf.keras.optimizers.experimental.RMSprop(
    learning_rate=0.001,
    rho=0.9,
    momentum=0.0,
    epsilon=1e-07,
    centered=False,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=100,
    jit_compile=True,
    name='RMSprop',
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RMSprop
opt = tf.keras.optimizers.experimental.RMSprop(learning_rate=0.1)
var1 = tf.Variable(10.0)
opt.minimize(loss, [var1])
var1.numpy()
9.683772"
"tf.keras.optimizers.experimental.RMSprop(
    learning_rate=0.001,
    rho=0.9,
    momentum=0.0,
    epsilon=1e-07,
    centered=False,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=100,
    jit_compile=True,
    name='RMSprop',
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RMSprop
opt = tf.keras.optimizers.experimental.RMSprop(learning_rate=0.1)
var1 = tf.Variable(10.0)
opt.minimize(loss, [var1])
var1.numpy()
9.683772"
"tf.keras.optimizers.legacy.Adam(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    amsgrad=False,
    name='Adam',
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Adam
opt = tf.keras.optimizers.legacy.Adam(learning_rate=0.1)
var1 = tf.Variable(10.0)
step_count = opt.minimize(loss, [var1]).numpy()
var1.numpy()
9.9"
"tf.keras.optimizers.legacy.Nadam(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    name='Nadam',
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Nadam
>>> opt = tf.keras.optimizers.legacy.Nadam(learning_rate=0.2)
>>> var1 = tf.Variable(10.0)
>>> loss = lambda: (var1 ** 2) / 2.0
>>> step_count = opt.minimize(loss, [var1]).numpy()
>>> ""{:.1f}"".format(var1.numpy())
9.8
"
"tf.keras.optimizers.legacy.RMSprop(
    learning_rate=0.001,
    rho=0.9,
    momentum=0.0,
    epsilon=1e-07,
    centered=False,
    name='RMSprop',
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RMSprop
opt = tf.keras.optimizers.legacy.RMSprop(learning_rate=0.1)
var1 = tf.Variable(10.0)
step_count = opt.minimize(loss, [var1]).numpy()
var1.numpy()
9.683772"
"tf.keras.optimizers.schedules.CosineDecayRestarts(
    initial_learning_rate,
    first_decay_steps,
    t_mul=2.0,
    m_mul=1.0,
    alpha=0.0,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CosineDecayRestarts
first_decay_steps = 1000
lr_decayed_fn = (
  tf.keras.optimizers.schedules.CosineDecayRestarts(
      initial_learning_rate,
      first_decay_steps))
"
"tf.keras.optimizers.schedules.PiecewiseConstantDecay(
    boundaries, values, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import PiecewiseConstantDecay
step = tf.Variable(0, trainable=False)
boundaries = [100000, 110000]
values = [1.0, 0.5, 0.1]
learning_rate_fn = keras.optimizers.schedules.PiecewiseConstantDecay(
    boundaries, values)

learning_rate = learning_rate_fn(step)
"
"tf.keras.optimizers.schedules.deserialize(
    config, custom_objects=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import deserialize
config = {
  'class_name': 'PolynomialDecay',
  'config': {'cycle': False,
    'decay_steps': 10000,
    'end_learning_rate': 0.01,
    'initial_learning_rate': 0.1,
    'name': None,
    'power': 0.5} }
lr_schedule = tf.keras.optimizers.schedules.deserialize(config)
"
"tf.keras.optimizers.schedules.serialize(
    learning_rate_schedule
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import serialize
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
  0.1, decay_steps=100000, decay_rate=0.96, staircase=True)
tf.keras.optimizers.schedules.serialize(lr_schedule)
{'class_name': 'ExponentialDecay', 'config': {...} }"
"tf.keras.optimizers.serialize(
    optimizer
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import serialize
tf.keras.optimizers.serialize(tf.keras.optimizers.legacy.SGD())
{'class_name': 'SGD', 'config': {'name': 'SGD', 'learning_rate': 0.01,
                                 'decay': 0.0, 'momentum': 0.0,
                                 'nesterov': False} }"
"tf.pad(
    tensor, paddings, mode='CONSTANT', constant_values=0, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import pad
t = tf.constant([[1, 2, 3], [4, 5, 6]])
paddings = tf.constant([[1, 1,], [2, 2]])


"
"tf.parallel_stack(
    values, name='parallel_stack'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import parallel_stack
x = tf.constant([1, 4])
y = tf.constant([2, 5])
z = tf.constant([3, 6])
"
"tf.math.pow(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import pow
x = tf.constant([[2, 2], [3, 3]])
y = tf.constant([[8, 16], [2, 3]])
"
"tf.print(
    *inputs, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import print
tensor = tf.range(10)
tf.print(tensor, output_stream=sys.stderr)
"
"tf.profiler.experimental.Profile(
    logdir, options=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Profile
with tf.profiler.experimental.Profile(""/path/to/logdir""):
"
"tf.profiler.experimental.Trace(
    name, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Trace
tf.profiler.experimental.start('logdir')
for step in range(num_steps):
  with tf.profiler.experimental.Trace(""Train"", step_num=step, _r=1):
    train_fn()
tf.profiler.experimental.stop()
"
"tf.profiler.experimental.client.monitor(
    service_addr, duration_ms, level=1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import monitor

  for query in range(0, 100):
    print(
      tf.profiler.experimental.client.monitor('grpc://10.0.0.2:8466', 1000))
"
"tf.profiler.experimental.client.trace(
    service_addr,
    logdir,
    duration_ms,
    worker_list='',
    num_tracing_attempts=3,
    options=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import trace
  tf.profiler.experimental.server.start(6009)
  tf.profiler.experimental.client.trace('grpc://localhost:6009',
                                        '/nfs/tb_log', 2000)
"
"tf.profiler.experimental.start(
    logdir, options=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import start
options = tf.profiler.experimental.ProfilerOptions(host_tracer_level = 3,
                                                   python_tracer_level = 1,
                                                   device_tracer_level = 1)
tf.profiler.experimental.start('logdir_path', options = options)
tf.profiler.experimental.stop()
"
"tf.py_function(
    func, inp, Tout, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import py_function
def log_huber(x, m):
  if tf.abs(x) <= m:
  if tf.abs(x) <= m:
    return x**2
  else:
    return m**2 * (1 - 2 * tf.math.log(m) + tf.math.log(x**2))

x = tf.constant(1.0)
m = tf.constant(2.0)

with tf.GradientTape() as t:
  t.watch([x, m])
  y = tf.py_function(func=log_huber, inp=[x, m], Tout=tf.float32)

dy_dx = t.gradient(y, x)
assert dy_dx.numpy() == 2.0
"
"tf.quantization.quantize_and_dequantize_v2(
    input,
    input_min,
    input_max,
    signed_input=True,
    num_bits=8,
    range_given=False,
    round_mode='HALF_TO_EVEN',
    name=None,
    narrow_range=False,
    axis=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import quantize_and_dequantize_v2
def getQuantizeOp(input):
    input_tensor = tf.placeholder(tf.float32, shape=[4, 4])
    net = tf.quantization.quantize_and_dequantize(input,
                                                  input_min=min_threshold,
                                                  input_max=max_threshold,
                                                  range_given=True)

To simulate v1 behavior:

def testDecomposeQuantizeDequantize(self):
    def f(input_tensor):
      return tf.quantization.quantize_and_dequantize_v2(input_tensor,
                                                        input_min = 5.0,
                                                        input_max= -10.0,
                                                        range_given=True)
    input_tensor = tf.placeholder(tf.float32, shape=[4, 4])
    net = tf.grad_pass_through(f)(input_tensor)
"
"tf.ragged.constant(
    pylist,
    dtype=None,
    ragged_rank=None,
    inner_shape=None,
    name=None,
    row_splits_dtype=tf.dtypes.int64
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import constant
tf.ragged.constant([[1, 2], [3], [4, 5, 6]])
<tf.RaggedTensor [[1, 2], [3], [4, 5, 6]]>
<tf.RaggedTensor [[1, 2], [3], [4, 5, 6]]>"
"tf.ragged.cross(
    inputs, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import cross
tf.ragged.cross([tf.ragged.constant([['a'], ['b', 'c']]),
                 tf.ragged.constant([['d'], ['e']]),
                 tf.ragged.constant([['f'], ['g']])])
<tf.RaggedTensor [[b'a_X_d_X_f'], [b'b_X_e_X_g', b'c_X_e_X_g']]>
<tf.RaggedTensor [[b'a_X_d_X_f'], [b'b_X_e_X_g', b'c_X_e_X_g']]>"
"tf.ragged.cross_hashed(
    inputs, num_buckets=0, hash_key=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import cross_hashed
tf.ragged.cross_hashed([tf.ragged.constant([['a'], ['b', 'c']]),
                        tf.ragged.constant([['d'], ['e']]),
                        tf.ragged.constant([['f'], ['g']])],
                       num_buckets=100)
<tf.RaggedTensor [[78], [66, 74]]>
<tf.RaggedTensor [[78], [66, 74]]>"
"tf.ragged.map_flat_values(
    op, *args, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import map_flat_values
rt = tf.ragged.constant([[1, 2, 3], [], [4, 5], [6]])
tf.ragged.map_flat_values(tf.ones_like, rt)
<tf.RaggedTensor [[1, 1, 1], [], [1, 1], [1]]>
<tf.RaggedTensor [[1, 1, 1], [], [1, 1], [1]]>
tf.ragged.map_flat_values(tf.multiply, rt, rt)
<tf.RaggedTensor [[1, 4, 9], [], [16, 25], [36]]>
<tf.RaggedTensor [[1, 4, 9], [], [16, 25], [36]]>
tf.ragged.map_flat_values(tf.add, rt, 5)
<tf.RaggedTensor [[6, 7, 8], [], [9, 10], [11]]>
<tf.RaggedTensor [[6, 7, 8], [], [9, 10], [11]]>"
"tf.ragged.range(
    starts,
    limits=None,
    deltas=1,
    dtype=None,
    name=None,
    row_splits_dtype=tf.dtypes.int64
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import range
ragged.range(starts, limits, deltas)[i] ==
    tf.range(starts[i], limits[i], deltas[i])
"
"tf.ragged.row_splits_to_segment_ids(
    splits, name=None, out_type=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import row_splits_to_segment_ids
print(tf.ragged.row_splits_to_segment_ids([0, 3, 3, 5, 6, 9]))
 tf.Tensor([0 0 0 2 2 3 4 4 4], shape=(9,), dtype=int64)"
"tf.ragged.segment_ids_to_row_splits(
    segment_ids, num_segments=None, out_type=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import segment_ids_to_row_splits
print(tf.ragged.segment_ids_to_row_splits([0, 0, 0, 2, 2, 3, 4, 4, 4]))
tf.Tensor([0 3 3 5 6 9], shape=(6,), dtype=int64)"
"tf.ragged.stack(
    values: typing.List[ragged_tensor.RaggedOrDense], axis=0, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stack
t1 = tf.ragged.constant([[1, 2], [3, 4, 5]])
t2 = tf.ragged.constant([[6], [7, 8, 9]])
tf.ragged.stack([t1, t2], axis=0)
<tf.RaggedTensor [[[1, 2], [3, 4, 5]], [[6], [7, 8, 9]]]>
<tf.RaggedTensor [[[1, 2], [3, 4, 5]], [[6], [7, 8, 9]]]>
tf.ragged.stack([t1, t2], axis=1)
<tf.RaggedTensor [[[1, 2], [6]], [[3, 4, 5], [7, 8, 9]]]>
<tf.RaggedTensor [[[1, 2], [6]], [[3, 4, 5], [7, 8, 9]]]>"
"tf.ragged.stack_dynamic_partitions(
    data, partitions, num_partitions, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stack_dynamic_partitions
data           = ['a', 'b', 'c', 'd', 'e']
partitions     = [  3,   0,   2,   2,   3]
num_partitions = 5
tf.ragged.stack_dynamic_partitions(data, partitions, num_partitions)
<tf.RaggedTensor [[b'b'], [], [b'c', b'd'], [b'a', b'e'], []]>
<tf.RaggedTensor [[b'b'], [], [b'c', b'd'], [b'a', b'e'], []]>"
"tf.random.Generator(
    copy_from=None, state=None, alg=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
g = tf.random.Generator.from_seed(1234)
g.normal(shape=(2, 3))
<tf.Tensor: shape=(2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 3), dtype=float32, numpy=
array([[ 0.9356609 ,  1.0854305 , -0.93788373],
       [-0.5061547 ,  1.3169702 ,  0.7137579 ]], dtype=float32)>"
"tf.random.categorical(
    logits, num_samples, dtype=None, seed=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import categorical
samples = tf.random.categorical(tf.math.log([[0.5, 0.5]]), 5)
"
"tf.random.create_rng_state(
    seed, alg
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import create_rng_state
tf.random.create_rng_state(
    1234, ""philox"")
<tf.Tensor: shape=(3,), dtype=int64, numpy=array([1234,    0,    0])>
<tf.Tensor: shape=(3,), dtype=int64, numpy=array([1234,    0,    0])>
tf.random.create_rng_state(
    [12, 34], ""threefry"")
<tf.Tensor: shape=(2,), dtype=int64, numpy=array([12, 34])>
<tf.Tensor: shape=(2,), dtype=int64, numpy=array([12, 34])>"
"tf.random.Generator(
    copy_from=None, state=None, alg=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
g = tf.random.Generator.from_seed(1234)
g.normal(shape=(2, 3))
<tf.Tensor: shape=(2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 3), dtype=float32, numpy=
array([[ 0.9356609 ,  1.0854305 , -0.93788373],
       [-0.5061547 ,  1.3169702 ,  0.7137579 ]], dtype=float32)>"
"tf.random.create_rng_state(
    seed, alg
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import create_rng_state
tf.random.create_rng_state(
    1234, ""philox"")
<tf.Tensor: shape=(3,), dtype=int64, numpy=array([1234,    0,    0])>
<tf.Tensor: shape=(3,), dtype=int64, numpy=array([1234,    0,    0])>
tf.random.create_rng_state(
    [12, 34], ""threefry"")
<tf.Tensor: shape=(2,), dtype=int64, numpy=array([12, 34])>
<tf.Tensor: shape=(2,), dtype=int64, numpy=array([12, 34])>"
"tf.random.experimental.index_shuffle(
    index, seed, max_index
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import index_shuffle
vector = tf.constant(['e0', 'e1', 'e2', 'e3'])
indices = tf.random.experimental.index_shuffle(tf.range(4), [5, 9], 3)
shuffled_vector = tf.gather(vector, indices)
print(shuffled_vector)
tf.Tensor([b'e2' b'e0' b'e1' b'e3'], shape=(4,), dtype=string)"
"tf.random.experimental.stateless_fold_in(
    seed, data, alg='auto_select'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_fold_in
master_seed = [1, 2]
replica_id = 3
replica_seed = tf.random.experimental.stateless_fold_in(
  master_seed, replica_id)
print(replica_seed)
tf.Tensor([1105988140          3], shape=(2,), dtype=int32)
tf.random.stateless_normal(shape=[3], seed=replica_seed)
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.03197195, 0.8979765 ,
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.03197195, 0.8979765 ,
0.13253039], dtype=float32)>"
"tf.random.experimental.stateless_split(
    seed, num=2, alg='auto_select'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_split
seed = [1, 2]
new_seeds = tf.random.experimental.stateless_split(seed, num=3)
print(new_seeds)
tf.Tensor(
[[1105988140 1738052849]
 [-335576002  370444179]
 [  10670227 -246211131]], shape=(3, 2), dtype=int32)
tf.random.stateless_normal(shape=[3], seed=new_seeds[0, :])
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([-0.59835213, -0.9578608 ,
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([-0.59835213, -0.9578608 ,
0.9002807 ], dtype=float32)>"
"tf.random.gamma(
    shape,
    alpha,
    beta=None,
    dtype=tf.dtypes.float32,
    seed=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import gamma
samples = tf.random.gamma([10], [0.5, 1.5])

samples = tf.random.gamma([7, 5], [0.5, 1.5])

alpha = tf.constant([[1.],[3.],[5.]])
beta = tf.constant([[3., 4.]])
samples = tf.random.gamma([30], alpha=alpha, beta=beta)

loss = tf.reduce_mean(tf.square(samples))
dloss_dalpha, dloss_dbeta = tf.gradients(loss, [alpha, beta])
"
"tf.random.normal(
    shape,
    mean=0.0,
    stddev=1.0,
    dtype=tf.dtypes.float32,
    seed=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import normal
tf.random.set_seed(5);
tf.random.normal([4], 0, 1, tf.float32)
<tf.Tensor: shape=(4,), dtype=float32, numpy=..., dtype=float32)>
<tf.Tensor: shape=(4,), dtype=float32, numpy=..., dtype=float32)>"
"tf.random.poisson(
    shape,
    lam,
    dtype=tf.dtypes.float32,
    seed=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import poisson
samples = tf.random.poisson([10], [0.5, 1.5])

samples = tf.random.poisson([7, 5], [12.2, 3.3])
"
"tf.random.stateless_binomial(
    shape,
    seed,
    counts,
    probs,
    output_dtype=tf.dtypes.int32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_binomial
counts = [10., 20.]
probs = [0.8]

binomial_samples = tf.random.stateless_binomial(
    shape=[2], seed=[123, 456], counts=counts, probs=probs)

shape = [3, 4, 3, 4, 2]
binomial_samples = tf.random.stateless_binomial(
    shape=shape, seed=[123, 456], counts=counts, probs=probs)
"
"tf.random.stateless_categorical(
    logits,
    num_samples,
    seed,
    dtype=tf.dtypes.int64,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_categorical
samples = tf.random.stateless_categorical(
    tf.math.log([[0.5, 0.5]]), 5, seed=[7, 17])
"
"tf.random.stateless_gamma(
    shape,
    seed,
    alpha,
    beta=None,
    dtype=tf.dtypes.float32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_gamma
samples = tf.random.stateless_gamma([10, 2], seed=[12, 34], alpha=[0.5, 1.5])

samples = tf.random.stateless_gamma([7, 5, 2], seed=[12, 34], alpha=[.5, 1.5])

alpha = tf.constant([[1.], [3.], [5.]])
beta = tf.constant([[3., 4.]])
samples = tf.random.stateless_gamma(
    [30, 3, 2], seed=[12, 34], alpha=alpha, beta=beta)

with tf.GradientTape() as tape:
  tape.watch([alpha, beta])
  loss = tf.reduce_mean(tf.square(tf.random.stateless_gamma(
      [30, 3, 2], seed=[12, 34], alpha=alpha, beta=beta)))
dloss_dalpha, dloss_dbeta = tape.gradient(loss, [alpha, beta])
"
"tf.random.stateless_parameterized_truncated_normal(
    shape, seed, means=0.0, stddevs=1.0, minvals=-2.0, maxvals=2.0, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_parameterized_truncated_normal
means = 0.
stddevs = tf.math.exp(tf.random.uniform(shape=[2, 3]))
minvals = [-1., -2., -1000.]
maxvals = [[10000.], [1.]]
y = tf.random.stateless_parameterized_truncated_normal(
  shape=[10, 2, 3], seed=[7, 17],
  means=means, stddevs=stddevs, minvals=minvals, maxvals=maxvals)
y.shape
TensorShape([10, 2, 3])"
"tf.random.stateless_poisson(
    shape,
    seed,
    lam,
    dtype=tf.dtypes.int32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_poisson
samples = tf.random.stateless_poisson([10, 2], seed=[12, 34], lam=[5, 15])

samples = tf.random.stateless_poisson([7, 5, 2], seed=[12, 34], lam=[5, 15])

rate = tf.constant([[1.], [3.], [5.]])
samples = tf.random.stateless_poisson([30, 3, 1], seed=[12, 34], lam=rate)
"
"tf.random.stateless_uniform(
    shape,
    seed,
    minval=0,
    maxval=None,
    dtype=tf.dtypes.float32,
    name=None,
    alg='auto_select'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_uniform
ints = tf.random.stateless_uniform(
    [10], seed=(2, 3), minval=None, maxval=None, dtype=tf.int32)
"
"tf.random.truncated_normal(
    shape,
    mean=0.0,
    stddev=1.0,
    dtype=tf.dtypes.float32,
    seed=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import truncated_normal
tf.random.truncated_normal(shape=[2])
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([..., ...], dtype=float32)>
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([..., ...], dtype=float32)>"
"tf.random.uniform(
    shape,
    minval=0,
    maxval=None,
    dtype=tf.dtypes.float32,
    seed=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import uniform
tf.random.uniform(shape=[2])
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([..., ...], dtype=float32)>
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([..., ...], dtype=float32)>
tf.random.uniform(shape=[], minval=-1., maxval=0.)
<tf.Tensor: shape=(), dtype=float32, numpy=-...>
<tf.Tensor: shape=(), dtype=float32, numpy=-...>
tf.random.uniform(shape=[], minval=5, maxval=10, dtype=tf.int64)
<tf.Tensor: shape=(), dtype=int64, numpy=...>
<tf.Tensor: shape=(), dtype=int64, numpy=...>"
"tf.random_normal_initializer(
    mean=0.0, stddev=0.05, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import random_normal_initializer
def make_variables(k, initializer):
  return (tf.Variable(initializer(shape=[k], dtype=tf.float32)),
          tf.Variable(initializer(shape=[k, k], dtype=tf.float32)))
v1, v2 = make_variables(3,
                        tf.random_normal_initializer(mean=1., stddev=2.))
v1
<tf.Variable ... shape=(3,) ... numpy=array([...], dtype=float32)>
<tf.Variable ... shape=(3,) ... numpy=array([...], dtype=float32)>
v2
<tf.Variable ... shape=(3, 3) ... numpy=
<tf.Variable ... shape=(3, 3) ... numpy=
make_variables(4, tf.random_uniform_initializer(minval=-1., maxval=1.))
(<tf.Variable...shape=(4,) dtype=float32...>, <tf.Variable...shape=(4, 4) ...
(<tf.Variable...shape=(4,) dtype=float32...>, <tf.Variable...shape=(4, 4) ..."
"tf.random_uniform_initializer(
    minval=-0.05, maxval=0.05, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import random_uniform_initializer
def make_variables(k, initializer):
  return (tf.Variable(initializer(shape=[k], dtype=tf.float32)),
          tf.Variable(initializer(shape=[k, k], dtype=tf.float32)))
v1, v2 = make_variables(3, tf.ones_initializer())
v1
<tf.Variable ... shape=(3,) ... numpy=array([1., 1., 1.], dtype=float32)>
<tf.Variable ... shape=(3,) ... numpy=array([1., 1., 1.], dtype=float32)>
v2
<tf.Variable ... shape=(3, 3) ... numpy=
<tf.Variable ... shape=(3, 3) ... numpy=
array([[1., 1., 1.],
       [1., 1., 1.],
       [1., 1., 1.]], dtype=float32)>
make_variables(4, tf.random_uniform_initializer(minval=-1., maxval=1.))
(<tf.Variable...shape=(4,) dtype=float32...>, <tf.Variable...shape=(4, 4) ...
(<tf.Variable...shape=(4,) dtype=float32...>, <tf.Variable...shape=(4, 4) ..."
"tf.rank(
    input, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import rank
t = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])
"
"tf.raw_ops.Bitcast(
    input, type, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
a = [1., 2., 3.]
equality_bitcast = tf.bitcast(a, tf.complex128)
Traceback (most recent call last):
InvalidArgumentError: Cannot bitcast from 1 to 18 [Op:Bitcast]
equality_cast = tf.cast(a, tf.complex128)
print(equality_cast)
tf.Tensor([1.+0.j 2.+0.j 3.+0.j], shape=(3,), dtype=complex128)"
"tf.raw_ops.ComplexAbs(
    x,
    Tout=tf.dtypes.float32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ComplexAbs
x = tf.complex(3.0, 4.0)
print((tf.raw_ops.ComplexAbs(x=x, Tout=tf.dtypes.float32, name=None)).numpy())
5.0"
"tf.raw_ops.Fingerprint(
    data, method, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Fingerprint
Fingerprint(data) == Fingerprint(Reshape(data, ...))
Fingerprint(data) == Fingerprint(Bitcast(data, ...))
"
"tf.raw_ops.IdentityN(
    input, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
with tf.get_default_graph().gradient_override_map(
    {'IdentityN': 'OverrideGradientWithG'}):
  y, _ = identity_n([f(x), x])

@tf.RegisterGradient('OverrideGradientWithG')
def ApplyG(op, dy, _):
"
"tf.raw_ops.Pack(
    values, axis=0, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
pack([x, y, z], axis=1) => [[1, 2, 3], [4, 5, 6]]
"
"tf.raw_ops.TPUReplicatedInput(
    inputs, is_mirrored_variable=False, index=-1, is_packed=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
%a = ""tf.opA""()
%b = ""tf.opB""()
%replicated_input = ""tf.TPUReplicatedInput""(%a, %b)
%computation = ""tf.Computation""(%replicated_input)
"
"tf.raw_ops.TPUReplicatedOutput(
    input, num_replicas, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
%computation = ""tf.Computation""()
%replicated_output:2 = ""tf.TPUReplicatedOutput""(%computation)
"
"tf.raw_ops.UniqueWithCountsV2(
    x,
    axis,
    out_idx=tf.dtypes.int32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import UniqueWithCountsV2
x = tf.constant([1, 1, 2, 4, 4, 4, 7, 8, 8])
y, idx, count = UniqueWithCountsV2(x, axis = [0])
y ==> [1, 2, 4, 7, 8]
idx ==> [0, 0, 1, 2, 2, 2, 3, 4, 4]
count ==> [2, 1, 3, 1, 2]
"
"tf.math.reduce_all(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_all
>>> x = tf.constant([[True,  True], [False, False]])
>>> tf.math.reduce_all(x)
<tf.Tensor: shape=(), dtype=bool, numpy=False>
<tf.Tensor: shape=(), dtype=bool, numpy=False>
>>> tf.math.reduce_all(x, 0)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False, False])>
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False, False])>
>>> tf.math.reduce_all(x, 1)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>
"
"tf.math.reduce_any(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_any
>>> x = tf.constant([[True,  True], [False, False]])
>>> tf.reduce_any(x)
<tf.Tensor: shape=(), dtype=bool, numpy=True>
<tf.Tensor: shape=(), dtype=bool, numpy=True>
>>> tf.reduce_any(x, 0)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  True])>
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  True])>
>>> tf.reduce_any(x, 1)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>
"
"tf.math.reduce_logsumexp(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_logsumexp
x = tf.constant([[0., 0., 0.], [0., 0., 0.]])
"
"tf.math.reduce_max(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_max
>>> x = tf.constant([5, 1, 2, 4])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=int32, numpy=5>
<tf.Tensor: shape=(), dtype=int32, numpy=5>
>>> x = tf.constant([-5, -1, -2, -4])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=int32, numpy=-1>
<tf.Tensor: shape=(), dtype=int32, numpy=-1>
>>> x = tf.constant([4, float('nan')])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=float32, numpy=nan>
<tf.Tensor: shape=(), dtype=float32, numpy=nan>
>>> x = tf.constant([float('nan'), float('nan')])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=float32, numpy=nan>
<tf.Tensor: shape=(), dtype=float32, numpy=nan>
>>> x = tf.constant([float('-inf'), float('inf')])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=float32, numpy=inf>
<tf.Tensor: shape=(), dtype=float32, numpy=inf>
"
"tf.math.reduce_mean(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_mean
x = tf.constant([[1., 1.], [2., 2.]])
tf.reduce_mean(x)
<tf.Tensor: shape=(), dtype=float32, numpy=1.5>
<tf.Tensor: shape=(), dtype=float32, numpy=1.5>
tf.reduce_mean(x, 0)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.5, 1.5], dtype=float32)>
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.5, 1.5], dtype=float32)>
tf.reduce_mean(x, 1)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 2.], dtype=float32)>
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 2.], dtype=float32)>"
"tf.math.reduce_min(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_min
a = tf.constant([
  [[1, 2], [3, 4]],
  [[1, 2], [3, 4]]
])
tf.reduce_min(a)
<tf.Tensor: shape=(), dtype=int32, numpy=1>
<tf.Tensor: shape=(), dtype=int32, numpy=1>"
"tf.math.reduce_prod(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_prod
>>> x = tf.constant([[1., 2.], [3., 4.]])
>>> tf.math.reduce_prod(x)
<tf.Tensor: shape=(), dtype=float32, numpy=24.>
<tf.Tensor: shape=(), dtype=float32, numpy=24.>
>>> tf.math.reduce_prod(x, 0)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([3., 8.], dtype=float32)>
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([3., 8.], dtype=float32)>
>>> tf.math.reduce_prod(x, 1)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 12.],
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 12.],
dtype=float32)>
"
"tf.math.reduce_sum(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_sum
>>> x = tf.constant([[1, 1, 1], [1, 1, 1]])
>>> x.numpy()
array([[1, 1, 1],
       [1, 1, 1]], dtype=int32)
>>> tf.reduce_sum(x).numpy()
6
>>> tf.reduce_sum(x, 0).numpy()
array([2, 2, 2], dtype=int32)
>>> tf.reduce_sum(x, 1).numpy()
array([3, 3], dtype=int32)
>>> tf.reduce_sum(x, 1, keepdims=True).numpy()
array([[3],
       [3]], dtype=int32)
>>> tf.reduce_sum(x, [0, 1]).numpy()
6
"
"tf.repeat(
    input, repeats, axis=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import repeat
repeat(['a', 'b', 'c'], repeats=[3, 0, 2], axis=0)
<tf.Tensor: shape=(5,), dtype=string,
<tf.Tensor: shape=(5,), dtype=string,
numpy=array([b'a', b'a', b'a', b'c', b'c'], dtype=object)>"
"tf.reshape(
    tensor, shape, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reshape
t1 = [[1, 2, 3],
      [4, 5, 6]]
print(tf.shape(t1).numpy())
[2 3]
t2 = tf.reshape(t1, [6])
t2
<tf.Tensor: shape=(6,), dtype=int32,
<tf.Tensor: shape=(6,), dtype=int32,
  numpy=array([1, 2, 3, 4, 5, 6], dtype=int32)>
tf.reshape(t2, [3, 2])
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
  array([[1, 2],
         [3, 4],
         [5, 6]], dtype=int32)>"
"tf.reverse(
    tensor, axis, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reverse

reverse(t, dims) ==> [[[[ 3,  2,  1,  0],
                        [ 7,  6,  5,  4],
                        [ 11, 10, 9, 8]],
                       [[15, 14, 13, 12],
                        [19, 18, 17, 16],
                        [23, 22, 21, 20]]]]

reverse(t, dims) ==> [[[[12, 13, 14, 15],
                        [16, 17, 18, 19],
                        [20, 21, 22, 23]
                       [[ 0,  1,  2,  3],
                        [ 4,  5,  6,  7],
                        [ 8,  9, 10, 11]]]]

reverse(t, dims) ==> [[[[8, 9, 10, 11],
                        [4, 5, 6, 7],
                        [0, 1, 2, 3]]
                       [[20, 21, 22, 23],
                        [16, 17, 18, 19],
                        [12, 13, 14, 15]]]]
"
"tf.reverse_sequence(
    input, seq_lengths, seq_axis=None, batch_axis=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reverse_sequence
seq_lengths = [7, 2, 3, 5]
input = [[1, 2, 3, 4, 5, 0, 0, 0], [1, 2, 0, 0, 0, 0, 0, 0],
         [1, 2, 3, 4, 0, 0, 0, 0], [1, 2, 3, 4, 5, 6, 7, 8]]
output = tf.reverse_sequence(input, seq_lengths, seq_axis=1, batch_axis=0)
output
<tf.Tensor: shape=(4, 8), dtype=int32, numpy=
<tf.Tensor: shape=(4, 8), dtype=int32, numpy=
array([[0, 0, 5, 4, 3, 2, 1, 0],
       [2, 1, 0, 0, 0, 0, 0, 0],
       [3, 2, 1, 4, 0, 0, 0, 0],
       [5, 4, 3, 2, 1, 6, 7, 8]], dtype=int32)>"
"tf.roll(
    input, shift, axis, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import roll
roll(t, shift=2, axis=0) ==> [3, 4, 0, 1, 2]

roll(t, shift=[1, -2], axis=[0, 1]) ==> [[7, 8, 9, 5, 6], [2, 3, 4, 0, 1]]

roll(t, shift=[2, -3], axis=[1, 1]) ==> [[1, 2, 3, 4, 0], [6, 7, 8, 9, 5]]
"
"tf.math.round(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import round
x = tf.constant([0.9, 2.5, 2.3, 1.5, -4.5])
"
"tf.saved_model.Asset(
    path
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Asset
filename = tf.saved_model.Asset(""file.txt"")

@tf.function(input_signature=[])
def func():
  return tf.io.read_file(filename)

trackable_obj = tf.train.Checkpoint()
trackable_obj.func = func
trackable_obj.filename = filename
tf.saved_model.save(trackable_obj, ""/tmp/saved_model"")

tf.io.gfile.remove(""file.txt"")
tf.io.gfile.rename(""/tmp/saved_model"", ""/tmp/new_location"")

reloaded_obj = tf.saved_model.load(""/tmp/new_location"")
print(reloaded_obj.func())
"
"tf.saved_model.experimental.TrackableResource(
    device=''
)
","import pandas as pd
import numpy as np
import tensorflow as tf
class DemoResource(tf.saved_model.experimental.TrackableResource):
  def __init__(self):
    super().__init__()
    self._initialize()
  def _create_resource(self):
    return tf.raw_ops.VarHandleOp(dtype=tf.float32, shape=[2])
  def _initialize(self):
    tf.raw_ops.AssignVariableOp(
        resource=self.resource_handle, value=tf.ones([2]))
  def _destroy_resource(self):
    tf.raw_ops.DestroyResourceOp(resource=self.resource_handle)
class DemoModule(tf.Module):
  def __init__(self):
    self.resource = DemoResource()
  def increment(self, tensor):
    return tensor + tf.raw_ops.ReadVariableOp(
        resource=self.resource.resource_handle, dtype=tf.float32)
demo = DemoModule()
demo.increment([5, 1])
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([6., 2.], dtype=float32)>
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([6., 2.], dtype=float32)>"
"tf.saved_model.load(
    export_dir, tags=None, options=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import load
imported = tf.saved_model.load(path)
f = imported.signatures[""serving_default""]
print(f(x=tf.constant([[1.]])))
"
"tf.math.scalar_mul(
    scalar, x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import scalar_mul
x = tf.reshape(tf.range(30, dtype=tf.float32), [10, 3])
with tf.GradientTape() as g:
  g.watch(x)
  z = tf.math.scalar_mul(10.0, y)"
"tf.scan(
    fn,
    elems,
    initializer=None,
    parallel_iterations=10,
    back_prop=True,
    swap_memory=False,
    infer_shape=True,
    reverse=False,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import scan
elems = np.array([1, 2, 3, 4, 5, 6])
sum = scan(lambda a, x: a + x, elems)
sum = scan(lambda a, x: a + x, elems, reverse=True)
"
"tf.searchsorted(
    sorted_sequence,
    values,
    side='left',
    out_type=tf.dtypes.int32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import searchsorted
edges = [-1, 3.3, 9.1, 10.0]
values = [0.0, 4.1, 12.0]
tf.searchsorted(edges, values).numpy()
array([1, 2, 4], dtype=int32)"
"tf.sets.difference(
    a, b, aminusb=True, validate_indices=True
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import difference
  import tensorflow as tf
  import collections

  a = collections.OrderedDict([
      ((0, 0, 0), 1),
      ((0, 0, 1), 2),
      ((0, 1, 0), 3),
      ((1, 0, 0), 4),
      ((1, 1, 0), 5),
      ((1, 1, 1), 6),
  ])
  a = tf.sparse.SparseTensor(list(a.keys()), list(a.values()),
                             dense_shape=[2, 2, 2])

  b = collections.OrderedDict([
      ((0, 0, 0), 1),
      ((0, 0, 1), 3),
      ((0, 1, 0), 2),
      ((1, 0, 0), 4),
      ((1, 0, 1), 5),
      ((1, 1, 0), 5),
      ((1, 1, 1), 6),
      ((1, 1, 2), 7),
      ((1, 1, 3), 8),
  ])
  b = tf.sparse.SparseTensor(list(b.keys()), list(b.values()),
                             dense_shape=[2, 2, 4])

  tf.sets.difference(a, b)

  #
  #
"
"tf.sets.intersection(
    a, b, validate_indices=True
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import intersection
  import tensorflow as tf
  import collections

  a = collections.OrderedDict([
      ((0, 0, 0), 1),
      ((0, 0, 1), 2),
      ((0, 1, 0), 3),
      ((1, 0, 0), 4),
      ((1, 1, 0), 5),
      ((1, 1, 1), 6),
  ])
  a = tf.sparse.SparseTensor(list(a.keys()), list(a.values()),
                             dense_shape=[2,2,2])

  b = collections.OrderedDict([
      ((0, 0, 0), 1),
      ((1, 0, 0), 4),
      ((1, 1, 0), 5),
      ((1, 1, 1), 6),
      ((1, 1, 2), 7),
      ((1, 1, 3), 8),
  ])
  b = tf.sparse.SparseTensor(list(b.keys()), list(b.values()),
                             dense_shape=[2, 2, 4])

  tf.sets.intersection(a, b)

  #
  #
"
"tf.sets.union(
    a, b, validate_indices=True
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import union
  import tensorflow as tf
  import collections

  a = collections.OrderedDict([
      ((0, 0, 0), 1),
      ((0, 0, 1), 2),
      ((0, 1, 0), 3),
      ((1, 0, 0), 4),
      ((1, 1, 0), 5),
      ((1, 1, 1), 6),
  ])
  a = tf.sparse.SparseTensor(list(a.keys()), list(a.values()),
                             dense_shape=[2, 2, 2])

  b = collections.OrderedDict([
      ((0, 0, 0), 1),
      ((0, 0, 1), 3),
      ((0, 1, 0), 2),
      ((1, 0, 0), 4),
      ((1, 0, 1), 5),
      ((1, 1, 0), 5),
      ((1, 1, 1), 6),
      ((1, 1, 2), 7),
      ((1, 1, 3), 8),
  ])
  b = tf.sparse.SparseTensor(list(b.keys()), list(b.values()),
                             dense_shape=[2, 2, 4])

  tf.sets.union(a, b)

  #
  #
"
"tf.shape(
    input,
    out_type=tf.dtypes.int32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import shape
tf.shape(1.)
<tf.Tensor: shape=(0,), dtype=int32, numpy=array([], dtype=int32)>
<tf.Tensor: shape=(0,), dtype=int32, numpy=array([], dtype=int32)>"
"tf.math.sigmoid(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sigmoid
x = tf.constant([0.0, 1.0, 50.0, 100.0])
tf.math.sigmoid(x)
<tf.Tensor: shape=(4,), dtype=float32,
<tf.Tensor: shape=(4,), dtype=float32,
numpy=array([0.5, 0.7310586, 1.0, 1.0], dtype=float32)>"
"tf.math.sign(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sign
tf.math.sign([0., 2., -3.])
<tf.Tensor: shape=(3,), dtype=float32,
<tf.Tensor: shape=(3,), dtype=float32,
numpy=array([ 0.,  1., -1.], dtype=float32)>"
"tf.signal.fftshift(
    x, axes=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import fftshift
x = tf.signal.fftshift([ 0.,  1.,  2.,  3.,  4., -5., -4., -3., -2., -1.])
"
"tf.signal.frame(
    signal,
    frame_length,
    frame_step,
    pad_end=False,
    pad_value=0,
    axis=-1,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import frame
audio = tf.random.normal([3, 9152])
frames = tf.signal.frame(audio, 512, 180)
frames.shape.assert_is_compatible_with([3, 49, 512])
frames = tf.signal.frame(audio, 512, 180, pad_end=True)
frames.shape.assert_is_compatible_with([3, 51, 512])"
"tf.signal.ifftshift(
    x, axes=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ifftshift
x = tf.signal.ifftshift([[ 0.,  1.,  2.],[ 3.,  4., -4.],[-3., -2., -1.]])
"
"tf.signal.inverse_mdct(
    mdcts,
    window_fn=tf.signal.vorbis_window,
    norm=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import inverse_mdct
@tf.function
def compare_round_trip():
  samples = 1000
  frame_length = 400
  halflen = frame_length // 2
  waveform = tf.random.normal(dtype=tf.float32, shape=[samples])
  waveform_pad = tf.pad(waveform, [[halflen, 0],])
  mdct = tf.signal.mdct(waveform_pad, frame_length, pad_end=True,
                        window_fn=tf.signal.vorbis_window)
  inverse_mdct = tf.signal.inverse_mdct(mdct,
                                        window_fn=tf.signal.vorbis_window)
  inverse_mdct = inverse_mdct[halflen: halflen + samples]
  return waveform, inverse_mdct
waveform, inverse_mdct = compare_round_trip()
np.allclose(waveform.numpy(), inverse_mdct.numpy(), rtol=1e-3, atol=1e-4)
True"
"tf.signal.inverse_stft(
    stfts,
    frame_length,
    frame_step,
    fft_length=None,
    window_fn=tf.signal.hann_window,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import inverse_stft
frame_length = 400
frame_step = 160
waveform = tf.random.normal(dtype=tf.float32, shape=[1000])
stft = tf.signal.stft(waveform, frame_length, frame_step)
inverse_stft = tf.signal.inverse_stft(
    stft, frame_length, frame_step,
    window_fn=tf.signal.inverse_stft_window_fn(frame_step))
"
"tf.signal.mfccs_from_log_mel_spectrograms(
    log_mel_spectrograms, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mfccs_from_log_mel_spectrograms
batch_size, num_samples, sample_rate = 32, 32000, 16000.0
pcm = tf.random.normal([batch_size, num_samples], dtype=tf.float32)

stfts = tf.signal.stft(pcm, frame_length=1024, frame_step=256,
                       fft_length=1024)
spectrograms = tf.abs(stfts)

num_spectrogram_bins = stfts.shape[-1].value
lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 80
linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(
  num_mel_bins, num_spectrogram_bins, sample_rate, lower_edge_hertz,
  upper_edge_hertz)
mel_spectrograms = tf.tensordot(
  spectrograms, linear_to_mel_weight_matrix, 1)
mel_spectrograms.set_shape(spectrograms.shape[:-1].concatenate(
  linear_to_mel_weight_matrix.shape[-1:]))

log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)

mfccs = tf.signal.mfccs_from_log_mel_spectrograms(
  log_mel_spectrograms)[..., :13]
"
"tf.math.sin(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sin
  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 200, 10, float(""inf"")])
  tf.math.sin(x) ==> [nan -0.4121185 -0.47942555 0.84147096 0.9320391 -0.87329733 -0.54402107 nan]
"
"tf.math.sinh(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sinh
  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 2, 10, float(""inf"")])
  tf.math.sinh(x) ==> [-inf -4.0515420e+03 -5.2109528e-01 1.1752012e+00 1.5094614e+00 3.6268604e+00 1.1013232e+04 inf]
"
"tf.size(
    input,
    out_type=tf.dtypes.int32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import size
t = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])
tf.size(t)
<tf.Tensor: shape=(), dtype=int32, numpy=12>
<tf.Tensor: shape=(), dtype=int32, numpy=12>"
"tf.slice(
    input_, begin, size, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import slice
t = tf.constant([[[1, 1, 1], [2, 2, 2]],
                 [[3, 3, 3], [4, 4, 4]],
                 [[5, 5, 5], [6, 6, 6]]])
"
"tf.sort(
    values, axis=-1, direction='ASCENDING', name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sort
a = [1, 10, 26.9, 2.8, 166.32, 62.3]
tf.sort(a).numpy()
array([  1.  ,   2.8 ,  10.  ,  26.9 ,  62.3 , 166.32], dtype=float32)"
"tf.sparse.bincount(
    values,
    weights=None,
    axis=0,
    minlength=None,
    maxlength=None,
    binary_output=False,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bincount
data = np.array([[10, 20, 30, 20], [11, 101, 11, 10001]], dtype=np.int64)
output = tf.sparse.bincount(data, axis=-1)
print(output)
SparseTensor(indices=tf.Tensor(
[[    0    10]
 [    0    20]
 [    0    30]
 [    1    11]
 [    1   101]
 [    1 10001]], shape=(6, 2), dtype=int64),
 values=tf.Tensor([1 2 1 2 1 1], shape=(6,), dtype=int64),
 dense_shape=tf.Tensor([    2 10002], shape=(2,), dtype=int64))"
"tf.sparse.from_dense(
    tensor, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import from_dense
sp = tf.sparse.from_dense([0, 0, 3, 0, 1])
sp.shape.as_list()
[5]
sp.values.numpy()
array([3, 1], dtype=int32)
sp.indices.numpy()
array([[2],
       [4]])"
"tf.sparse.map_values(
    op, *args, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import map_values
s = tf.sparse.from_dense([[1, 2, 0],
                          [0, 4, 0],
                          [1, 0, 0]])
tf.sparse.to_dense(tf.sparse.map_values(tf.ones_like, s)).numpy()
array([[1, 1, 0],
       [0, 1, 0],
       [1, 0, 0]], dtype=int32)"
"tf.sparse.mask(
    a, mask_indices, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mask

b = tf.sparse.mask(a, [12, 45])

"
"tf.sparse.maximum(
    sp_a, sp_b, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import maximum
>>> sp_zero = tf.sparse.SparseTensor([[0]], [0], [7])
>>> sp_one = tf.sparse.SparseTensor([[1]], [1], [7])
>>> res = tf.sparse.maximum(sp_zero, sp_one)
>>> res.indices
<tf.Tensor: shape=(2, 1), dtype=int64, numpy=
<tf.Tensor: shape=(2, 1), dtype=int64, numpy=
array([[0],
       [1]])>
>>> res.values
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 1], dtype=int32)>
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 1], dtype=int32)>
>>> res.dense_shape
<tf.Tensor: shape=(1,), dtype=int64, numpy=array([7])>
<tf.Tensor: shape=(1,), dtype=int64, numpy=array([7])>
"
"tf.sparse.minimum(
    sp_a, sp_b, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import minimum
>>> sp_zero = tf.sparse.SparseTensor([[0]], [0], [7])
>>> sp_one = tf.sparse.SparseTensor([[1]], [1], [7])
>>> res = tf.sparse.minimum(sp_zero, sp_one)
>>> res.indices
<tf.Tensor: shape=(2, 1), dtype=int64, numpy=
<tf.Tensor: shape=(2, 1), dtype=int64, numpy=
array([[0],
       [1]])>
>>> res.values
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 0], dtype=int32)>
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 0], dtype=int32)>
>>> res.dense_shape
<tf.Tensor: shape=(1,), dtype=int64, numpy=array([7])>
<tf.Tensor: shape=(1,), dtype=int64, numpy=array([7])>
"
"tf.sparse.reduce_max(
    sp_input, axis=None, keepdims=None, output_is_sparse=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_max
x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 2, 3], [2, 3])
tf.sparse.reduce_max(x)
<tf.Tensor: shape=(), dtype=int32, numpy=3>
<tf.Tensor: shape=(), dtype=int32, numpy=3>
tf.sparse.reduce_max(x, 0)
<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 3, 2], dtype=int32)>
<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 3, 2], dtype=int32)>
tf.sparse.reduce_max(x, 1)
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 3], dtype=int32)>
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 3], dtype=int32)>
tf.sparse.reduce_max(x, 1, keepdims=True)
<tf.Tensor: shape=(2, 1), dtype=int32, numpy=
<tf.Tensor: shape=(2, 1), dtype=int32, numpy=
array([[2],
       [3]], dtype=int32)>
tf.sparse.reduce_max(x, [0, 1])
<tf.Tensor: shape=(), dtype=int32, numpy=3>
<tf.Tensor: shape=(), dtype=int32, numpy=3>"
"tf.sparse.reduce_sum(
    sp_input, axis=None, keepdims=None, output_is_sparse=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_sum
x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 1, 1], [2, 3])
tf.sparse.reduce_sum(x)
<tf.Tensor: shape=(), dtype=int32, numpy=3>
<tf.Tensor: shape=(), dtype=int32, numpy=3>
tf.sparse.reduce_sum(x, 0)
<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 1, 1], dtype=int32)>
<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 1, 1], dtype=int32)>
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 1], dtype=int32)>
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 1], dtype=int32)>
tf.sparse.reduce_sum(x, 1, keepdims=True)
<tf.Tensor: shape=(2, 1), dtype=int32, numpy=
<tf.Tensor: shape=(2, 1), dtype=int32, numpy=
array([[2],
       [1]], dtype=int32)>
tf.sparse.reduce_sum(x, [0, 1])
<tf.Tensor: shape=(), dtype=int32, numpy=3>
<tf.Tensor: shape=(), dtype=int32, numpy=3>"
"tf.sparse.segment_sum(
    data, indices, segment_ids, num_segments=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import segment_sum
c = tf.constant([[1,2,3,4], [-1,-2,-3,-4], [5,6,7,8]])

tf.sparse.segment_sum(c, tf.constant([0, 1]), tf.constant([0, 0]))

tf.sparse.segment_sum(c, tf.constant([0, 1]), tf.constant([0, 1]))

tf.sparse.segment_sum(c, tf.constant([0, 1]), tf.constant([0, 2]),
                      num_segments=4)

tf.sparse.segment_sum(c, tf.constant([0, 1, 2]), tf.constant([0, 0, 1]))

tf.math.segment_sum(c, tf.constant([0, 0, 1]))
"
"tf.sparse.softmax(
    sp_input, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import softmax
values = np.asarray([[[0., np.e], [1., 0.]], [[np.e, 0.], [np.e, np.e]]])
indices = np.vstack(np.where(values)).astype(np.int64).T

result = tf.sparse.softmax(tf.sparse.SparseTensor(indices, values, shape))
"
"tf.split(
    value, num_or_size_splits, axis=0, num=None, name='split'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import split
x = tf.Variable(tf.random.uniform([5, 30], -1, 1))
s0, s1, s2 = tf.split(x, num_or_size_splits=3, axis=1)
tf.shape(s0).numpy()
array([ 5, 10], dtype=int32)
split0, split1, split2 = tf.split(x, [4, 15, 11], 1)
tf.shape(split0).numpy()
array([5, 4], dtype=int32)
tf.shape(split1).numpy()
array([ 5, 15], dtype=int32)
tf.shape(split2).numpy()
array([ 5, 11], dtype=int32)"
"tf.math.sqrt(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sqrt
x = tf.constant([[4.0], [16.0]])
tf.sqrt(x)
<tf.Tensor: shape=(2, 1), dtype=float32, numpy=
<tf.Tensor: shape=(2, 1), dtype=float32, numpy=
  array([[2.],
         [4.]], dtype=float32)>
y = tf.constant([[-4.0], [16.0]])
tf.sqrt(y)
<tf.Tensor: shape=(2, 1), dtype=float32, numpy=
<tf.Tensor: shape=(2, 1), dtype=float32, numpy=
  array([[nan],
         [ 4.]], dtype=float32)>
z = tf.constant([[-1.0], [16.0]], dtype=tf.complex128)
tf.sqrt(z)
<tf.Tensor: shape=(2, 1), dtype=complex128, numpy=
<tf.Tensor: shape=(2, 1), dtype=complex128, numpy=
  array([[0.0+1.j],
         [4.0+0.j]])>"
"tf.math.square(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import square
tf.math.square([-2., 0., 3.])
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([4., 0., 9.], dtype=float32)>
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([4., 0., 9.], dtype=float32)>"
"tf.squeeze(
    input, axis=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import squeeze
"
"tf.stack(
    values, axis=0, name='stack'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stack
x = tf.constant([1, 4])
y = tf.constant([2, 5])
z = tf.constant([3, 6])
tf.stack([x, y, z])
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
array([[1, 4],
       [2, 5],
       [3, 6]], dtype=int32)>
tf.stack([x, y, z], axis=1)
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
array([[1, 2, 3],
       [4, 5, 6]], dtype=int32)>"
"tf.strided_slice(
    input_,
    begin,
    end,
    strides=None,
    begin_mask=0,
    end_mask=0,
    ellipsis_mask=0,
    new_axis_mask=0,
    shrink_axis_mask=0,
    var=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import strided_slice
t = tf.constant([[[1, 1, 1], [2, 2, 2]],
                 [[3, 3, 3], [4, 4, 4]],
                 [[5, 5, 5], [6, 6, 6]]])
"
"tf.strings.as_string(
    input,
    precision=-1,
    scientific=False,
    shortest=False,
    width=-1,
    fill='',
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import as_string
tf.strings.as_string([3, 2])
<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'3', b'2'], dtype=object)>
<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'3', b'2'], dtype=object)>
tf.strings.as_string([3.1415926, 2.71828], precision=2).numpy()
array([b'3.14', b'2.72'], dtype=object)"
"tf.strings.bytes_split(
    input, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bytes_split
tf.strings.bytes_split('hello').numpy()
array([b'h', b'e', b'l', b'l', b'o'], dtype=object)
tf.strings.bytes_split(['hello', '123'])
<tf.RaggedTensor [[b'h', b'e', b'l', b'l', b'o'], [b'1', b'2', b'3']]>
<tf.RaggedTensor [[b'h', b'e', b'l', b'l', b'o'], [b'1', b'2', b'3']]>"
"tf.strings.format(
    template, inputs, placeholder='{}', summarize=3, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import format
tensor = tf.range(5)
tf.strings.format(""tensor: {}, suffix"", tensor)
<tf.Tensor: shape=(), dtype=string, numpy=b'tensor: [0 1 2 3 4], suffix'>
<tf.Tensor: shape=(), dtype=string, numpy=b'tensor: [0 1 2 3 4], suffix'>"
"tf.strings.join(
    inputs, separator='', name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import join
tf.strings.join(['abc','def']).numpy()
b'abcdef'
tf.strings.join([['abc','123'],
                 ['def','456'],
                 ['ghi','789']]).numpy()
array([b'abcdefghi', b'123456789'], dtype=object)
tf.strings.join([['abc','123'],
                 ['def','456']],
                 separator="" "").numpy()
array([b'abc def', b'123 456'], dtype=object)"
"tf.strings.length(
    input, unit='BYTE', name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import length
strings = tf.constant(['Hello','TensorFlow', '\U0001F642'])
array([ 5, 10, 4], dtype=int32)
tf.strings.length(strings, unit=""UTF8_CHAR"").numpy()
array([ 5, 10, 1], dtype=int32)"
"tf.strings.lower(
    input, encoding='', name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import lower
tf.strings.lower(""CamelCase string and ALL CAPS"")
<tf.Tensor: shape=(), dtype=string, numpy=b'camelcase string and all caps'>
<tf.Tensor: shape=(), dtype=string, numpy=b'camelcase string and all caps'>"
"tf.strings.ngrams(
    data,
    ngram_width,
    separator=' ',
    pad_values=None,
    padding_width=None,
    preserve_short_sequences=False,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ngrams
tf.strings.ngrams([""A"", ""B"", ""C"", ""D""], 2).numpy()
array([b'A B', b'B C', b'C D'], dtype=object)
tf.strings.ngrams([""TF"", ""and"", ""keras""], 1).numpy()
array([b'TF', b'and', b'keras'], dtype=object)"
"tf.strings.reduce_join(
    inputs, axis=None, keepdims=False, separator='', name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_join
tf.strings.reduce_join([['abc','123'],
                        ['def','456']]).numpy()
b'abc123def456'
tf.strings.reduce_join([['abc','123'],
                        ['def','456']], axis=-1).numpy()
array([b'abc123', b'def456'], dtype=object)
tf.strings.reduce_join([['abc','123'],
                        ['def','456']],
                       axis=-1,
                       separator="" "").numpy()
array([b'abc 123', b'def 456'], dtype=object)"
"tf.strings.regex_full_match(
    input, pattern, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import regex_full_match
tf.strings.regex_full_match([""TF lib"", ""lib TF""], "".*lib$"")
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>
tf.strings.regex_full_match([""TF lib"", ""lib TF""], "".*TF$"")
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>"
"tf.strings.regex_replace(
    input, pattern, rewrite, replace_global=True, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import regex_replace
tf.strings.regex_replace(""Text with tags.<br /><b>contains html</b>"",
tf.strings.regex_replace(""Text with tags.<br /><b>contains html</b>"",
                         ""<[^>]+>"", "" "")
                         ""<[^>]+>"", "" "")
<tf.Tensor: shape=(), dtype=string, numpy=b'Text with tags.  contains html '>
<tf.Tensor: shape=(), dtype=string, numpy=b'Text with tags.  contains html '>"
"tf.strings.split(
    input, sep=None, maxsplit=-1, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import split
tf.strings.split('hello world').numpy()
 array([b'hello', b'world'], dtype=object)
tf.strings.split(['hello world', 'a b c'])
<tf.RaggedTensor [[b'hello', b'world'], [b'a', b'b', b'c']]>
<tf.RaggedTensor [[b'hello', b'world'], [b'a', b'b', b'c']]>"
"tf.strings.strip(
    input, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import strip
tf.strings.strip([""\nTensorFlow"", ""     The python library    ""]).numpy()
array([b'TensorFlow', b'The python library'], dtype=object)"
"tf.strings.to_hash_bucket(
    input, num_buckets, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import to_hash_bucket
tf.strings.to_hash_bucket([""Hello"", ""TensorFlow"", ""2.x""], 3)
<tf.Tensor: shape=(3,), dtype=int64, numpy=array([2, 0, 1])>
<tf.Tensor: shape=(3,), dtype=int64, numpy=array([2, 0, 1])>"
"tf.strings.to_hash_bucket_fast(
    input, num_buckets, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import to_hash_bucket_fast
tf.strings.to_hash_bucket_fast([""Hello"", ""TensorFlow"", ""2.x""], 3).numpy()
array([0, 2, 2])"
"tf.strings.to_hash_bucket_strong(
    input, num_buckets, key, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import to_hash_bucket_strong
tf.strings.to_hash_bucket_strong([""Hello"", ""TF""], 3, [1, 2]).numpy()
array([2, 0])"
"tf.strings.to_number(
    input,
    out_type=tf.dtypes.float32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import to_number
tf.strings.to_number(""1.55"")
<tf.Tensor: shape=(), dtype=float32, numpy=1.55>
<tf.Tensor: shape=(), dtype=float32, numpy=1.55>
tf.strings.to_number(""3"", tf.int32)
<tf.Tensor: shape=(), dtype=int32, numpy=3>
<tf.Tensor: shape=(), dtype=int32, numpy=3>"
"tf.strings.unicode_decode(
    input,
    input_encoding,
    errors='replace',
    replacement_char=65533,
    replace_control_characters=False,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import unicode_decode
input = [s.encode('utf8') for s in (u'G\xf6\xf6dnight', u'\U0001f60a')]
tf.strings.unicode_decode(input, 'UTF-8').to_list()
[[71, 246, 246, 100, 110, 105, 103, 104, 116], [128522]]"
"tf.strings.unicode_decode_with_offsets(
    input,
    input_encoding,
    errors='replace',
    replacement_char=65533,
    replace_control_characters=False,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import unicode_decode_with_offsets
input = [s.encode('utf8') for s in (u'G\xf6\xf6dnight', u'\U0001f60a')]
result = tf.strings.unicode_decode_with_offsets(input, 'UTF-8')
[[71, 246, 246, 100, 110, 105, 103, 104, 116], [128522]]
[[0, 1, 3, 5, 6, 7, 8, 9, 10], [0]]"
"tf.strings.unicode_script(
    input, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import unicode_script
tf.strings.unicode_script([1, 31, 38])
<tf.Tensor: shape=(3,), dtype=int32, numpy=array([0, 0, 0], dtype=int32)>
<tf.Tensor: shape=(3,), dtype=int32, numpy=array([0, 0, 0], dtype=int32)>"
"tf.strings.unicode_split(
    input,
    input_encoding,
    errors='replace',
    replacement_char=65533,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import unicode_split
input = [s.encode('utf8') for s in (u'G\xf6\xf6dnight', u'\U0001f60a')]
tf.strings.unicode_split(input, 'UTF-8').to_list()
[[b'G', b'\xc3\xb6', b'\xc3\xb6', b'd', b'n', b'i', b'g', b'h', b't'],
 [b'\xf0\x9f\x98\x8a']]"
"tf.strings.unicode_split_with_offsets(
    input,
    input_encoding,
    errors='replace',
    replacement_char=65533,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import unicode_split_with_offsets
input = [s.encode('utf8') for s in (u'G\xf6\xf6dnight', u'\U0001f60a')]
result = tf.strings.unicode_split_with_offsets(input, 'UTF-8')
[[b'G', b'\xc3\xb6', b'\xc3\xb6', b'd', b'n', b'i', b'g', b'h', b't'],
 [b'\xf0\x9f\x98\x8a']]
[[0, 1, 3, 5, 6, 7, 8, 9, 10], [0]]"
"tf.strings.unicode_transcode(
    input,
    input_encoding,
    output_encoding,
    errors='replace',
    replacement_char=65533,
    replace_control_characters=False,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import unicode_transcode
tf.strings.unicode_transcode([""Hello"", ""TensorFlow"", ""2.x""], ""UTF-8"", ""UTF-16-BE"")
<tf.Tensor: shape=(3,), dtype=string, numpy=
<tf.Tensor: shape=(3,), dtype=string, numpy=
array([b'\x00H\x00e\x00l\x00l\x00o',
       b'\x00T\x00e\x00n\x00s\x00o\x00r\x00F\x00l\x00o\x00w',
       b'\x002\x00.\x00x'], dtype=object)>
tf.strings.unicode_transcode([""A"", ""B"", ""C""], ""US ASCII"", ""UTF-8"").numpy()
array([b'A', b'B', b'C'], dtype=object)"
"tf.strings.upper(
    input, encoding='', name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import upper
tf.strings.upper(""CamelCase string and ALL CAPS"")
<tf.Tensor: shape=(), dtype=string, numpy=b'CAMELCASE STRING AND ALL CAPS'>
<tf.Tensor: shape=(), dtype=string, numpy=b'CAMELCASE STRING AND ALL CAPS'>"
"tf.math.subtract(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import subtract
x = [1, 2, 3, 4, 5]
y = 1
tf.subtract(x, y)
<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 1, 2, 3, 4], dtype=int32)>
<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 1, 2, 3, 4], dtype=int32)>
tf.subtract(y, x)
<tf.Tensor: shape=(5,), dtype=int32,
<tf.Tensor: shape=(5,), dtype=int32,
numpy=array([ 0, -1, -2, -3, -4], dtype=int32)>"
"tf.summary.graph(
    graph_data
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import graph
writer = tf.summary.create_file_writer(""/tmp/mylogs"")

@tf.function
def f():
  x = constant_op.constant(2)
  y = constant_op.constant(3)
  return x**y

with writer.as_default():
  tf.summary.graph(f.get_concrete_function().graph)

graph = tf.Graph()
with graph.as_default():
  c = tf.constant(30.0)
with writer.as_default():
  tf.summary.graph(graph)
"
"tf.summary.histogram(
    name, data, step=None, buckets=None, description=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import histogram
w = tf.summary.create_file_writer('test/logs')
with w.as_default():
    tf.summary.histogram(""activations"", tf.random.uniform([100, 50]), step=0)
    tf.summary.histogram(""initial_weights"", tf.random.normal([1000]), step=0)
"
"tf.summary.image(
    name, data, step=None, max_outputs=3, description=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import image
w = tf.summary.create_file_writer('test/logs')
with w.as_default():
  image1 = tf.random.uniform(shape=[8, 8, 1])
  image2 = tf.random.uniform(shape=[8, 8, 1])
  tf.summary.image(""grayscale_noise"", [image1, image2], step=0)
"
"tf.summary.scalar(
    name, data, step=None, description=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import scalar
test_summary_writer = tf.summary.create_file_writer('test/logdir')
with test_summary_writer.as_default():
    tf.summary.scalar('loss', 0.345, step=1)
    tf.summary.scalar('loss', 0.234, step=2)
    tf.summary.scalar('loss', 0.123, step=3)
"
"tf.summary.text(
    name, data, step=None, description=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import text
test_summary_writer = tf.summary.create_file_writer('test/logdir')
with test_summary_writer.as_default():
    tf.summary.text('first_text', 'hello world!', step=0)
    tf.summary.text('first_text', 'nice to meet you!', step=1)
"
"tf.math.tan(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import tan
  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
  tf.math.tan(x) ==> [nan 0.45231566 -0.5463025 1.5574077 2.572152 -1.7925274 0.32097113 nan]
"
"tf.tensor_scatter_nd_max(
    tensor, indices, updates, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import tensor_scatter_nd_max
tensor = [0, 0, 0, 0, 0, 0, 0, 0]
indices = [[1], [4], [5]]
updates = [1, -1, 1]
tf.tensor_scatter_nd_max(tensor, indices, updates).numpy()
array([0, 1, 0, 0, 0, 1, 0, 0], dtype=int32)"
"tf.test.compute_gradient(
    f, x, delta=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import compute_gradient
@tf.function
def test_func(x):
  return x*x
class MyTest(tf.test.TestCase):
  def test_gradient_of_test_func(self):
    theoretical, numerical = tf.test.compute_gradient(test_func, [1.0])
    self.assertAllClose(theoretical, numerical)"
"tf.test.create_local_cluster(
    num_workers,
    num_ps,
    protocol='grpc',
    worker_config=None,
    ps_config=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import create_local_cluster
workers, _ = tf.test.create_local_cluster(num_workers=2, num_ps=2)

worker_sessions = [tf.compat.v1.Session(w.target) for w in workers]

with tf.device(""/job:ps/task:0""):
  ...
with tf.device(""/job:ps/task:1""):
  ...
with tf.device(""/job:worker/task:0""):
  ...
with tf.device(""/job:worker/task:1""):
  ...

worker_sessions[0].run(...)
"
"tf.test.is_gpu_available(
    cuda_only=False, min_cuda_compute_capability=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import is_gpu_available
>>> gpu_available = tf.test.is_gpu_available()
>>> is_cuda_gpu_available = tf.test.is_gpu_available(cuda_only=True)
>>> is_cuda_gpu_min_3 = tf.test.is_gpu_available(True, (3,0))
"
"tf.tile(
    input, multiples, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import tile
a = tf.constant([[1,2,3],[4,5,6]], tf.int32)
b = tf.constant([1,2], tf.int32)
tf.tile(a, b)
<tf.Tensor: shape=(2, 6), dtype=int32, numpy=
<tf.Tensor: shape=(2, 6), dtype=int32, numpy=
array([[1, 2, 3, 1, 2, 3],
       [4, 5, 6, 4, 5, 6]], dtype=int32)>
c = tf.constant([2,1], tf.int32)
tf.tile(a, c)
<tf.Tensor: shape=(4, 3), dtype=int32, numpy=
<tf.Tensor: shape=(4, 3), dtype=int32, numpy=
array([[1, 2, 3],
       [4, 5, 6],
       [1, 2, 3],
       [4, 5, 6]], dtype=int32)>
d = tf.constant([2,2], tf.int32)
tf.tile(a, d)
<tf.Tensor: shape=(4, 6), dtype=int32, numpy=
<tf.Tensor: shape=(4, 6), dtype=int32, numpy=
array([[1, 2, 3, 1, 2, 3],
       [4, 5, 6, 4, 5, 6],
       [1, 2, 3, 1, 2, 3],
       [4, 5, 6, 4, 5, 6]], dtype=int32)>"
"tf.tpu.experimental.embedding.Adagrad(
    learning_rate: Union[float, Callable[[], float]] = 0.001,
    initial_accumulator_value: float = 0.1,
    use_gradient_accumulation: bool = True,
    clip_weight_min: Optional[float] = None,
    clip_weight_max: Optional[float] = None,
    weight_decay_factor: Optional[float] = None,
    multiply_weight_decay_factor_by_learning_rate: bool = None,
    slot_variable_creation_fn: Optional[SlotVarCreationFnType] = None,
    clipvalue: Optional[ClipValueType] = None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Adagrad
embedding = tf.tpu.experimental.embedding.TPUEmbedding(
    ...
    optimizer=tf.tpu.experimental.embedding.Adagrad(0.1))
"
"tf.tpu.experimental.embedding.AdagradMomentum(
    learning_rate: Union[float, Callable[[], float]] = 0.001,
    momentum: float = 0.0,
    use_nesterov: bool = False,
    exponent: float = 2,
    beta2: float = 1,
    epsilon: float = 1e-10,
    use_gradient_accumulation: bool = True,
    clip_weight_min: Optional[float] = None,
    clip_weight_max: Optional[float] = None,
    weight_decay_factor: Optional[float] = None,
    multiply_weight_decay_factor_by_learning_rate: bool = None,
    slot_variable_creation_fn: Optional[SlotVarCreationFnType] = None,
    clipvalue: Optional[ClipValueType] = None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import AdagradMomentum
embedding = tf.tpu.experimental.embedding.TPUEmbedding(
    ...
    optimizer=tf.tpu.experimental.embedding.AdagradMomentum(0.1))
"
"tf.tpu.experimental.embedding.Adam(
    learning_rate: Union[float, Callable[[], float]] = 0.001,
    beta_1: float = 0.9,
    beta_2: float = 0.999,
    epsilon: float = 1e-07,
    lazy_adam: bool = True,
    sum_inside_sqrt: bool = True,
    use_gradient_accumulation: bool = True,
    clip_weight_min: Optional[float] = None,
    clip_weight_max: Optional[float] = None,
    weight_decay_factor: Optional[float] = None,
    multiply_weight_decay_factor_by_learning_rate: bool = None,
    slot_variable_creation_fn: Optional[SlotVarCreationFnType] = None,
    clipvalue: Optional[ClipValueType] = None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Adam
embedding = tf.tpu.experimental.embedding.TPUEmbedding(
    ...
    optimizer=tf.tpu.experimental.embedding.Adam(0.1))
"
"tf.tpu.experimental.embedding.FTRL(
    learning_rate: Union[float, Callable[[], float]] = 0.001,
    learning_rate_power: float = -0.5,
    l1_regularization_strength: float = 0.0,
    l2_regularization_strength: float = 0.0,
    beta: float = 0.0,
    initial_accumulator_value: float = 0.1,
    use_gradient_accumulation: bool = True,
    clip_weight_min: Optional[float] = None,
    clip_weight_max: Optional[float] = None,
    weight_decay_factor: Optional[float] = None,
    multiply_weight_decay_factor_by_learning_rate: bool = None,
    slot_variable_creation_fn: Optional[SlotVarCreationFnType] = None,
    clipvalue: Optional[ClipValueType] = None,
    multiply_linear_by_learning_rate: bool = False,
    allow_zero_accumulator: bool = False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import FTRL
embedding = tf.tpu.experimental.embedding.TPUEmbedding(
    ...
    optimizer=tf.tpu.experimental.embedding.FTRL(0.1))
"
"tf.tpu.experimental.embedding.FeatureConfig(
    table: tf.tpu.experimental.embedding.TableConfig,
    max_sequence_length: int = 0,
    validate_weights_and_indices: bool = True,
    output_shape: Optional[Union[List[int], tf.TensorShape]] = None,
    name: Optional[Text] = None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import FeatureConfig
table_config_one = tf.tpu.experimental.embedding.TableConfig(
    vocabulary_size=...,
    dim=...)
table_config_two = tf.tpu.experimental.embedding.TableConfig(
    vocabulary_size=...,
    dim=...)
feature_config = {
    'feature_one': tf.tpu.experimental.embedding.FeatureConfig(
        table=table_config_one),
    'feature_two': tf.tpu.experimental.embedding.FeatureConfig(
        table=table_config_one),
    'feature_three': tf.tpu.experimental.embedding.FeatureConfig(
        table=table_config_two)}
embedding = tf.tpu.experimental.embedding.TPUEmbedding(
    feature_config=feature_config,
    batch_size=...
    optimizer=tf.tpu.experimental.embedding.Adam(0.1))
"
"tf.tpu.experimental.embedding.SGD(
    learning_rate: Union[float, Callable[[], float]] = 0.01,
    use_gradient_accumulation: bool = True,
    clip_weight_min: Optional[float] = None,
    clip_weight_max: Optional[float] = None,
    weight_decay_factor: Optional[float] = None,
    multiply_weight_decay_factor_by_learning_rate: bool = None,
    clipvalue: Optional[ClipValueType] = None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SGD
embedding = tf.tpu.experimental.embedding.TPUEmbedding(
    ...
    optimizer=tf.tpu.experimental.embedding.SGD(0.1))
"
"tf.tpu.experimental.embedding.TPUEmbeddingV0(
    feature_config: Union[tf.tpu.experimental.embedding.FeatureConfig, Iterable],
    optimizer: Optional[tpu_embedding_v2_utils._Optimizer]
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TPUEmbeddingV0
strategy = tf.distribute.TPUStrategy(...)
with strategy.scope():
  embedding = tf.tpu.experimental.embedding.TPUEmbeddingV0(
      feature_config=feature_config,
      optimizer=tf.tpu.experimental.embedding.SGD(0.1))
"
"tf.tpu.experimental.embedding.TableConfig(
    vocabulary_size: int,
    dim: int,
    initializer: Optional[Callable[[Any], None]] = None,
    optimizer: Optional[_Optimizer] = None,
    combiner: Text = 'mean',
    name: Optional[Text] = None,
    quantization_config: tf.tpu.experimental.embedding.QuantizationConfig = None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TableConfig
table_config_one = tf.tpu.experimental.embedding.TableConfig(
    vocabulary_size=...,
    dim=...)
table_config_two = tf.tpu.experimental.embedding.TableConfig(
    vocabulary_size=...,
    dim=...)
feature_config = {
    'feature_one': tf.tpu.experimental.embedding.FeatureConfig(
        table=table_config_one),
    'feature_two': tf.tpu.experimental.embedding.FeatureConfig(
        table=table_config_one),
    'feature_three': tf.tpu.experimental.embedding.FeatureConfig(
        table=table_config_two)}
embedding = tf.tpu.experimental.embedding.TPUEmbedding(
    feature_config=feature_config,
    batch_size=...
    optimizer=tf.tpu.experimental.embedding.Adam(0.1))
"
"tf.tpu.experimental.embedding.serving_embedding_lookup(
    inputs: Any,
    weights: Optional[Any],
    tables: Dict[tf.tpu.experimental.embedding.TableConfig, tf.Variable],
    feature_config: Union[tf.tpu.experimental.embedding.FeatureConfig, Iterable]
) -> Any
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import serving_embedding_lookup
model = model_fn(...)
embedding = tf.tpu.experimental.embedding.TPUEmbedding(
    feature_config=feature_config,
    batch_size=1024,
    optimizer=tf.tpu.experimental.embedding.SGD(0.1))
checkpoint = tf.train.Checkpoint(model=model, embedding=embedding)
checkpoint.restore(...)

@tf.function(input_signature=[{'feature_one': tf.TensorSpec(...),
                               'feature_two': tf.TensorSpec(...),
                               'feature_three': tf.TensorSpec(...)}])
def serve_tensors(embedding_features):
  embedded_features = tf.tpu.experimental.embedding.serving_embedding_lookup(
      embedding_features, None, embedding.embedding_tables,
      feature_config)
  return model(embedded_features)

model.embedding_api = embedding
tf.saved_model.save(model,
                    export_dir=...,
                    signatures={'serving_default': serve_tensors})

"
"tf.train.Checkpoint(
    root=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Checkpoint
model = tf.keras.Model(...)
checkpoint = tf.train.Checkpoint(model)

save_path = checkpoint.save('/tmp/training_checkpoints')

checkpoint.restore(save_path)
"
"tf.train.CheckpointManager(
    checkpoint,
    directory,
    max_to_keep,
    keep_checkpoint_every_n_hours=None,
    checkpoint_name='ckpt',
    step_counter=None,
    checkpoint_interval=None,
    init_fn=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CheckpointManager
import tensorflow as tf
checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)
manager = tf.train.CheckpointManager(
    checkpoint, directory=""/tmp/model"", max_to_keep=5)
status = checkpoint.restore(manager.latest_checkpoint)
while True:
  manager.save()
"
"tf.train.CheckpointOptions(
    experimental_io_device=None, experimental_enable_async_checkpoint=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CheckpointOptions
step = tf.Variable(0, name=""step"")
checkpoint = tf.train.Checkpoint(step=step)
options = tf.train.CheckpointOptions(experimental_io_device=""/job:localhost"")
checkpoint.save(""/tmp/ckpt"", options=options)
"
"tf.train.ClusterSpec(
    cluster
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ClusterSpec
cluster = tf.train.ClusterSpec({""worker"": [""worker0.example.com:2222"",
                                           ""worker1.example.com:2222"",
                                           ""worker2.example.com:2222""],
                                ""ps"": [""ps0.example.com:2222"",
                                       ""ps1.example.com:2222""]})
"
"tf.train.Coordinator(
    clean_stop_exception_types=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Coordinator
coord = Coordinator()
...start thread 1...(coord, ...)
...start thread N...(coord, ...)
coord.join(threads)
"
"tf.train.ExponentialMovingAverage(
    decay,
    num_updates=None,
    zero_debias=False,
    name='ExponentialMovingAverage'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ExponentialMovingAverage
var0 = tf.Variable(...)
var1 = tf.Variable(...)

ema = tf.train.ExponentialMovingAverage(decay=0.9999)

ema.apply([var0, var1])

averages = [ema.average(var0), ema.average(var1)]

...
def train_step(...):
...
  opt.minimize(my_loss, [var0, var1])

  ema.apply([var0, var1])

...train the model by running train_step multiple times...
"
"tf.train.list_variables(
    ckpt_dir_or_file
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import list_variables
</td>
</td>
</tr>
</tr>

</table>
</table>


import tensorflow as tf
import os
ckpt_directory = ""/tmp/training_checkpoints/ckpt""
ckpt = tf.train.Checkpoint(optimizer=optimizer, model=model)
manager = tf.train.CheckpointManager(ckpt, ckpt_directory, max_to_keep=3)
train_and_checkpoint(model, manager)
tf.train.list_variables(manager.latest_checkpoint)
"
"tf.transpose(
    a, perm=None, conjugate=False, name='transpose'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import transpose
x = tf.constant([[1, 2, 3], [4, 5, 6]])
tf.transpose(x)
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
array([[1, 4],
       [2, 5],
       [3, 6]], dtype=int32)>"
"tf.type_spec_from_value(
    value
) -> tf.TypeSpec
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import type_spec_from_value
>>> tf.type_spec_from_value(tf.constant([1, 2, 3]))
TensorSpec(shape=(3,), dtype=tf.int32, name=None)
>>> tf.type_spec_from_value(np.array([4.0, 5.0], np.float64))
TensorSpec(shape=(2,), dtype=tf.float64, name=None)
>>> tf.type_spec_from_value(tf.ragged.constant([[1, 2], [3, 4, 5]]))
RaggedTensorSpec(TensorShape([2, None]), tf.int32, 1, tf.int64)
"
"tf.unique(
    x,
    out_idx=tf.dtypes.int32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import unique
y, idx = unique(x)
y ==> [1, 2, 4, 7, 8]
idx ==> [0, 0, 1, 2, 2, 2, 3, 4, 4]
"
"tf.unique_with_counts(
    x,
    out_idx=tf.dtypes.int32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import unique_with_counts
y, idx, count = unique_with_counts(x)
y ==> [1, 2, 4, 7, 8]
idx ==> [0, 0, 1, 2, 2, 2, 3, 4, 4]
count ==> [2, 1, 3, 1, 2]
"
"tf.unravel_index(
    indices, dims, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import unravel_index
y = tf.unravel_index(indices=[2, 5, 7], dims=[3, 3])
y ==> [[0, 1, 2], [2, 2, 1]]
"
"tf.unstack(
    value, num=None, axis=0, name='unstack'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import unstack
x = tf.reshape(tf.range(12), (3,4))
p, q, r = tf.unstack(x)
p.shape.as_list()
[4]"
"tf.vectorized_map(
    fn, elems, fallback_to_while_loop=True, warn=True
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import vectorized_map
def outer_product(a):
  return tf.tensordot(a, a, 0)

batch_size = 100
a = tf.ones((batch_size, 32, 32))
c = tf.vectorized_map(outer_product, a)
assert c.shape == (batch_size, 32, 32, 32, 32)
"
"tf.where(
    condition, x=None, y=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import where
tf.where([True, False, False, True]).numpy()
array([[0],
       [3]])"
"tf.while_loop(
    cond,
    body,
    loop_vars,
    shape_invariants=None,
    parallel_iterations=10,
    back_prop=True,
    swap_memory=False,
    maximum_iterations=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import while_loop
i = tf.constant(0)
c = lambda i: tf.less(i, 10)
b = lambda i: (tf.add(i, 1), )
r = tf.while_loop(c, b, [i])
"
"tf.zeros(
    shape,
    dtype=tf.dtypes.float32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import zeros
tf.zeros([3, 4], tf.int32)
<tf.Tensor: shape=(3, 4), dtype=int32, numpy=
<tf.Tensor: shape=(3, 4), dtype=int32, numpy=
array([[0, 0, 0, 0],
       [0, 0, 0, 0],
       [0, 0, 0, 0]], dtype=int32)>"
"tf.zeros_like(
    input, dtype=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import zeros_like
>>> tensor = tf.constant([[1, 2, 3], [4, 5, 6]])
>>> tf.zeros_like(tensor)
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
array([[0, 0, 0],
       [0, 0, 0]], dtype=int32)>
"
"tf.debugging.Assert(
    condition, data, summarize=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Assert
assert_op = tf.Assert(tf.less_equal(tf.reduce_max(x), 1.), [x])
with tf.control_dependencies([assert_op]):
  ... code using x ...
"
"tf.DeviceSpec(
    job=None, replica=None, task=None, device_type=None, device_index=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import DeviceSpec
device_spec = DeviceSpec(job=""ps"", device_type=""GPU"", device_index=0)
with tf.device(device_spec.to_string()):
  my_var = tf.Variable(..., name=""my_variable"")
  squared_var = tf.square(my_var)
"
"tf.GradientTape(
    persistent=False, watch_accessed_variables=True
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GradientTape
x = tf.constant(3.0)
with tf.GradientTape() as g:
  g.watch(x)
  y = x * x
dy_dx = g.gradient(y, x)
print(dy_dx)
tf.Tensor(6.0, shape=(), dtype=float32)"
"tf.Module(
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
class Dense(tf.Module):
  def __init__(self, input_dim, output_size, name=None):
    super().__init__(name=name)
    self.w = tf.Variable(
      tf.random.normal([input_dim, output_size]), name='w')
    self.b = tf.Variable(tf.zeros([output_size]), name='b')
  def __call__(self, x):
    y = tf.matmul(x, self.w) + self.b
    return tf.nn.relu(y)"
"tf.OptionalSpec(
    element_spec
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import OptionalSpec
@tf.function(input_signature=[tf.OptionalSpec(
  tf.TensorSpec(shape=(), dtype=tf.int32, name=None))])
def maybe_square(optional):
  if optional.has_value():
    x = optional.get_value()
    return x * x
  return -1
optional = tf.experimental.Optional.from_value(5)
print(maybe_square(optional))
tf.Tensor(25, shape=(), dtype=int32)"
"tf.RegisterGradient(
    op_type
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RegisterGradient
@tf.RegisterGradient(""Sub"")
def _sub_grad(unused_op, grad):
  return grad, tf.negative(grad)
"
"tf.Tensor(
    op, value_index, dtype
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Tensor
c = tf.constant([[1.0, 2.0], [3.0, 4.0]])
d = tf.constant([[1.0, 1.0], [0.0, 1.0]])
e = tf.matmul(c, d)
print(e)
tf.Tensor(
[[1. 3.]
 [3. 7.]], shape=(2, 2), dtype=float32)"
"tf.TensorArray(
    dtype,
    size=None,
    dynamic_size=None,
    clear_after_read=None,
    tensor_array_name=None,
    handle=None,
    flow=None,
    infer_shape=True,
    element_shape=None,
    colocate_with_first_write_call=True,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TensorArray
ta = tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False)
ta = ta.write(0, 10)
ta = ta.write(1, 20)
ta = ta.write(2, 30)
ta.read(0)
<tf.Tensor: shape=(), dtype=float32, numpy=10.0>
<tf.Tensor: shape=(), dtype=float32, numpy=10.0>
ta.read(1)
<tf.Tensor: shape=(), dtype=float32, numpy=20.0>
<tf.Tensor: shape=(), dtype=float32, numpy=20.0>
ta.read(2)
<tf.Tensor: shape=(), dtype=float32, numpy=30.0>
<tf.Tensor: shape=(), dtype=float32, numpy=30.0>
ta.stack()
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([10., 20., 30.],
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([10., 20., 30.],
dtype=float32)>"
"tf.TensorSpec(
    shape,
    dtype=tf.dtypes.float32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
experimental_as_proto() -> struct_pb2.TensorSpecProto
"
"tf.Variable(
    initial_value=None,
    trainable=None,
    validate_shape=True,
    caching_device=None,
    name=None,
    variable_def=None,
    dtype=None,
    import_scope=None,
    constraint=None,
    synchronization=tf.VariableSynchronization.AUTO,
    aggregation=tf.compat.v1.VariableAggregation.NONE,
    shape=None,
    experimental_enable_variable_lifting=True
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Variable
v = tf.Variable(1.)
v.assign(2.)
<tf.Variable ... shape=() dtype=float32, numpy=2.0>
<tf.Variable ... shape=() dtype=float32, numpy=2.0>
v.assign_add(0.5)
<tf.Variable ... shape=() dtype=float32, numpy=2.5>
<tf.Variable ... shape=() dtype=float32, numpy=2.5>"
"tf.math.abs(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import abs
x = tf.constant([-2.25, 3.25])
tf.abs(x)
<tf.Tensor: shape=(2,), dtype=float32,
<tf.Tensor: shape=(2,), dtype=float32,
numpy=array([2.25, 3.25], dtype=float32)>"
"tf.math.acos(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import acos
x = tf.constant([1.0, -0.5, 3.4, 0.2, 0.0, -2], dtype = tf.float32)
tf.math.acos(x)
<tf.Tensor: shape=(6,), dtype=float32,
<tf.Tensor: shape=(6,), dtype=float32,
numpy= array([0. , 2.0943952, nan, 1.3694383, 1.5707964, nan],
dtype=float32)>"
"tf.math.acosh(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import acosh
x = tf.constant([-2, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
tf.math.acosh(x) ==> [nan nan 0. 0.62236255 5.9914584 9.903487 inf]
"
"tf.math.add(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import add
x = [1, 2, 3, 4, 5]
y = 1
tf.add(x, y)
<tf.Tensor: shape=(5,), dtype=int32, numpy=array([2, 3, 4, 5, 6],
<tf.Tensor: shape=(5,), dtype=int32, numpy=array([2, 3, 4, 5, 6],
dtype=int32)>"
"tf.math.add_n(
    inputs, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import add_n
a = tf.constant([[3, 5], [4, 8]])
b = tf.constant([[1, 6], [2, 9]])
tf.math.add_n([a, b, a]).numpy()
array([[ 7, 16],
       [10, 25]], dtype=int32)"
"tf.math.argmax(
    input,
    axis=None,
    output_type=tf.dtypes.int64,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import argmax
A = tf.constant([2, 20, 30, 3, 6])
<tf.Tensor: shape=(), dtype=int64, numpy=2>
<tf.Tensor: shape=(), dtype=int64, numpy=2>
B = tf.constant([[2, 20, 30, 3, 6], [3, 11, 16, 1, 8],
                 [14, 45, 23, 5, 27]])
tf.math.argmax(B, 0)
<tf.Tensor: shape=(5,), dtype=int64, numpy=array([2, 2, 0, 2, 2])>
<tf.Tensor: shape=(5,), dtype=int64, numpy=array([2, 2, 0, 2, 2])>
tf.math.argmax(B, 1)
<tf.Tensor: shape=(3,), dtype=int64, numpy=array([2, 2, 1])>
<tf.Tensor: shape=(3,), dtype=int64, numpy=array([2, 2, 1])>
C = tf.constant([0, 0, 0, 0])
<tf.Tensor: shape=(), dtype=int64, numpy=0>
<tf.Tensor: shape=(), dtype=int64, numpy=0>"
"tf.math.argmin(
    input,
    axis=None,
    output_type=tf.dtypes.int64,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import argmin
import tensorflow as tf
a = [1, 10, 26.9, 2.8, 166.32, 62.3]
b = tf.math.argmin(input = a)
c = tf.keras.backend.eval(b)
"
"tf.argsort(
    values, axis=-1, direction='ASCENDING', stable=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import argsort
values = [1, 10, 26.9, 2.8, 166.32, 62.3]
sort_order = tf.argsort(values)
sort_order.numpy()
array([0, 3, 1, 2, 5, 4], dtype=int32)"
"tf.strings.as_string(
    input,
    precision=-1,
    scientific=False,
    shortest=False,
    width=-1,
    fill='',
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import as_string
tf.strings.as_string([3, 2])
<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'3', b'2'], dtype=object)>
<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'3', b'2'], dtype=object)>
tf.strings.as_string([3.1415926, 2.71828], precision=2).numpy()
array([b'3.14', b'2.72'], dtype=object)"
"tf.math.asin(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import asin
x = tf.constant([1.047, 0.785])

"
"tf.math.asinh(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import asinh
  x = tf.constant([-float(""inf""), -2, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
  tf.math.asinh(x) ==> [-inf -1.4436355 -0.4812118 0.8813736 1.0159732 5.991471 9.903487 inf]
"
"tf.math.atan(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import atan
x = tf.constant([1.047, 0.785])

"
"tf.math.atan2(
    y, x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import atan2
x = [1., 1.]
y = [1., -1.]
print((tf.math.atan2(y,x) * (180 / np.pi)).numpy())
[ 45. -45.]"
"tf.math.atanh(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import atanh
  x = tf.constant([-float(""inf""), -1, -0.5, 1, 0, 0.5, 10, float(""inf"")])
  tf.math.atanh(x) ==> [nan -inf -0.54930615 inf  0. 0.54930615 nan nan]
"
"tf.autodiff.ForwardAccumulator(
    primals, tangents
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ForwardAccumulator
x = tf.constant([[2.0, 3.0], [1.0, 4.0]])
targets = tf.constant([[1.], [-1.]])
dense = tf.keras.layers.Dense(1)
dense.build([None, 2])
with tf.autodiff.ForwardAccumulator(
   primals=dense.kernel,
   tangents=tf.constant([[1.], [0.]])) as acc:
  loss = tf.reduce_sum((dense(x) - targets) ** 2.)
acc.jvp(loss)
<tf.Tensor: shape=(), dtype=float32, numpy=...>
<tf.Tensor: shape=(), dtype=float32, numpy=...>"
"tf.GradientTape(
    persistent=False, watch_accessed_variables=True
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GradientTape
x = tf.constant(3.0)
with tf.GradientTape() as g:
  g.watch(x)
  y = x * x
dy_dx = g.gradient(y, x)
print(dy_dx)
tf.Tensor(6.0, shape=(), dtype=float32)"
"tf.autograph.experimental.set_loop_options(
    parallel_iterations=UNSPECIFIED,
    swap_memory=UNSPECIFIED,
    maximum_iterations=UNSPECIFIED,
    shape_invariants=UNSPECIFIED
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import set_loop_options
>>> @tf.function(autograph=True)
... def f():
...   n = 0
...   for i in tf.range(10):
...     tf.autograph.experimental.set_loop_options(maximum_iterations=3)
...     n += 1
...   return n
"
"tf.autograph.set_verbosity(
    level, alsologtostdout=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import set_verbosity
import os
import tensorflow as tf

os.environ['AUTOGRAPH_VERBOSITY'] = '5'

tf.autograph.set_verbosity(0)

os.environ['AUTOGRAPH_VERBOSITY'] = '1'
"
"tf.autograph.to_code(
    entity, recursive=True, experimental_optional_features=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import to_code
def f(x):
  if x < 0:
  if x < 0:
    x = -x
  return x
tf.autograph.to_code(f)
""...def tf__f(x):..."""
"tf.autograph.to_graph(
    entity, recursive=True, experimental_optional_features=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import to_graph
def f(x):
  if x > 0:
    y = x * x
  else:
    y = -x
  return y
converted_f = to_graph(f)
x = tf.constant(2)
<tf.Tensor: shape=(), dtype=int32, numpy=4>
<tf.Tensor: shape=(), dtype=int32, numpy=4>"
"tf.autograph.trace(
    *args
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import trace
import tensorflow as tf

for i in tf.range(10):
  tf.autograph.trace(i)
# Output: <Tensor ...>
"
"tf.bitcast(
    input, type, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bitcast
a = [1., 2., 3.]
equality_bitcast = tf.bitcast(a, tf.complex128)
Traceback (most recent call last):
InvalidArgumentError: Cannot bitcast from 1 to 18 [Op:Bitcast]
equality_cast = tf.cast(a, tf.complex128)
print(equality_cast)
tf.Tensor([1.+0.j 2.+0.j 3.+0.j], shape=(3,), dtype=complex128)"
"tf.bitwise.bitwise_and(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bitwise_and
import tensorflow as tf
from tensorflow.python.ops import bitwise_ops
dtype_list = [tf.int8, tf.int16, tf.int32, tf.int64,
              tf.uint8, tf.uint16, tf.uint32, tf.uint64]

for dtype in dtype_list:
  lhs = tf.constant([0, 5, 3, 14], dtype=dtype)
  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)
  exp = tf.constant([0, 0, 3, 10], dtype=tf.float32)

  res = bitwise_ops.bitwise_and(lhs, rhs)
"
"tf.bitwise.bitwise_or(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bitwise_or
import tensorflow as tf
from tensorflow.python.ops import bitwise_ops
dtype_list = [tf.int8, tf.int16, tf.int32, tf.int64,
              tf.uint8, tf.uint16, tf.uint32, tf.uint64]

for dtype in dtype_list:
  lhs = tf.constant([0, 5, 3, 14], dtype=dtype)
  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)
  exp = tf.constant([5, 5, 7, 15], dtype=tf.float32)

  res = bitwise_ops.bitwise_or(lhs, rhs)
"
"tf.bitwise.bitwise_xor(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bitwise_xor
import tensorflow as tf
from tensorflow.python.ops import bitwise_ops
dtype_list = [tf.int8, tf.int16, tf.int32, tf.int64,
              tf.uint8, tf.uint16, tf.uint32, tf.uint64]

for dtype in dtype_list:
  lhs = tf.constant([0, 5, 3, 14], dtype=dtype)
  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)
  exp = tf.constant([5, 5, 4, 5],  dtype=tf.float32)

  res = bitwise_ops.bitwise_xor(lhs, rhs)
"
"tf.bitwise.invert(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import invert
import tensorflow as tf
from tensorflow.python.ops import bitwise_ops

tf.assert_equal(-3, bitwise_ops.invert(2))

dtype_list = [dtypes.int8, dtypes.int16, dtypes.int32, dtypes.int64,
              dtypes.uint8, dtypes.uint16, dtypes.uint32, dtypes.uint64]

inputs = [0, 5, 3, 14]
for dtype in dtype_list:
  input_tensor = tf.constant([0, 5, 3, 14], dtype=dtype)
  not_a_and_a, not_a_or_a, not_0 = [bitwise_ops.bitwise_and(
                                      input_tensor, bitwise_ops.invert(input_tensor)),
                                    bitwise_ops.bitwise_or(
                                      input_tensor, bitwise_ops.invert(input_tensor)),
                                    bitwise_ops.invert(
                                      tf.constant(0, dtype=dtype))]

  expected = tf.constant([0, 0, 0, 0], dtype=tf.float32)
  tf.assert_equal(tf.cast(not_a_and_a, tf.float32), expected)

  expected = tf.cast([not_0] * 4, tf.float32)
  tf.assert_equal(tf.cast(not_a_or_a, tf.float32), expected)

  if dtype.is_unsigned:
    inverted = bitwise_ops.invert(input_tensor)
    expected = tf.constant([dtype.max - x for x in inputs], dtype=tf.float32)
    tf.assert_equal(tf.cast(inverted, tf.float32), tf.cast(expected, tf.float32))
"
"tf.bitwise.left_shift(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import left_shift
import tensorflow as tf
from tensorflow.python.ops import bitwise_ops
import numpy as np
dtype_list = [tf.int8, tf.int16, tf.int32, tf.int64]

for dtype in dtype_list:
  lhs = tf.constant([-1, -5, -3, -14], dtype=dtype)
  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)

  left_shift_result = bitwise_ops.left_shift(lhs, rhs)

  print(left_shift_result)


lhs = np.array([-2, 64, 101, 32], dtype=np.int8)
rhs = np.array([-1, -5, -3, -14], dtype=np.int8)
bitwise_ops.left_shift(lhs, rhs)
# <tf.Tensor: shape=(4,), dtype=int8, numpy=array([ -2,  64, 101,  32], dtype=int8)>
"
"tf.bitwise.right_shift(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import right_shift
import tensorflow as tf
from tensorflow.python.ops import bitwise_ops
import numpy as np
dtype_list = [tf.int8, tf.int16, tf.int32, tf.int64]

for dtype in dtype_list:
  lhs = tf.constant([-1, -5, -3, -14], dtype=dtype)
  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)

  right_shift_result = bitwise_ops.right_shift(lhs, rhs)

  print(right_shift_result)


lhs = np.array([-2, 64, 101, 32], dtype=np.int8)
rhs = np.array([-1, -5, -3, -14], dtype=np.int8)
bitwise_ops.right_shift(lhs, rhs)
# <tf.Tensor: shape=(4,), dtype=int8, numpy=array([ -2,  64, 101,  32], dtype=int8)>
"
"tf.boolean_mask(
    tensor, mask, axis=None, name='boolean_mask'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import boolean_mask
mask = np.array([True, False, True, False])
tf.boolean_mask(tensor, mask)
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 2], dtype=int32)>
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 2], dtype=int32)>"
"tf.broadcast_dynamic_shape(
    shape_x, shape_y
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import broadcast_dynamic_shape
shape_x = (1, 2, 3)
shape_y = (5, 1, 3)
tf.broadcast_dynamic_shape(shape_x, shape_y)
<tf.Tensor: shape=(3,), dtype=int32, numpy=array([5, 2, 3], ...>
<tf.Tensor: shape=(3,), dtype=int32, numpy=array([5, 2, 3], ...>"
"tf.broadcast_static_shape(
    shape_x, shape_y
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import broadcast_static_shape
shape_x = tf.TensorShape([1, 2, 3])
shape_y = tf.TensorShape([5, 1 ,3])
tf.broadcast_static_shape(shape_x, shape_y)
TensorShape([5, 2, 3])"
"tf.broadcast_to(
    input, shape, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import broadcast_to
y = tf.broadcast_to(x, [2, 3])
print(y)
tf.Tensor(
    [[1 2 3]
     [1 2 3]], shape=(2, 3), dtype=int32)"
"tf.cast(
    x, dtype, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import cast
x = tf.constant([1.8, 2.2], dtype=tf.float32)
tf.cast(x, tf.int32)
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>"
"tf.clip_by_norm(
    t, clip_norm, axes=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import clip_by_norm
some_nums = tf.constant([[1, 2, 3, 4, 5]], dtype=tf.float32)
tf.clip_by_norm(some_nums, 2.0).numpy()
array([[0.26967996, 0.5393599 , 0.80903983, 1.0787199 , 1.3483998 ]],
      dtype=float32)"
"tf.clip_by_value(
    t, clip_value_min, clip_value_max, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import clip_by_value
t = tf.constant([[-10., -1., 0.], [0., 2., 10.]])
t2 = tf.clip_by_value(t, clip_value_min=-1, clip_value_max=1)
t2.numpy()
array([[-1., -1.,  0.],
       [ 0.,  1.,  1.]], dtype=float32)"
"tf.compat.dimension_at_index(
    shape, index
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import dimension_at_index
dim = tensor_shape[i]

dim = dimension_at_index(tensor_shape, i)


if tensor_shape.rank is None:
  dim = Dimension(None)
else:
  dim = tensor_shape.dims[i]

"
"tf.compat.dimension_value(
    dimension
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import dimension_value
value = tensor_shape[i].value

value = dimension_value(tensor_shape[i])

"
"tf.compat.path_to_str(
    path
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import path_to_str
$ tf.compat.path_to_str('C:\XYZ\tensorflow\./.././tensorflow')
$ tf.compat.path_to_str(Path('C:\XYZ\tensorflow\./.././tensorflow'))
$ tf.compat.path_to_str(Path('./corpus'))
$ tf.compat.path_to_str('./.././Corpus')
$ tf.compat.path_to_str(Path('./.././Corpus'))
$ tf.compat.path_to_str(Path('./..////../'))

"
"tf.dtypes.complex(
    real, imag, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import complex
real = tf.constant([2.25, 3.25])
imag = tf.constant([4.75, 5.75])
"
"tf.cond(
    pred, true_fn=None, false_fn=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import cond
z = tf.multiply(a, b)
result = tf.cond(x < y, lambda: tf.add(x, z), lambda: tf.square(y))
result = tf.cond(x < y, lambda: tf.add(x, z), lambda: tf.square(y))
"
"tf.config.experimental.enable_tensor_float_32_execution(
    enabled
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import enable_tensor_float_32_execution
x = tf.fill((2, 2), 1.0001)
y = tf.fill((2, 2), 1.)
tf.config.experimental.enable_tensor_float_32_execution(False)
"
"tf.config.experimental.get_device_details(
    device
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get_device_details
gpu_devices = tf.config.list_physical_devices('GPU')
if gpu_devices:
  details = tf.config.experimental.get_device_details(gpu_devices[0])
  details.get('device_name', 'Unknown GPU')"
"tf.config.experimental.get_memory_growth(
    device
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get_memory_growth
physical_devices = tf.config.list_physical_devices('GPU')
try:
  tf.config.experimental.set_memory_growth(physical_devices[0], True)
  assert tf.config.experimental.get_memory_growth(physical_devices[0])
except:
  pass"
"tf.config.experimental.get_memory_info(
    device
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get_memory_info
if tf.config.list_physical_devices('GPU'):
  # Returns a dict in the form {'current': <current mem usage>,
  #                             'peak': <peak mem usage>}
  tf.config.experimental.get_memory_info('GPU:0')"
"tf.config.experimental.get_memory_usage(
    device
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get_memory_usage
gpu_devices = tf.config.list_physical_devices('GPU')
if gpu_devices:
  tf.config.experimental.get_memory_usage('GPU:0')"
"tf.config.get_logical_device_configuration(
    device
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get_logical_device_configuration
physical_devices = tf.config.list_physical_devices('CPU')
assert len(physical_devices) == 1, ""No CPUs found""
configs = tf.config.get_logical_device_configuration(
  physical_devices[0])
try:
  assert configs is None
  tf.config.set_logical_device_configuration(
    physical_devices[0],
    [tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration()])
  configs = tf.config.get_logical_device_configuration(
    physical_devices[0])
  assert len(configs) == 2
except:
  pass"
"tf.config.get_visible_devices(
    device_type=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get_visible_devices
physical_devices = tf.config.list_physical_devices('GPU')
try:
  tf.config.set_visible_devices([], 'GPU')
  visible_devices = tf.config.get_visible_devices()
  for device in visible_devices:
    assert device.device_type != 'GPU'
except:
  pass"
"tf.config.list_logical_devices(
    device_type=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import list_logical_devices
logical_devices = tf.config.list_logical_devices('GPU')
if len(logical_devices) > 0:
  with tf.device(logical_devices[0].name):
    one = tf.constant(1)
  with tf.device(logical_devices[1].name):
    two = tf.constant(2)"
"tf.config.list_physical_devices(
    device_type=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import list_physical_devices
physical_devices = tf.config.list_physical_devices('GPU')
print(""Num GPUs:"", len(physical_devices))
Num GPUs: ..."
"tf.config.experimental.reset_memory_stats(
    device
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reset_memory_stats
if tf.config.list_physical_devices('GPU'):
  tf.config.experimental.reset_memory_stats('GPU:0')
  x1 = tf.ones(1000 * 1000, dtype=tf.float64)
  peak1 = tf.config.experimental.get_memory_info('GPU:0')['peak']
  tf.config.experimental.reset_memory_stats('GPU:0')
  x2 = tf.ones(1000 * 1000, dtype=tf.float32)
  del x2
  peak2 = tf.config.experimental.get_memory_info('GPU:0')['peak']
  assert peak2 < peak1  # tf.float32 consumes less memory than tf.float64."
"tf.config.experimental.set_memory_growth(
    device, enable
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import set_memory_growth
physical_devices = tf.config.list_physical_devices('GPU')
try:
  tf.config.experimental.set_memory_growth(physical_devices[0], True)
except:
  pass"
"tf.config.set_logical_device_configuration(
    device, logical_devices
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import set_logical_device_configuration
physical_devices = tf.config.list_physical_devices('CPU')
assert len(physical_devices) == 1, ""No CPUs found""
try:
  tf.config.set_logical_device_configuration(
    physical_devices[0],
    [tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration()])
  logical_devices = tf.config.list_logical_devices('CPU')
  assert len(logical_devices) == 2
  tf.config.set_logical_device_configuration(
    physical_devices[0],
    [tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration()])
except:
  pass"
"tf.config.set_visible_devices(
    devices, device_type=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import set_visible_devices
physical_devices = tf.config.list_physical_devices('GPU')
try:
  tf.config.set_visible_devices(physical_devices[1:], 'GPU')
  logical_devices = tf.config.list_logical_devices('GPU')
  assert len(logical_devices) == len(physical_devices) - 1
except:
  pass"
"tf.config.experimental_connect_to_cluster(
    cluster_spec_or_resolver,
    job_name='localhost',
    task_index=0,
    protocol=None,
    make_master_device_default=True,
    cluster_device_filters=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import experimental_connect_to_cluster
cdf = tf.config.experimental.ClusterDeviceFilters()
for i in range(num_workers):
  cdf.set_device_filters('worker', i, ['/job:ps'])
for i in range(num_ps):
  cdf.set_device_filters('ps', i, ['/job:worker'])

tf.config.experimental_connect_to_cluster(cluster_def,
                                          cluster_device_filters=cdf)
"
"tf.config.experimental_connect_to_host(
    remote_host=None, job_name='worker'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import experimental_connect_to_host
tf.config.experimental_connect_to_host(""exampleaddr.com:9876"")

with ops.device(""job:worker/replica:0/task:1/device:CPU:0""):
  x1 = array_ops.ones([2, 2])
  x2 = array_ops.ones([2, 2])
  y = math_ops.matmul(x1, x2)
"
"tf.config.get_logical_device_configuration(
    device
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get_logical_device_configuration
physical_devices = tf.config.list_physical_devices('CPU')
assert len(physical_devices) == 1, ""No CPUs found""
configs = tf.config.get_logical_device_configuration(
  physical_devices[0])
try:
  assert configs is None
  tf.config.set_logical_device_configuration(
    physical_devices[0],
    [tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration()])
  configs = tf.config.get_logical_device_configuration(
    physical_devices[0])
  assert len(configs) == 2
except:
  pass"
"tf.config.get_visible_devices(
    device_type=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get_visible_devices
physical_devices = tf.config.list_physical_devices('GPU')
try:
  tf.config.set_visible_devices([], 'GPU')
  visible_devices = tf.config.get_visible_devices()
  for device in visible_devices:
    assert device.device_type != 'GPU'
except:
  pass"
"tf.config.list_logical_devices(
    device_type=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import list_logical_devices
logical_devices = tf.config.list_logical_devices('GPU')
if len(logical_devices) > 0:
  with tf.device(logical_devices[0].name):
    one = tf.constant(1)
  with tf.device(logical_devices[1].name):
    two = tf.constant(2)"
"tf.config.list_physical_devices(
    device_type=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import list_physical_devices
physical_devices = tf.config.list_physical_devices('GPU')
print(""Num GPUs:"", len(physical_devices))
Num GPUs: ..."
"tf.config.set_logical_device_configuration(
    device, logical_devices
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import set_logical_device_configuration
physical_devices = tf.config.list_physical_devices('CPU')
assert len(physical_devices) == 1, ""No CPUs found""
try:
  tf.config.set_logical_device_configuration(
    physical_devices[0],
    [tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration()])
  logical_devices = tf.config.list_logical_devices('CPU')
  assert len(logical_devices) == 2
  tf.config.set_logical_device_configuration(
    physical_devices[0],
    [tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration()])
except:
  pass"
"tf.config.set_visible_devices(
    devices, device_type=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import set_visible_devices
physical_devices = tf.config.list_physical_devices('GPU')
try:
  tf.config.set_visible_devices(physical_devices[1:], 'GPU')
  logical_devices = tf.config.list_logical_devices('GPU')
  assert len(logical_devices) == len(physical_devices) - 1
except:
  pass"
"tf.constant(
    value, dtype=None, shape=None, name='Const'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import constant
tf.constant([1, 2, 3, 4, 5, 6])
<tf.Tensor: shape=(6,), dtype=int32,
<tf.Tensor: shape=(6,), dtype=int32,
    numpy=array([1, 2, 3, 4, 5, 6], dtype=int32)>
a = np.array([[1, 2, 3], [4, 5, 6]])
tf.constant(a)
<tf.Tensor: shape=(2, 3), dtype=int64, numpy=
<tf.Tensor: shape=(2, 3), dtype=int64, numpy=
  array([[1, 2, 3],
         [4, 5, 6]])>"
"tf.constant_initializer(
    value=0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import constant_initializer
def make_variables(k, initializer):
  return (tf.Variable(initializer(shape=[k], dtype=tf.float32)),
          tf.Variable(initializer(shape=[k, k], dtype=tf.float32)))
v1, v2 = make_variables(3, tf.constant_initializer(2.))
v1
<tf.Variable ... shape=(3,) ... numpy=array([2., 2., 2.], dtype=float32)>
<tf.Variable ... shape=(3,) ... numpy=array([2., 2., 2.], dtype=float32)>
v2
<tf.Variable ... shape=(3, 3) ... numpy=
<tf.Variable ... shape=(3, 3) ... numpy=
array([[2., 2., 2.],
       [2., 2., 2.],
       [2., 2., 2.]], dtype=float32)>
make_variables(4, tf.random_uniform_initializer(minval=-1., maxval=1.))
(<tf.Variable...shape=(4,) dtype=float32...>, <tf.Variable...shape=(4, 4) ...
(<tf.Variable...shape=(4,) dtype=float32...>, <tf.Variable...shape=(4, 4) ..."
"tf.convert_to_tensor(
    value, dtype=None, dtype_hint=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import convert_to_tensor
import numpy as np
def my_func(arg):
  arg = tf.convert_to_tensor(arg, dtype=tf.float32)
  return arg"
"tf.math.cos(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import cos
  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
  tf.math.cos(x) ==> [nan -0.91113025 0.87758255 0.5403023 0.36235774 0.48718765 -0.95215535 nan]
"
"tf.math.cosh(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import cosh
  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 2, 10, float(""inf"")])
  tf.math.cosh(x) ==> [inf 4.0515420e+03 1.1276259e+00 1.5430807e+00 1.8106556e+00 3.7621956e+00 1.1013233e+04 inf]
"
"tf.math.cumsum(
    x, axis=0, exclusive=False, reverse=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import cumsum
x = tf.constant([2, 4, 6, 8])
tf.cumsum(x)
<tf.Tensor: shape=(4,), dtype=int32,
<tf.Tensor: shape=(4,), dtype=int32,
numpy=array([ 2,  6, 12, 20], dtype=int32)>"
"tf.data.Dataset(
    variant_tensor
)
","import pandas as pd
import numpy as np
import tensorflow as tf
dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
for element in dataset:
  print(element)
tf.Tensor(1, shape=(), dtype=int32)
tf.Tensor(2, shape=(), dtype=int32)
tf.Tensor(3, shape=(), dtype=int32)"
"tf.data.DatasetSpec(
    element_spec, dataset_shape=()
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import DatasetSpec
dataset = tf.data.Dataset.range(3)
tf.data.DatasetSpec.from_value(dataset)
DatasetSpec(TensorSpec(shape=(), dtype=tf.int64, name=None), TensorShape([]))"
"tf.data.IteratorSpec(
    element_spec
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import IteratorSpec
@tf.function(input_signature=[tf.data.IteratorSpec(
  tf.TensorSpec(shape=(), dtype=tf.int32, name=None))])
def square(iterator):
  x = iterator.get_next()
  return x * x
dataset = tf.data.Dataset.from_tensors(5)
iterator = iter(dataset)
print(square(iterator))
tf.Tensor(25, shape=(), dtype=int32)"
"tf.data.experimental.CheckpointInputPipelineHook(
    estimator, external_state_policy=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CheckpointInputPipelineHook
est = tf.estimator.Estimator(model_fn)
while True:
  est.train(
      train_input_fn,
      hooks=[tf.data.experimental.CheckpointInputPipelineHook(est)],
      steps=train_steps_per_eval)
  metrics = est.evaluate(eval_input_fn)
  if should_stop_the_training(metrics):
    break
"
"tf.data.experimental.Counter(
    start=0,
    step=1,
    dtype=tf.dtypes.int64
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Counter
dataset = tf.data.experimental.Counter().take(5)
list(dataset.as_numpy_iterator())
[0, 1, 2, 3, 4]
dataset.element_spec
TensorSpec(shape=(), dtype=tf.int64, name=None)
dataset = tf.data.experimental.Counter(dtype=tf.int32)
dataset.element_spec
TensorSpec(shape=(), dtype=tf.int32, name=None)
dataset = tf.data.experimental.Counter(start=2).take(5)
list(dataset.as_numpy_iterator())
[2, 3, 4, 5, 6]
dataset = tf.data.experimental.Counter(start=2, step=5).take(5)
list(dataset.as_numpy_iterator())
[2, 7, 12, 17, 22]
dataset = tf.data.experimental.Counter(start=10, step=-1).take(5)
list(dataset.as_numpy_iterator())
[10, 9, 8, 7, 6]"
"tf.data.experimental.DatasetInitializer(
    dataset
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import DatasetInitializer
keys = tf.data.Dataset.range(100)
values = tf.data.Dataset.range(100).map(
    lambda x: tf.strings.as_string(x * 2))
ds = tf.data.Dataset.zip((keys, values))
init = tf.data.experimental.DatasetInitializer(ds)
table = tf.lookup.StaticHashTable(init, """")
table.lookup(tf.constant([0, 1, 2], dtype=tf.int64)).numpy()
array([b'0', b'2', b'4'], dtype=object)"
"tf.data.experimental.Reducer(
    init_func, reduce_func, finalize_func
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Reducer
def init_func(_):
  return (0.0, 0.0)

def reduce_func(state, value):
  return (state[0] + value['features'], state[1] + 1)

def finalize_func(s, n):
  return s / n

reducer = tf.data.experimental.Reducer(init_func, reduce_func, finalize_func)
"
"tf.data.experimental.SqlDataset(
    driver_name, data_source_name, query, output_types
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SqlDataset
dataset = tf.data.experimental.SqlDataset(""sqlite"", ""/foo/bar.sqlite3"",
                                          ""SELECT name, age FROM people"",
                                          (tf.string, tf.int32))
for element in dataset:
  print(element)
"
"tf.data.experimental.TFRecordWriter(
    filename, compression_type=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TFRecordWriter
dataset = tf.data.Dataset.range(3)
dataset = dataset.map(tf.io.serialize_tensor)
writer = tf.data.experimental.TFRecordWriter(""/path/to/file.tfrecord"")
writer.write(dataset)
"
"tf.data.experimental.assert_cardinality(
    expected_cardinality
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import assert_cardinality
dataset = tf.data.TFRecordDataset(""examples.tfrecord"")
cardinality = tf.data.experimental.cardinality(dataset)
print((cardinality == tf.data.experimental.UNKNOWN_CARDINALITY).numpy())
True
dataset = dataset.apply(tf.data.experimental.assert_cardinality(42))
print(tf.data.experimental.cardinality(dataset).numpy())
42"
"tf.data.experimental.cardinality(
    dataset
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import cardinality
dataset = tf.data.Dataset.range(42)
print(tf.data.experimental.cardinality(dataset).numpy())
42
dataset = dataset.repeat()
cardinality = tf.data.experimental.cardinality(dataset)
print((cardinality == tf.data.experimental.INFINITE_CARDINALITY).numpy())
True
dataset = dataset.filter(lambda x: True)
cardinality = tf.data.experimental.cardinality(dataset)
print((cardinality == tf.data.experimental.UNKNOWN_CARDINALITY).numpy())
True"
"tf.data.experimental.choose_from_datasets(
    datasets, choice_dataset, stop_on_empty_dataset=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import choose_from_datasets
datasets = [tf.data.Dataset.from_tensors(""foo"").repeat(),
            tf.data.Dataset.from_tensors(""bar"").repeat(),
            tf.data.Dataset.from_tensors(""baz"").repeat()]

choice_dataset = tf.data.Dataset.range(3).repeat(3)

result = tf.data.experimental.choose_from_datasets(datasets, choice_dataset)
"
"tf.data.experimental.dense_to_ragged_batch(
    batch_size,
    drop_remainder=False,
    row_splits_dtype=tf.dtypes.int64
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import dense_to_ragged_batch
dataset = tf.data.Dataset.from_tensor_slices(np.arange(6))
dataset = dataset.map(lambda x: tf.range(x))
dataset.element_spec.shape
TensorShape([None])
dataset = dataset.apply(
    tf.data.experimental.dense_to_ragged_batch(batch_size=2))
for batch in dataset:
  print(batch)
<tf.RaggedTensor [[], [0]]>
<tf.RaggedTensor [[], [0]]>
<tf.RaggedTensor [[0, 1], [0, 1, 2]]>
<tf.RaggedTensor [[0, 1], [0, 1, 2]]>
<tf.RaggedTensor [[0, 1, 2, 3], [0, 1, 2, 3, 4]]>
<tf.RaggedTensor [[0, 1, 2, 3], [0, 1, 2, 3, 4]]>"
"tf.data.experimental.dense_to_sparse_batch(
    batch_size, row_shape
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import dense_to_sparse_batch
a = { ['a', 'b', 'c'], ['a', 'b'], ['a', 'b', 'c', 'd'] }

a.apply(tf.data.experimental.dense_to_sparse_batch(
    batch_size=2, row_shape=[6])) ==
{
    ([[0, 0], [0, 1], [0, 2], [0, 3]],
     ['a', 'b', 'c', 'd'],
     [1, 6])
}
"
"tf.data.experimental.enumerate_dataset(
    start=0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import enumerate_dataset
a = { 1, 2, 3 }
b = { (7, 8), (9, 10) }

a.apply(tf.data.experimental.enumerate_dataset(start=5))
=> { (5, 1), (6, 2), (7, 3) }
b.apply(tf.data.experimental.enumerate_dataset())
=> { (0, (7, 8)), (1, (9, 10)) }
"
"tf.data.experimental.from_list(
    elements, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import from_list
dataset = tf.data.experimental.from_list([(1, 'a'), (2, 'b'), (3, 'c')])
list(dataset.as_numpy_iterator())
[(1, b'a'), (2, b'b'), (3, b'c')]"
"tf.data.experimental.get_single_element(
    dataset
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get_single_element
def preprocessing_fn(raw_feature):
  return feature

dataset = (tf.data.Dataset.from_tensor_slices(raw_features)
           .map(preprocessing_fn, num_parallel_calls=BATCH_SIZE)
           .batch(BATCH_SIZE))

processed_features = tf.data.experimental.get_single_element(dataset)
"
"tf.data.experimental.get_structure(
    dataset_or_iterator
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get_structure
dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
tf.data.experimental.get_structure(dataset)
TensorSpec(shape=(), dtype=tf.int32, name=None)"
"tf.data.experimental.ignore_errors(
    log_warning=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ignore_errors
dataset = tf.data.Dataset.from_tensor_slices([1., 2., 0., 4.])

InvalidArgumentError.
dataset = dataset.map(lambda x: tf.debugging.check_numerics(1. / x, ""error""))

dataset =
"
"tf.data.experimental.index_table_from_dataset(
    dataset=None,
    num_oov_buckets=0,
    vocab_size=None,
    default_value=-1,
    hasher_spec=lookup_ops.FastHashSpec,
    key_dtype=tf.dtypes.string,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import index_table_from_dataset
ds = tf.data.Dataset.range(100).map(lambda x: tf.strings.as_string(x * 2))
table = tf.data.experimental.index_table_from_dataset(
                                    ds, key_dtype=dtypes.int64)
table.lookup(tf.constant(['0', '2', '4'], dtype=tf.string)).numpy()
array([0, 1, 2])"
"tf.data.experimental.load(
    path, element_spec=None, compression=None, reader_func=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import load
import tempfile
path = os.path.join(tempfile.gettempdir(), ""saved_data"")
dataset = tf.data.Dataset.range(2)
tf.data.experimental.save(dataset, path)
new_dataset = tf.data.experimental.load(path)
for elem in new_dataset:
  print(elem)
tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor(1, shape=(), dtype=int64)"
"tf.data.experimental.make_csv_dataset(
    file_pattern,
    batch_size,
    column_names=None,
    column_defaults=None,
    label_name=None,
    select_columns=None,
    field_delim=',',
    use_quote_delim=True,
    na_value='',
    header=True,
    num_epochs=None,
    shuffle=True,
    shuffle_buffer_size=10000,
    shuffle_seed=None,
    prefetch_buffer_size=None,
    num_parallel_reads=None,
    sloppy=False,
    num_rows_for_inference=100,
    compression_type=None,
    ignore_errors=False,
    encoding='utf-8'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import make_csv_dataset
dataset = tf.data.experimental.make_csv_dataset(filename, batch_size=2)
iterator = dataset.as_numpy_iterator()
print(dict(next(iterator)))
"
"tf.data.experimental.make_saveable_from_iterator(
    iterator, external_state_policy=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import make_saveable_from_iterator
with tf.Graph().as_default():
  ds = tf.data.Dataset.range(10)
  iterator = ds.make_initializable_iterator()
  saveable_obj = tf.data.experimental.make_saveable_from_iterator(iterator)
  tf.compat.v1.add_to_collection(tf.GraphKeys.SAVEABLE_OBJECTS, saveable_obj)
  saver = tf.compat.v1.train.Saver()

  while continue_training:
    ... Perform training ...
    if should_save_checkpoint:
      saver.save()
"
"tf.data.experimental.parallel_interleave(
    map_func,
    cycle_length,
    block_length=1,
    sloppy=False,
    buffer_output_elements=None,
    prefetch_input_elements=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import parallel_interleave
filenames = tf.data.Dataset.list_files(""/path/to/data/train*.tfrecords"")
dataset = filenames.apply(
    tf.data.experimental.parallel_interleave(
        lambda filename: tf.data.TFRecordDataset(filename),
        cycle_length=4))
"
"tf.data.experimental.prefetch_to_device(
    device, buffer_size=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import prefetch_to_device
>>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
>>> dataset = dataset.apply(tf.data.experimental.prefetch_to_device(""/cpu:0""))
>>> for element in dataset:
...   print(f'Tensor {element} is on device {element.device}')
Tensor 1 is on device /job:localhost/replica:0/task:0/device:CPU:0
Tensor 2 is on device /job:localhost/replica:0/task:0/device:CPU:0
Tensor 3 is on device /job:localhost/replica:0/task:0/device:CPU:0
"
"tf.data.experimental.save(
    dataset, path, compression=None, shard_func=None, checkpoint_args=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import save
import tempfile
path = os.path.join(tempfile.gettempdir(), ""saved_data"")
dataset = tf.data.Dataset.range(2)
tf.data.experimental.save(dataset, path)
new_dataset = tf.data.experimental.load(path)
for elem in new_dataset:
  print(elem)
tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor(1, shape=(), dtype=int64)"
"tf.data.experimental.service.CrossTrainerCache(
    trainer_id
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CrossTrainerCache
dataset = dataset.apply(tf.data.experimental.service.distribute(
    processing_mode=tf.data.experimental.service.ShardingPolicy.OFF,
    service=FLAGS.tf_data_service_address,
    job_name=""job"",
    cross_trainer_cache=data_service_ops.CrossTrainerCache(
        trainer_id=trainer_id())))
"
"tf.data.experimental.service.DispatchServer(
    config=None, start=True
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import DispatchServer
dispatcher = tf.data.experimental.service.DispatchServer()
dispatcher_address = dispatcher.target.split(""://"")[1]
worker = tf.data.experimental.service.WorkerServer(
    tf.data.experimental.service.WorkerConfig(
    dispatcher_address=dispatcher_address))
dataset = tf.data.Dataset.range(10)
dataset = dataset.apply(tf.data.experimental.service.distribute(
    processing_mode=""parallel_epochs"", service=dispatcher.target))
print(list(dataset.as_numpy_iterator()))
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
"tf.data.experimental.service.WorkerServer(
    config, start=True
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import WorkerServer
dispatcher = tf.data.experimental.service.DispatchServer()
dispatcher_address = dispatcher.target.split(""://"")[1]
worker = tf.data.experimental.service.WorkerServer(
    tf.data.experimental.service.WorkerConfig(
        dispatcher_address=dispatcher_address))
dataset = tf.data.Dataset.range(10)
dataset = dataset.apply(tf.data.experimental.service.distribute(
    processing_mode=""parallel_epochs"", service=dispatcher.target))
print(list(dataset.as_numpy_iterator()))
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
"tf.data.experimental.service.distribute(
    processing_mode,
    service,
    job_name=None,
    consumer_index=None,
    num_consumers=None,
    max_outstanding_requests=None,
    data_transfer_protocol=None,
    compression='AUTO',
    cross_trainer_cache=None,
    target_workers='AUTO'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import distribute
dispatcher = tf.data.experimental.service.DispatchServer()
dispatcher_address = dispatcher.target.split(""://"")[1]
workers = [
    tf.data.experimental.service.WorkerServer(
        tf.data.experimental.service.WorkerConfig(
            dispatcher_address=dispatcher_address)) for _ in range(2)
]
dataset = tf.data.Dataset.range(10)
dataset = dataset.apply(tf.data.experimental.service.distribute(
    processing_mode=""parallel_epochs"", service=dispatcher.target))
print(sorted(list(dataset.as_numpy_iterator())))
[0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9]"
"tf.data.experimental.service.from_dataset_id(
    processing_mode,
    service,
    dataset_id,
    element_spec=None,
    job_name=None,
    consumer_index=None,
    num_consumers=None,
    max_outstanding_requests=None,
    data_transfer_protocol=None,
    cross_trainer_cache=None,
    target_workers='AUTO'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import from_dataset_id
dispatcher = tf.data.experimental.service.DispatchServer()
dispatcher_address = dispatcher.target.split(""://"")[1]
worker = tf.data.experimental.service.WorkerServer(
    tf.data.experimental.service.WorkerConfig(
        dispatcher_address=dispatcher_address))
dataset = tf.data.Dataset.range(10)
dataset_id = tf.data.experimental.service.register_dataset(
    dispatcher.target, dataset)
dataset = tf.data.experimental.service.from_dataset_id(
    processing_mode=""parallel_epochs"",
    service=dispatcher.target,
    dataset_id=dataset_id,
    element_spec=dataset.element_spec)
print(list(dataset.as_numpy_iterator()))
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
"tf.data.experimental.service.register_dataset(
    service, dataset, compression='AUTO', dataset_id=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import register_dataset
dispatcher = tf.data.experimental.service.DispatchServer()
dispatcher_address = dispatcher.target.split(""://"")[1]
worker = tf.data.experimental.service.WorkerServer(
    tf.data.experimental.service.WorkerConfig(
        dispatcher_address=dispatcher_address))
dataset = tf.data.Dataset.range(10)
dataset_id = tf.data.experimental.service.register_dataset(
    dispatcher.target, dataset)
dataset = tf.data.experimental.service.from_dataset_id(
    processing_mode=""parallel_epochs"",
    service=dispatcher.target,
    dataset_id=dataset_id,
    element_spec=dataset.element_spec)
print(list(dataset.as_numpy_iterator()))
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
"tf.data.experimental.shuffle_and_repeat(
    buffer_size, count=None, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import shuffle_and_repeat
d = tf.data.Dataset.from_tensor_slices([1, 2, 3])
d = d.apply(tf.data.experimental.shuffle_and_repeat(2, count=2))
[2, 3, 1, 1, 3, 2]"
"tf.data.experimental.snapshot(
    path, compression='AUTO', reader_func=None, shard_func=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import snapshot
dataset = ...
dataset = dataset.enumerate()
dataset = dataset.apply(tf.data.experimental.snapshot(""/path/to/snapshot/dir"",
    shard_func=lambda x, y: x % NUM_SHARDS, ...))
dataset = dataset.map(lambda x, y: y)
"
"tf.data.experimental.table_from_dataset(
    dataset=None,
    num_oov_buckets=0,
    vocab_size=None,
    default_value=None,
    hasher_spec=lookup_ops.FastHashSpec,
    key_dtype=tf.dtypes.string,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import table_from_dataset
keys = tf.data.Dataset.range(100)
values = tf.data.Dataset.range(100).map(
    lambda x: tf.strings.as_string(x * 2))
ds = tf.data.Dataset.zip((keys, values))
table = tf.data.experimental.table_from_dataset(
                              ds, default_value='n/a', key_dtype=tf.int64)
table.lookup(tf.constant([0, 1, 2], dtype=tf.int64)).numpy()
array([b'0', b'2', b'4'], dtype=object)"
"tf.debugging.Assert(
    condition, data, summarize=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Assert
assert_op = tf.Assert(tf.less_equal(tf.reduce_max(x), 1.), [x])
with tf.control_dependencies([assert_op]):
  ... code using x ...
"
"tf.debugging.assert_shapes(
    shapes, data=None, summarize=None, message=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import assert_shapes
n = 10
q = 3
d = 7
x = tf.zeros([n,q])
y = tf.ones([n,d])
param = tf.Variable([1.0, 2.0, 3.0])
scalar = 1.0
tf.debugging.assert_shapes([
 (x, ('N', 'Q')),
 (y, ('N', 'D')),
 (param, ('Q',)),
 (scalar, ()),
])"
"tf.debugging.assert_type(
    tensor, tf_type, message=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import assert_type
a = tf.Variable(1.0)
tf.debugging.assert_type(a, tf_type= tf.float32)"
"tf.debugging.check_numerics(
    tensor, message, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import check_numerics
a = tf.Variable(1.0)
tf.debugging.check_numerics(a, message='')

b = tf.Variable(np.nan)
try:
  tf.debugging.check_numerics(b, message='Checking b')
except Exception as e:
  assert ""Checking b : Tensor had NaN values"" in e.message

c = tf.Variable(np.inf)
try:
  tf.debugging.check_numerics(c, message='Checking c')
except Exception as e:
  assert ""Checking c : Tensor had Inf values"" in e.message
"
"tf.debugging.enable_check_numerics(
    stack_height_limit=30, path_length_limit=50
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import enable_check_numerics
tf.config.set_soft_device_placement(True)
tf.debugging.enable_check_numerics()

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
strategy = tf.distribute.TPUStrategy(resolver)
with strategy.scope():
"
"tf.debugging.experimental.enable_dump_debug_info(
    dump_root,
    tensor_debug_mode=DEFAULT_TENSOR_DEBUG_MODE,
    circular_buffer_size=1000,
    op_regex=None,
    tensor_dtypes=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import enable_dump_debug_info
tf.debugging.experimental.enable_dump_debug_info('/tmp/my-tfdbg-dumps')

"
"tf.debugging.set_log_device_placement(
    enabled
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import set_log_device_placement
tf.debugging.set_log_device_placement(True)
tf.ones([])
with tf.device(""CPU""):
 tf.ones([])
tf.debugging.set_log_device_placement(False)"
"tf.device(
    device_name
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import device
with tf.device('/job:foo'):
  with tf.device('/job:bar/task:0/device:gpu:2'):
  with tf.device('/device:gpu:1'):
"
"tf.distribute.HierarchicalCopyAllReduce(
    num_packs=1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import HierarchicalCopyAllReduce
  strategy = tf.distribute.MirroredStrategy(
    cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())
"
"tf.distribute.InputOptions(
    experimental_fetch_to_device=None,
    experimental_replication_mode=tf.distribute.InputReplicationMode.PER_WORKER,
    experimental_place_dataset_on_device=False,
    experimental_per_replica_buffer_size=1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import InputOptions
resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.TPUStrategy(resolver)

dataset = tf.data.Dataset.range(16)
distributed_dataset_on_host = (
    strategy.experimental_distribute_dataset(
        dataset,
        tf.distribute.InputOptions(
            experimental_replication_mode=
            experimental_replication_mode.PER_WORKER,
            experimental_place_dataset_on_device=False,
            experimental_per_replica_buffer_size=1)))
"
"tf.distribute.MirroredStrategy(
    devices=None, cross_device_ops=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MirroredStrategy
strategy = tf.distribute.MirroredStrategy([""GPU:0"", ""GPU:1""])
with strategy.scope():
  x = tf.Variable(1.)
x
MirroredVariable:{
  0: <tf.Variable ... shape=() dtype=float32, numpy=1.0>,
  0: <tf.Variable ... shape=() dtype=float32, numpy=1.0>,
  1: <tf.Variable ... shape=() dtype=float32, numpy=1.0>
  1: <tf.Variable ... shape=() dtype=float32, numpy=1.0>
}"
"tf.distribute.NcclAllReduce(
    num_packs=1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import NcclAllReduce
  strategy = tf.distribute.MirroredStrategy(
    cross_device_ops=tf.distribute.NcclAllReduce())
"
"tf.distribute.OneDeviceStrategy(
    device
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import OneDeviceStrategy
strategy = tf.distribute.OneDeviceStrategy(device=""/gpu:0"")

with strategy.scope():
  v = tf.Variable(1.0)

def step_fn(x):
  return x * 2

result = 0
for i in range(10):
  result += strategy.run(step_fn, args=(i,))
"
"tf.distribute.experimental.ParameterServerStrategy(
    cluster_resolver, variable_partitioner=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ParameterServerStrategy
strategy = tf.distribute.experimental.ParameterServerStrategy(
    cluster_resolver=...,
    variable_partitioner=...)
coordinator = tf.distribute.experimental.coordinator.ClusterCoordinator(
    strategy=strategy)

distributed_dataset = coordinator.create_per_worker_dataset(dataset_fn=...)

with strategy.scope():
  model = ...
  checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)
  checkpoint_manager = tf.train.CheckpointManager(
      checkpoint, checkpoint_dir, max_to_keep=2)
  initial_epoch = load_checkpoint(checkpoint_manager) or 0

@tf.function
def worker_fn(iterator):

  def replica_fn(inputs):
    batch_data, labels = inputs

  strategy.run(replica_fn, args=(next(iterator),))

for epoch in range(initial_epoch, num_epoch):
  for step in range(steps_per_epoch):

    coordinator.schedule(worker_fn, args=(distributed_iterator,))

  coordinator.join()
  logging.info('Metric result: %r', metrics.result())
  train_accuracy.reset_states()
  checkpoint_manager.save()
"
"tf.distribute.ReductionToOneDevice(
    reduce_to_device=None, accumulation_fn=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ReductionToOneDevice
  strategy = tf.distribute.MirroredStrategy(
    cross_device_ops=tf.distribute.ReductionToOneDevice())
"
"tf.distribute.Server(
    server_or_cluster_def,
    job_name=None,
    task_index=None,
    protocol=None,
    config=None,
    start=True
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Server
server = tf.distribute.Server(...)
with tf.compat.v1.Session(server.target):
"
"tf.distribute.TPUStrategy(
    tpu_cluster_resolver=None,
    experimental_device_assignment=None,
    experimental_spmd_xla_partitioning=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TPUStrategy
resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.TPUStrategy(resolver)"
"tf.distribute.cluster_resolver.GCEClusterResolver(
    project,
    zone,
    instance_group,
    port,
    task_type='worker',
    task_id=0,
    rpc_layer='grpc',
    credentials='default',
    service=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GCEClusterResolver
  cluster_resolver = GCEClusterResolver(""my-project"", ""us-west1"",
                                        ""my-instance-group"",
                                        task_type=""worker"", task_id=0)
  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(
      cluster_resolver=cluster_resolver)

  cluster_resolver = GCEClusterResolver(""my-project"", ""us-west1"",
                                        ""my-instance-group"",
                                        task_type=""worker"", task_id=1)
  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(
      cluster_resolver=cluster_resolver)
"
"tf.distribute.cluster_resolver.KubernetesClusterResolver(
    job_to_label_mapping=None,
    tf_server_port=8470,
    rpc_layer='grpc',
    override_client=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import KubernetesClusterResolver
  cluster_resolver = KubernetesClusterResolver(
      {""worker"": [""job-name=worker-cluster-a"", ""job-name=worker-cluster-b""]})
  cluster_resolver.task_type = ""worker""
  cluster_resolver.task_id = 0
  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(
      cluster_resolver=cluster_resolver)

  cluster_resolver = KubernetesClusterResolver(
      {""worker"": [""job-name=worker-cluster-a"", ""job-name=worker-cluster-b""]})
  cluster_resolver.task_type = ""worker""
  cluster_resolver.task_id = 1
  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(
      cluster_resolver=cluster_resolver)
"
"tf.distribute.cluster_resolver.SimpleClusterResolver(
    cluster_spec,
    master='',
    task_type=None,
    task_id=None,
    environment='',
    num_accelerators=None,
    rpc_layer=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SimpleClusterResolver
  cluster = tf.train.ClusterSpec({""worker"": [""worker0.example.com:2222"",
                                             ""worker1.example.com:2222""]})

  cluster_resolver = SimpleClusterResolver(cluster, task_type=""worker"",
                                           task_id=0,
                                           num_accelerators={""GPU"": 8},
                                           rpc_layer=""grpc"")
  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(
      cluster_resolver=cluster_resolver)

  cluster_resolver = SimpleClusterResolver(cluster, task_type=""worker"",
                                           task_id=1,
                                           num_accelerators={""GPU"": 8},
                                           rpc_layer=""grpc"")
  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(
      cluster_resolver=cluster_resolver)
"
"tf.distribute.experimental.CentralStorageStrategy(
    compute_devices=None, parameter_device=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CentralStorageStrategy
strategy = tf.distribute.experimental.CentralStorageStrategy()
ds = tf.data.Dataset.range(5).batch(2)
dist_dataset = strategy.experimental_distribute_dataset(ds)

with strategy.scope():
  @tf.function
  def train_step(val):
    return val + 1

  for x in dist_dataset:
    strategy.run(train_step, args=(x,))
"
"tf.distribute.experimental.CollectiveHints(
    bytes_per_pack=0, timeout_seconds=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CollectiveHints
hints = tf.distribute.experimental.CollectiveHints(
    bytes_per_pack=50 * 1024 * 1024)
grads = tf.distribute.get_replica_context().all_reduce(
    'sum', grads, experimental_hints=hints)
optimizer.apply_gradients(zip(grads, vars),
    experimental_aggregate_gradients=False)
"
"tf.distribute.experimental.CommunicationOptions(
    bytes_per_pack=0,
    timeout_seconds=None,
    implementation=tf.distribute.experimental.CollectiveCommunication.AUTO
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CommunicationOptions
options = tf.distribute.experimental.CommunicationOptions(
    bytes_per_pack=50 * 1024 * 1024,
    timeout_seconds=120.0,
    implementation=tf.distribute.experimental.CommunicationImplementation.NCCL
)
grads = tf.distribute.get_replica_context().all_reduce(
    'sum', grads, options=options)
optimizer.apply_gradients(zip(grads, vars),
    experimental_aggregate_gradients=False)
"
"tf.distribute.experimental.ParameterServerStrategy(
    cluster_resolver, variable_partitioner=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ParameterServerStrategy
strategy = tf.distribute.experimental.ParameterServerStrategy(
    cluster_resolver=...,
    variable_partitioner=...)
coordinator = tf.distribute.experimental.coordinator.ClusterCoordinator(
    strategy=strategy)

distributed_dataset = coordinator.create_per_worker_dataset(dataset_fn=...)

with strategy.scope():
  model = ...
  checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)
  checkpoint_manager = tf.train.CheckpointManager(
      checkpoint, checkpoint_dir, max_to_keep=2)
  initial_epoch = load_checkpoint(checkpoint_manager) or 0

@tf.function
def worker_fn(iterator):

  def replica_fn(inputs):
    batch_data, labels = inputs

  strategy.run(replica_fn, args=(next(iterator),))

for epoch in range(initial_epoch, num_epoch):
  for step in range(steps_per_epoch):

    coordinator.schedule(worker_fn, args=(distributed_iterator,))

  coordinator.join()
  logging.info('Metric result: %r', metrics.result())
  train_accuracy.reset_states()
  checkpoint_manager.save()
"
"tf.distribute.experimental.PreemptionCheckpointHandler(
    cluster_resolver,
    checkpoint_or_checkpoint_manager,
    checkpoint_dir=None,
    termination_config=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import PreemptionCheckpointHandler
strategy = tf.distribute.MultiWorkerMirroredStrategy()

with strategy.scope():
  dataset, model, optimizer = ...

  checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)

  preemption_handler = tf.distribute.experimental.PreemptionCheckpointHandler(cluster_resolver, checkpoint, checkpoint_directory)

  for epoch in range(preemption_handler.total_run_calls // STEPS_PER_EPOCH, num_epochs):
    for step in range(preemption_handler.total_run_calls % STEPS_PER_EPOCH, STEPS_PER_EPOCH):
      loss += preemption_handler.run(distributed_train_step, args=(next(dataset),))
"
"tf.distribute.experimental.TPUStrategy(
    tpu_cluster_resolver=None, device_assignment=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TPUStrategy
resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.experimental.TPUStrategy(resolver)"
"tf.distribute.experimental.partitioners.FixedShardsPartitioner(
    num_shards
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import FixedShardsPartitioner
partitioner = FixedShardsPartitioner(num_shards=2)
partitions = partitioner(tf.TensorShape([10, 3]), tf.float32)
[2, 1]"
"tf.distribute.experimental.partitioners.MaxSizePartitioner(
    max_shard_bytes, max_shards=None, bytes_per_string=16
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MaxSizePartitioner
partitioner = MaxSizePartitioner(max_shard_bytes=4)
partitions = partitioner(tf.TensorShape([6, 1]), tf.float32)
[6, 1]
partitioner = MaxSizePartitioner(max_shard_bytes=4, max_shards=2)
partitions = partitioner(tf.TensorShape([6, 1]), tf.float32)
[2, 1]
partitioner = MaxSizePartitioner(max_shard_bytes=1024)
partitions = partitioner(tf.TensorShape([6, 1]), tf.float32)
[1, 1]"
"tf.distribute.experimental.partitioners.MinSizePartitioner(
    min_shard_bytes=(256 << 10), max_shards=1, bytes_per_string=16
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MinSizePartitioner
partitioner = MinSizePartitioner(min_shard_bytes=4, max_shards=2)
partitions = partitioner(tf.TensorShape([6, 1]), tf.float32)
[2, 1]
partitioner = MinSizePartitioner(min_shard_bytes=4, max_shards=10)
partitions = partitioner(tf.TensorShape([6, 1]), tf.float32)
[6, 1]"
"tf.distribute.experimental_set_strategy(
    strategy
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import experimental_set_strategy
tf.distribute.experimental_set_strategy(strategy1)
f()
tf.distribute.experimental_set_strategy(strategy2)
g()
tf.distribute.experimental_set_strategy(None)
h()
"
"tf.math.divide(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import divide
x = tf.constant([16, 12, 11])
y = tf.constant([4, 6, 2])
tf.divide(x,y)
<tf.Tensor: shape=(3,), dtype=float64,
<tf.Tensor: shape=(3,), dtype=float64,
numpy=array([4. , 2. , 5.5])>"
"tf.cast(
    x, dtype, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import cast
x = tf.constant([1.8, 2.2], dtype=tf.float32)
tf.cast(x, tf.int32)
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>"
"tf.dtypes.complex(
    real, imag, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import complex
real = tf.constant([2.25, 3.25])
imag = tf.constant([4.75, 5.75])
"
"tf.edit_distance(
    hypothesis, truth, normalize=True, name='edit_distance'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import edit_distance
hypothesis = tf.SparseTensor(
  [[0, 0, 0],
   [1, 0, 0]],
  [""a"", ""b""],
  (2, 1, 1))
truth = tf.SparseTensor(
  [[0, 1, 0],
   [1, 0, 0],
   [1, 0, 1],
   [1, 1, 0]],
   [""a"", ""b"", ""c"", ""a""],
   (2, 2, 2))
tf.edit_distance(hypothesis, truth, normalize=True)
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[inf, 1. ],
       [0.5, 1. ]], dtype=float32)>"
"tf.ensure_shape(
    x, shape, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ensure_shape
x = tf.constant([[1, 2, 3],
                 [4, 5, 6]])
x = tf.ensure_shape(x, [2, 3])"
"tf.math.equal(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import equal
x = tf.constant([2, 4])
y = tf.constant(2)
tf.math.equal(x, y)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  False])>
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  False])>"
"tf.errors.InvalidArgumentError(
    node_def, op, message, *args
)
","import pandas as pd
import numpy as np
import tensorflow as tf
tf.reshape([1, 2, 3], (2,))
Traceback (most recent call last):
InvalidArgumentError: ..."
"tf.estimator.BaselineClassifier(
    model_dir=None,
    n_classes=2,
    weight_column=None,
    label_vocabulary=None,
    optimizer='Ftrl',
    config=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import BaselineClassifier

classifier = tf.estimator.BaselineClassifier(n_classes=3)

def input_fn_train:
  pass

def input_fn_eval:
  pass

classifier.train(input_fn=input_fn_train)

loss = classifier.evaluate(input_fn=input_fn_eval)[""loss""]

predictions = classifier.predict(new_samples)

"
"tf.estimator.BaselineEstimator(
    head, model_dir=None, optimizer='Ftrl', config=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import BaselineEstimator

estimator = tf.estimator.BaselineEstimator(
    head=tf.estimator.MultiLabelHead(n_classes=3))

def input_fn_train:
  pass

def input_fn_eval:
  pass

estimator.train(input_fn=input_fn_train)

loss = estimator.evaluate(input_fn=input_fn_eval)[""loss""]

predictions = estimator.predict(new_samples)

"
"tf.estimator.BaselineRegressor(
    model_dir=None,
    label_dimension=1,
    weight_column=None,
    optimizer='Ftrl',
    config=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import BaselineRegressor

regressor = tf.estimator.BaselineRegressor()

def input_fn_train:
  pass

def input_fn_eval:
  pass

regressor.train(input_fn=input_fn_train)

loss = regressor.evaluate(input_fn=input_fn_eval)[""loss""]

predictions = regressor.predict(new_samples)
"
"tf.estimator.BinaryClassHead(
    weight_column=None,
    thresholds=None,
    label_vocabulary=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,
    loss_fn=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import BinaryClassHead
head = tf.estimator.BinaryClassHead()
logits = np.array(((45,), (-41,),), dtype=np.float32)
labels = np.array(((1,), (1,),), dtype=np.int32)
features = {'x': np.array(((42,),), dtype=np.float32)}
loss = head.loss(labels, logits, features=features)
print('{:.2f}'.format(loss.numpy()))
20.50
eval_metrics = head.metrics()
updated_metrics = head.update_metrics(
  eval_metrics, features, logits, labels)
for k in sorted(updated_metrics):
 print('{} : {:.2f}'.format(k, updated_metrics[k].result().numpy()))
  accuracy : 0.50
  accuracy_baseline : 1.00
  auc : 0.00
  auc_precision_recall : 1.00
  average_loss : 20.50
  label/mean : 1.00
  precision : 1.00
  prediction/mean : 0.50
  recall : 0.50
preds = head.predictions(logits)
print(preds['logits'])
tf.Tensor(
  [[ 45.]
   [-41.]], shape=(2, 1), dtype=float32)"
"tf.estimator.DNNClassifier(
    hidden_units,
    feature_columns,
    model_dir=None,
    n_classes=2,
    weight_column=None,
    label_vocabulary=None,
    optimizer='Adagrad',
    activation_fn=tf.nn.relu,
    dropout=None,
    config=None,
    warm_start_from=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,
    batch_norm=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import DNNClassifier
categorical_feature_a = categorical_column_with_hash_bucket(...)
categorical_feature_b = categorical_column_with_hash_bucket(...)

categorical_feature_a_emb = embedding_column(
    categorical_column=categorical_feature_a, ...)
categorical_feature_b_emb = embedding_column(
    categorical_column=categorical_feature_b, ...)

estimator = tf.estimator.DNNClassifier(
    feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],
    hidden_units=[1024, 512, 256])

estimator = tf.estimator.DNNClassifier(
    feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],
    hidden_units=[1024, 512, 256],
    optimizer=tf.compat.v1.train.ProximalAdagradOptimizer(
      learning_rate=0.1,
      l1_regularization_strength=0.001
    ))

estimator = tf.estimator.DNNClassifier(
    feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],
    hidden_units=[1024, 512, 256],
    optimizer=lambda: tf.keras.optimizers.Adam(
        learning_rate=tf.compat.v1.train.exponential_decay(
            learning_rate=0.1,
            global_step=tf.compat.v1.train.get_global_step(),
            decay_steps=10000,
            decay_rate=0.96))

estimator = tf.estimator.DNNClassifier(
    feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],
    hidden_units=[1024, 512, 256],
    warm_start_from=""/path/to/checkpoint/dir"")

def input_fn_train:
  pass
def input_fn_eval:
  pass
def input_fn_predict:
  pass
estimator.train(input_fn=input_fn_train)
metrics = estimator.evaluate(input_fn=input_fn_eval)
predictions = estimator.predict(input_fn=input_fn_predict)
"
"tf.estimator.DNNEstimator(
    head,
    hidden_units,
    feature_columns,
    model_dir=None,
    optimizer='Adagrad',
    activation_fn=tf.nn.relu,
    dropout=None,
    config=None,
    warm_start_from=None,
    batch_norm=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import DNNEstimator
sparse_feature_a = sparse_column_with_hash_bucket(...)
sparse_feature_b = sparse_column_with_hash_bucket(...)

sparse_feature_a_emb = embedding_column(sparse_id_column=sparse_feature_a,
                                        ...)
sparse_feature_b_emb = embedding_column(sparse_id_column=sparse_feature_b,
                                        ...)

estimator = tf.estimator.DNNEstimator(
    head=tf.estimator.MultiLabelHead(n_classes=3),
    feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],
    hidden_units=[1024, 512, 256])

estimator = tf.estimator.DNNEstimator(
    head=tf.estimator.MultiLabelHead(n_classes=3),
    feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],
    hidden_units=[1024, 512, 256],
    optimizer=tf.compat.v1.train.ProximalAdagradOptimizer(
      learning_rate=0.1,
      l1_regularization_strength=0.001
    ))

estimator = tf.estimator.DNNEstimator(
    head=tf.estimator.MultiLabelHead(n_classes=3),
    feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],
    hidden_units=[1024, 512, 256],
    optimizer=lambda: tf.keras.optimizers.Adam(
        learning_rate=tf.compat.v1.train.exponential_decay(
            learning_rate=0.1,
            global_step=tf.compat.v1.train.get_global_step(),
            decay_steps=10000,
            decay_rate=0.96))

estimator = tf.estimator.DNNEstimator(
    head=tf.estimator.MultiLabelHead(n_classes=3),
    feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],
    hidden_units=[1024, 512, 256],
    warm_start_from=""/path/to/checkpoint/dir"")

def input_fn_train:
  pass
def input_fn_eval:
  pass
def input_fn_predict:
  pass
estimator.train(input_fn=input_fn_train)
metrics = estimator.evaluate(input_fn=input_fn_eval)
predictions = estimator.predict(input_fn=input_fn_predict)
"
"tf.estimator.DNNLinearCombinedClassifier(
    model_dir=None,
    linear_feature_columns=None,
    linear_optimizer='Ftrl',
    dnn_feature_columns=None,
    dnn_optimizer='Adagrad',
    dnn_hidden_units=None,
    dnn_activation_fn=tf.nn.relu,
    dnn_dropout=None,
    n_classes=2,
    weight_column=None,
    label_vocabulary=None,
    config=None,
    warm_start_from=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,
    batch_norm=False,
    linear_sparse_combiner='sum'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import DNNLinearCombinedClassifier
numeric_feature = numeric_column(...)
categorical_column_a = categorical_column_with_hash_bucket(...)
categorical_column_b = categorical_column_with_hash_bucket(...)

categorical_feature_a_x_categorical_feature_b = crossed_column(...)
categorical_feature_a_emb = embedding_column(
    categorical_column=categorical_feature_a, ...)
categorical_feature_b_emb = embedding_column(
    categorical_id_column=categorical_feature_b, ...)

estimator = tf.estimator.DNNLinearCombinedClassifier(
    linear_feature_columns=[categorical_feature_a_x_categorical_feature_b],
    linear_optimizer=tf.keras.optimizers.Ftrl(...),
    dnn_feature_columns=[
        categorical_feature_a_emb, categorical_feature_b_emb,
        numeric_feature],
    dnn_hidden_units=[1000, 500, 100],
    dnn_optimizer=tf.keras.optimizers.Adagrad(...),
    warm_start_from=""/path/to/checkpoint/dir"")

tf.compat.v1.train.ProximalAdagradOptimizer(
    learning_rate=0.1,
    l1_regularization_strength=0.001,
    l2_regularization_strength=0.001)
lambda: tf.keras.optimizers.Adam(
    learning_rate=tf.compat.v1.train.exponential_decay(
        learning_rate=0.1,
        global_step=tf.compat.v1.train.get_global_step(),
        decay_steps=10000,
        decay_rate=0.96)

def input_fn_train:
  pass
def input_fn_eval:
  pass
def input_fn_predict:
  pass
estimator.train(input_fn=input_fn_train, steps=100)
metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)
predictions = estimator.predict(input_fn=input_fn_predict)
"
"tf.estimator.DNNLinearCombinedEstimator(
    head,
    model_dir=None,
    linear_feature_columns=None,
    linear_optimizer='Ftrl',
    dnn_feature_columns=None,
    dnn_optimizer='Adagrad',
    dnn_hidden_units=None,
    dnn_activation_fn=tf.nn.relu,
    dnn_dropout=None,
    config=None,
    batch_norm=False,
    linear_sparse_combiner='sum'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import DNNLinearCombinedEstimator
numeric_feature = numeric_column(...)
categorical_column_a = categorical_column_with_hash_bucket(...)
categorical_column_b = categorical_column_with_hash_bucket(...)

categorical_feature_a_x_categorical_feature_b = crossed_column(...)
categorical_feature_a_emb = embedding_column(
    categorical_column=categorical_feature_a, ...)
categorical_feature_b_emb = embedding_column(
    categorical_column=categorical_feature_b, ...)

estimator = tf.estimator.DNNLinearCombinedEstimator(
    head=tf.estimator.MultiLabelHead(n_classes=3),
    linear_feature_columns=[categorical_feature_a_x_categorical_feature_b],
    linear_optimizer=tf.keras.optimizers.Ftrl(...),
    dnn_feature_columns=[
        categorical_feature_a_emb, categorical_feature_b_emb,
        numeric_feature],
    dnn_hidden_units=[1000, 500, 100],
    dnn_optimizer=tf.keras.optimizers.Adagrad(...))

tf.compat.v1.train.ProximalAdagradOptimizer(
    learning_rate=0.1,
    l1_regularization_strength=0.001,
    l2_regularization_strength=0.001)
lambda: tf.keras.optimizers.Adam(
    learning_rate=tf.compat.v1.train.exponential_decay(
        learning_rate=0.1,
        global_step=tf.compat.v1.train.get_global_step(),
        decay_steps=10000,
        decay_rate=0.96)

def input_fn_train:
  pass
def input_fn_eval:
  pass
def input_fn_predict:
  pass
estimator.train(input_fn=input_fn_train, steps=100)
metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)
predictions = estimator.predict(input_fn=input_fn_predict)
"
"tf.estimator.DNNLinearCombinedRegressor(
    model_dir=None,
    linear_feature_columns=None,
    linear_optimizer='Ftrl',
    dnn_feature_columns=None,
    dnn_optimizer='Adagrad',
    dnn_hidden_units=None,
    dnn_activation_fn=tf.nn.relu,
    dnn_dropout=None,
    label_dimension=1,
    weight_column=None,
    config=None,
    warm_start_from=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,
    batch_norm=False,
    linear_sparse_combiner='sum'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import DNNLinearCombinedRegressor
numeric_feature = numeric_column(...)
categorical_column_a = categorical_column_with_hash_bucket(...)
categorical_column_b = categorical_column_with_hash_bucket(...)

categorical_feature_a_x_categorical_feature_b = crossed_column(...)
categorical_feature_a_emb = embedding_column(
    categorical_column=categorical_feature_a, ...)
categorical_feature_b_emb = embedding_column(
    categorical_column=categorical_feature_b, ...)

estimator = tf.estimator.DNNLinearCombinedRegressor(
    linear_feature_columns=[categorical_feature_a_x_categorical_feature_b],
    linear_optimizer=tf.keras.optimizers.Ftrl(...),
    dnn_feature_columns=[
        categorical_feature_a_emb, categorical_feature_b_emb,
        numeric_feature],
    dnn_hidden_units=[1000, 500, 100],
    dnn_optimizer=tf.keras.optimizers.Adagrad(...),
    warm_start_from=""/path/to/checkpoint/dir"")

tf.compat.v1.train.ProximalAdagradOptimizer(
    learning_rate=0.1,
    l1_regularization_strength=0.001,
    l2_regularization_strength=0.001)
lambda: tf.keras.optimizers.Adam(
    learning_rate=tf.compat.v1.train.exponential_decay(
        learning_rate=0.1,
        global_step=tf.compat.v1.train.get_global_step(),
        decay_steps=10000,
        decay_rate=0.96)

def input_fn_train:
  pass
def input_fn_eval:
  pass
def input_fn_predict:
  pass
estimator.train(input_fn=input_fn_train, steps=100)
metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)
predictions = estimator.predict(input_fn=input_fn_predict)
"
"tf.estimator.DNNRegressor(
    hidden_units,
    feature_columns,
    model_dir=None,
    label_dimension=1,
    weight_column=None,
    optimizer='Adagrad',
    activation_fn=tf.nn.relu,
    dropout=None,
    config=None,
    warm_start_from=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,
    batch_norm=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import DNNRegressor
categorical_feature_a = categorical_column_with_hash_bucket(...)
categorical_feature_b = categorical_column_with_hash_bucket(...)

categorical_feature_a_emb = embedding_column(
    categorical_column=categorical_feature_a, ...)
categorical_feature_b_emb = embedding_column(
    categorical_column=categorical_feature_b, ...)

estimator = tf.estimator.DNNRegressor(
    feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],
    hidden_units=[1024, 512, 256])

estimator = tf.estimator.DNNRegressor(
    feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],
    hidden_units=[1024, 512, 256],
    optimizer=tf.compat.v1.train.ProximalAdagradOptimizer(
      learning_rate=0.1,
      l1_regularization_strength=0.001
    ))

estimator = tf.estimator.DNNRegressor(
    feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],
    hidden_units=[1024, 512, 256],
    optimizer=lambda: tf.keras.optimizers.Adam(
        learning_rate=tf.compat.v1.train.exponential_decay(
            learning_rate=0.1,
            global_step=tf.compat.v1.train.get_global_step(),
            decay_steps=10000,
            decay_rate=0.96))

estimator = tf.estimator.DNNRegressor(
    feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],
    hidden_units=[1024, 512, 256],
    warm_start_from=""/path/to/checkpoint/dir"")

def input_fn_train:
  pass
def input_fn_eval:
  pass
def input_fn_predict:
  pass
estimator.train(input_fn=input_fn_train)
metrics = estimator.evaluate(input_fn=input_fn_eval)
predictions = estimator.predict(input_fn=input_fn_predict)
"
"tf.estimator.LinearClassifier(
    feature_columns,
    model_dir=None,
    n_classes=2,
    weight_column=None,
    label_vocabulary=None,
    optimizer='Ftrl',
    config=None,
    warm_start_from=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,
    sparse_combiner='sum'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearClassifier
categorical_column_a = categorical_column_with_hash_bucket(...)
categorical_column_b = categorical_column_with_hash_bucket(...)

categorical_feature_a_x_categorical_feature_b = crossed_column(...)

estimator = tf.estimator.LinearClassifier(
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b])

estimator = tf.estimator.LinearClassifier(
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b],
    optimizer=tf.keras.optimizers.Ftrl(
      learning_rate=0.1,
      l1_regularization_strength=0.001
    ))

estimator = tf.estimator.LinearClassifier(
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b],
    optimizer=lambda: tf.keras.optimizers.Ftrl(
        learning_rate=tf.exponential_decay(
            learning_rate=0.1,
            global_step=tf.get_global_step(),
            decay_steps=10000,
            decay_rate=0.96))

estimator = tf.estimator.LinearClassifier(
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b],
    warm_start_from=""/path/to/checkpoint/dir"")


def input_fn_train:
  pass
def input_fn_eval:
  pass
def input_fn_predict:
  pass
estimator.train(input_fn=input_fn_train)
metrics = estimator.evaluate(input_fn=input_fn_eval)
predictions = estimator.predict(input_fn=input_fn_predict)
"
"tf.estimator.LinearEstimator(
    head,
    feature_columns,
    model_dir=None,
    optimizer='Ftrl',
    config=None,
    sparse_combiner='sum',
    warm_start_from=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearEstimator
categorical_column_a = categorical_column_with_hash_bucket(...)
categorical_column_b = categorical_column_with_hash_bucket(...)

categorical_feature_a_x_categorical_feature_b = crossed_column(...)

estimator = tf.estimator.LinearEstimator(
    head=tf.estimator.MultiLabelHead(n_classes=3),
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b])

estimator = tf.estimator.LinearEstimator(
    head=tf.estimator.MultiLabelHead(n_classes=3),
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b],
    optimizer=lambda: tf.keras.optimizers.Ftrl(
        learning_rate=tf.compat.v1.train.exponential_decay(
            learning_rate=0.1,
            global_step=tf.compat.v1.train.get_global_step(),
            decay_steps=10000,
            decay_rate=0.96))

estimator = tf.estimator.LinearEstimator(
    head=tf.estimator.MultiLabelHead(n_classes=3),
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b])
    optimizer=tf.keras.optimizers.Ftrl(
        learning_rate=0.1,
        l1_regularization_strength=0.001
    ))

def input_fn_train:
  pass
def input_fn_eval:
  pass
def input_fn_predict:
  pass
estimator.train(input_fn=input_fn_train, steps=100)
metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)
predictions = estimator.predict(input_fn=input_fn_predict)
"
"tf.estimator.LinearRegressor(
    feature_columns,
    model_dir=None,
    label_dimension=1,
    weight_column=None,
    optimizer='Ftrl',
    config=None,
    warm_start_from=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,
    sparse_combiner='sum'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearRegressor
categorical_column_a = categorical_column_with_hash_bucket(...)
categorical_column_b = categorical_column_with_hash_bucket(...)

categorical_feature_a_x_categorical_feature_b = crossed_column(...)

estimator = tf.estimator.LinearRegressor(
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b])

estimator = tf.estimator.LinearRegressor(
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b],
    optimizer=tf.keras.optimizers.Ftrl(
      learning_rate=0.1,
      l1_regularization_strength=0.001
    ))

estimator = tf.estimator.LinearRegressor(
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b],
    optimizer=lambda: tf.keras.optimizers.Ftrl(
        learning_rate=tf.compat.v1.train.exponential_decay(
            learning_rate=0.1,
            global_step=tf.compat.v1.train.get_global_step(),
            decay_steps=10000,
            decay_rate=0.96))

estimator = tf.estimator.LinearRegressor(
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b],
    warm_start_from=""/path/to/checkpoint/dir"")


def input_fn_train:
  pass
def input_fn_eval:
  pass
def input_fn_predict:
  pass
estimator.train(input_fn=input_fn_train)
metrics = estimator.evaluate(input_fn=input_fn_eval)
predictions = estimator.predict(input_fn=input_fn_predict)
"
"tf.estimator.LogisticRegressionHead(
    weight_column=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LogisticRegressionHead
my_head = tf.estimator.LogisticRegressionHead()
my_estimator = tf.estimator.DNNEstimator(
    head=my_head,
    hidden_units=...,
    feature_columns=...)
"
"tf.estimator.MultiClassHead(
    n_classes,
    weight_column=None,
    label_vocabulary=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,
    loss_fn=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MultiClassHead
n_classes = 3
head = tf.estimator.MultiClassHead(n_classes)
logits = np.array(((10, 0, 0), (0, 10, 0),), dtype=np.float32)
labels = np.array(((1,), (1,)), dtype=np.int64)
features = {'x': np.array(((42,),), dtype=np.int32)}
loss = head.loss(labels, logits, features=features)
print('{:.2f}'.format(loss.numpy()))
5.00
eval_metrics = head.metrics()
updated_metrics = head.update_metrics(
  eval_metrics, features, logits, labels)
for k in sorted(updated_metrics):
  print('{} : {:.2f}'.format(k, updated_metrics[k].result().numpy()))
accuracy : 0.50
average_loss : 5.00
preds = head.predictions(logits)
print(preds['logits'])
tf.Tensor(
  [[10.  0.  0.]
   [ 0. 10.  0.]], shape=(2, 3), dtype=float32)"
"tf.estimator.MultiHead(
    heads, head_weights=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MultiHead
head1 = tf.estimator.MultiLabelHead(n_classes=2, name='head1')
head2 = tf.estimator.MultiLabelHead(n_classes=3, name='head2')
multi_head = tf.estimator.MultiHead([head1, head2])
logits = {
   'head1': np.array([[-10., 10.], [-15., 10.]], dtype=np.float32),
   'head2': np.array([[20., -20., 20.], [-30., 20., -20.]],
   dtype=np.float32),}
labels = {
   'head1': np.array([[1, 0], [1, 1]], dtype=np.int64),
   'head2': np.array([[0, 1, 0], [1, 1, 0]], dtype=np.int64),}
features = {'x': np.array(((42,),), dtype=np.float32)}
# loss = labels * (logits < 0) * (-logits) +
loss = multi_head.loss(labels, logits, features=features)
print('{:.2f}'.format(loss.numpy()))
23.75
eval_metrics = multi_head.metrics()
updated_metrics = multi_head.update_metrics(
  eval_metrics, features, logits, labels)
for k in sorted(updated_metrics):
 print('{} : {:.2f}'.format(k, updated_metrics[k].result().numpy()))
auc/head1 : 0.17
auc/head2 : 0.33
auc_precision_recall/head1 : 0.60
auc_precision_recall/head2 : 0.40
average_loss/head1 : 8.75
average_loss/head2 : 15.00
loss/head1 : 8.75
loss/head2 : 15.00
preds = multi_head.predictions(logits)
print(preds[('head1', 'logits')])
tf.Tensor(
  [[-10.  10.]
   [-15.  10.]], shape=(2, 2), dtype=float32)"
"tf.estimator.MultiLabelHead(
    n_classes,
    weight_column=None,
    thresholds=None,
    label_vocabulary=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,
    loss_fn=None,
    classes_for_class_based_metrics=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MultiLabelHead
n_classes = 2
head = tf.estimator.MultiLabelHead(n_classes)
logits = np.array([[-1., 1.], [-1.5, 1.5]], dtype=np.float32)
labels = np.array([[1, 0], [1, 1]], dtype=np.int64)
features = {'x': np.array([[41], [42]], dtype=np.int32)}
loss = head.loss(labels, logits, features=features)
print('{:.2f}'.format(loss.numpy()))
1.13
eval_metrics = head.metrics()
updated_metrics = head.update_metrics(
  eval_metrics, features, logits, labels)
for k in sorted(updated_metrics):
 print('{} : {:.2f}'.format(k, updated_metrics[k].result().numpy()))
auc : 0.33
auc_precision_recall : 0.77
average_loss : 1.13
preds = head.predictions(logits)
print(preds['logits'])
tf.Tensor(
  [[-1.   1. ]
   [-1.5  1.5]], shape=(2, 2), dtype=float32)"
"tf.estimator.PoissonRegressionHead(
    label_dimension=1,
    weight_column=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,
    compute_full_loss=True,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import PoissonRegressionHead
my_head = tf.estimator.PoissonRegressionHead()
my_estimator = tf.estimator.DNNEstimator(
    head=my_head,
    hidden_units=...,
    feature_columns=...)
"
"tf.estimator.RegressionHead(
    label_dimension=1,
    weight_column=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,
    loss_fn=None,
    inverse_link_fn=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RegressionHead
head = tf.estimator.RegressionHead()
logits = np.array(((45,), (41,),), dtype=np.float32)
labels = np.array(((43,), (44,),), dtype=np.int32)
features = {'x': np.array(((42,),), dtype=np.float32)}
loss = head.loss(labels, logits, features=features)
print('{:.2f}'.format(loss.numpy()))
6.50
eval_metrics = head.metrics()
updated_metrics = head.update_metrics(
  eval_metrics, features, logits, labels)
for k in sorted(updated_metrics):
 print('{} : {:.2f}'.format(k, updated_metrics[k].result().numpy()))
  average_loss : 6.50
  label/mean : 43.50
  prediction/mean : 43.00
preds = head.predictions(logits)
print(preds['predictions'])
tf.Tensor(
  [[45.]
   [41.]], shape=(2, 1), dtype=float32)"
"tf.estimator.TrainSpec(
    input_fn, max_steps=None, hooks=None, saving_listeners=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TrainSpec
train_spec = tf.estimator.TrainSpec(
   input_fn=lambda: 1,
   max_steps=100,
   hooks=[_StopAtSecsHook(stop_after_secs=10)],
   saving_listeners=[_NewCheckpointListenerForEvaluate(None, 20, None)])
train_spec.saving_listeners[0]._eval_throttle_secs
20
train_spec.hooks[0]._stop_after_secs
10
train_spec.max_steps
100"
"tf.estimator.add_metrics(
    estimator, metric_fn
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import add_metrics
  def my_auc(labels, predictions):
    auc_metric = tf.keras.metrics.AUC(name=""my_auc"")
    auc_metric.update_state(y_true=labels, y_pred=predictions['logistic'])
    return {'auc': auc_metric}

  estimator = tf.estimator.DNNClassifier(...)
  estimator = tf.estimator.add_metrics(estimator, my_auc)
  estimator.train(...)
  estimator.evaluate(...)
"
"tf.estimator.classifier_parse_example_spec(
    feature_columns,
    label_key,
    label_dtype=tf.dtypes.int64,
    label_default=None,
    weight_column=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import classifier_parse_example_spec
feature_b = tf.feature_column.numeric_column(...)
feature_c_bucketized = tf.feature_column.bucketized_column(
  tf.feature_column.numeric_column(""feature_c""), ...)
feature_a_x_feature_c = tf.feature_column.crossed_column(
    columns=[""feature_a"", feature_c_bucketized], ...)

feature_columns = [feature_b, feature_c_bucketized, feature_a_x_feature_c]
parsing_spec = tf.estimator.classifier_parse_example_spec(
    feature_columns, label_key='my-label', label_dtype=tf.string)

assert parsing_spec == {
  ""feature_a"": parsing_ops.VarLenFeature(tf.string),
  ""feature_b"": parsing_ops.FixedLenFeature([1], dtype=tf.float32),
  ""feature_c"": parsing_ops.FixedLenFeature([1], dtype=tf.float32)
  ""my-label"" : parsing_ops.FixedLenFeature([1], dtype=tf.string)
}
"
"tf.estimator.experimental.InMemoryEvaluatorHook(
    estimator, input_fn, steps=None, hooks=None, name=None, every_n_iter=100
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import InMemoryEvaluatorHook
def train_input_fn():
  ...
  return train_dataset

def eval_input_fn():
  ...
  return eval_dataset

estimator = tf.estimator.DNNClassifier(...)

evaluator = tf.estimator.experimental.InMemoryEvaluatorHook(
    estimator, eval_input_fn)
estimator.train(train_input_fn, hooks=[evaluator])
"
"tf.estimator.experimental.LinearSDCA(
    example_id_column,
    num_loss_partitions=1,
    num_table_shards=None,
    symmetric_l1_regularization=0.0,
    symmetric_l2_regularization=1.0,
    adaptive=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearSDCA
real_feature_column = numeric_column(...)
sparse_feature_column = categorical_column_with_hash_bucket(...)
linear_sdca = tf.estimator.experimental.LinearSDCA(
    example_id_column='example_id',
    num_loss_partitions=1,
    num_table_shards=1,
    symmetric_l2_regularization=2.0)
classifier = tf.estimator.LinearClassifier(
    feature_columns=[real_feature_column, sparse_feature_column],
    weight_column=...,
    optimizer=linear_sdca)
classifier.train(input_fn_train, steps=50)
classifier.evaluate(input_fn=input_fn_eval)
"
"tf.estimator.experimental.RNNClassifier(
    sequence_feature_columns,
    context_feature_columns=None,
    units=None,
    cell_type=USE_DEFAULT,
    rnn_cell_fn=None,
    return_sequences=False,
    model_dir=None,
    n_classes=2,
    weight_column=None,
    label_vocabulary=None,
    optimizer='Adagrad',
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,
    sequence_mask='sequence_mask',
    config=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RNNClassifier
token_sequence = sequence_categorical_column_with_hash_bucket(...)
token_emb = embedding_column(categorical_column=token_sequence, ...)

estimator = RNNClassifier(
    sequence_feature_columns=[token_emb],
    units=[32, 16], cell_type='lstm')

  pass
estimator.train(input_fn=input_fn_train, steps=100)

  pass
metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)
  pass
predictions = estimator.predict(input_fn=input_fn_predict)
"
"tf.estimator.experimental.RNNEstimator(
    head,
    sequence_feature_columns,
    context_feature_columns=None,
    units=None,
    cell_type=USE_DEFAULT,
    rnn_cell_fn=None,
    return_sequences=False,
    model_dir=None,
    optimizer='Adagrad',
    config=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RNNEstimator
token_sequence = sequence_categorical_column_with_hash_bucket(...)
token_emb = embedding_column(categorical_column=token_sequence, ...)

estimator = RNNEstimator(
    head=tf.estimator.RegressionHead(),
    sequence_feature_columns=[token_emb],
    units=[32, 16], cell_type='lstm')

def rnn_cell_fn(_):
  cells = [ tf.keras.layers.LSTMCell(size) for size in [32, 16] ]
  return tf.keras.layers.StackedRNNCells(cells)

estimator = RNNEstimator(
    head=tf.estimator.RegressionHead(),
    sequence_feature_columns=[token_emb],
    rnn_cell_fn=rnn_cell_fn)

  pass
estimator.train(input_fn=input_fn_train, steps=100)

  pass
metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)
  pass
predictions = estimator.predict(input_fn=input_fn_predict)
"
"tf.estimator.experimental.make_early_stopping_hook(
    estimator, should_stop_fn, run_every_secs=60, run_every_steps=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import make_early_stopping_hook
estimator = ...
hook = early_stopping.make_early_stopping_hook(
    estimator, should_stop_fn=make_stop_fn(...))
train_spec = tf.estimator.TrainSpec(..., hooks=[hook])
tf.estimator.train_and_evaluate(estimator, train_spec, ...)
"
"tf.estimator.experimental.stop_if_higher_hook(
    estimator,
    metric_name,
    threshold,
    eval_dir=None,
    min_steps=0,
    run_every_secs=60,
    run_every_steps=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stop_if_higher_hook
estimator = ...
hook = early_stopping.stop_if_higher_hook(estimator, ""accuracy"", 0.9)
train_spec = tf.estimator.TrainSpec(..., hooks=[hook])
tf.estimator.train_and_evaluate(estimator, train_spec, ...)
"
"tf.estimator.experimental.stop_if_lower_hook(
    estimator,
    metric_name,
    threshold,
    eval_dir=None,
    min_steps=0,
    run_every_secs=60,
    run_every_steps=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stop_if_lower_hook
estimator = ...
hook = early_stopping.stop_if_lower_hook(estimator, ""loss"", 100)
train_spec = tf.estimator.TrainSpec(..., hooks=[hook])
tf.estimator.train_and_evaluate(estimator, train_spec, ...)
"
"tf.estimator.experimental.stop_if_no_decrease_hook(
    estimator,
    metric_name,
    max_steps_without_decrease,
    eval_dir=None,
    min_steps=0,
    run_every_secs=60,
    run_every_steps=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stop_if_no_decrease_hook
estimator = ...
hook = early_stopping.stop_if_no_decrease_hook(estimator, ""loss"", 100000)
train_spec = tf.estimator.TrainSpec(..., hooks=[hook])
tf.estimator.train_and_evaluate(estimator, train_spec, ...)
"
"tf.estimator.experimental.stop_if_no_increase_hook(
    estimator,
    metric_name,
    max_steps_without_increase,
    eval_dir=None,
    min_steps=0,
    run_every_secs=60,
    run_every_steps=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stop_if_no_increase_hook
estimator = ...
hook = early_stopping.stop_if_no_increase_hook(estimator, ""accuracy"", 100000)
train_spec = tf.estimator.TrainSpec(..., hooks=[hook])
tf.estimator.train_and_evaluate(estimator, train_spec, ...)
"
"tf.estimator.regressor_parse_example_spec(
    feature_columns,
    label_key,
    label_dtype=tf.dtypes.float32,
    label_default=None,
    label_dimension=1,
    weight_column=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import regressor_parse_example_spec
feature_b = tf.feature_column.numeric_column(...)
feature_c_bucketized = tf.feature_column.bucketized_column(
  tf.feature_column.numeric_column(""feature_c""), ...)
feature_a_x_feature_c = tf.feature_column.crossed_column(
    columns=[""feature_a"", feature_c_bucketized], ...)

feature_columns = [feature_b, feature_c_bucketized, feature_a_x_feature_c]
parsing_spec = tf.estimator.regressor_parse_example_spec(
    feature_columns, label_key='my-label')

assert parsing_spec == {
  ""feature_a"": parsing_ops.VarLenFeature(tf.string),
  ""feature_b"": parsing_ops.FixedLenFeature([1], dtype=tf.float32),
  ""feature_c"": parsing_ops.FixedLenFeature([1], dtype=tf.float32)
  ""my-label"" : parsing_ops.FixedLenFeature([1], dtype=tf.float32)
}
"
"tf.estimator.train_and_evaluate(
    estimator, train_spec, eval_spec
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import train_and_evaluate
categorial_feature_a = categorial_column_with_hash_bucket(...)
categorial_feature_a_emb = embedding_column(
    categorical_column=categorial_feature_a, ...)

estimator = DNNClassifier(
    feature_columns=[categorial_feature_a_emb, ...],
    hidden_units=[1024, 512, 256])


  pass
  pass

train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=1000)
eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn)

tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
"
"tf.math.exp(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import exp
x = tf.constant(2.0)
tf.math.exp(x)
<tf.Tensor: shape=(), dtype=float32, numpy=7.389056>
<tf.Tensor: shape=(), dtype=float32, numpy=7.389056>"
"tf.experimental.BatchableExtensionType(
    *args, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
class Vehicle(tf.experimental.BatchableExtensionType):
  top_speed: tf.Tensor
  mpg: tf.Tensor
batch = Vehicle([120, 150, 80], [30, 40, 12])
tf.map_fn(lambda vehicle: vehicle.top_speed * vehicle.mpg, batch,
          fn_output_signature=tf.int32).numpy()
array([3600, 6000,  960], dtype=int32)"
"tf.experimental.DynamicRaggedShape.Spec(
    row_partitions: Tuple[RowPartitionSpec, ...],
    static_inner_shape: tf.TensorShape,
    dtype: tf.dtypes.DType
)
","import pandas as pd
import numpy as np
import tensorflow as tf
experimental_as_proto() -> struct_pb2.TypeSpecProto
"
"tf.experimental.ExtensionType(
    *args, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
class MaskedTensor(ExtensionType):
  values: tf.Tensor
  mask: tf.Tensor"
"tf.experimental.RowPartition(
    row_splits,
    row_lengths=None,
    value_rowids=None,
    nrows=None,
    uniform_row_length=None,
    nvals=None,
    internal=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
p1 = RowPartition.from_row_lengths([4, 0, 3, 1, 0])
p2 = RowPartition.from_row_splits([0, 4, 4, 7, 8, 8])
p3 = RowPartition.from_row_starts([0, 4, 4, 7, 8], nvals=8)
p4 = RowPartition.from_row_limits([4, 4, 7, 8, 8])
p5 = RowPartition.from_value_rowids([0, 0, 0, 0, 2, 2, 2, 3], nrows=5)"
"tf.experimental.StructuredTensor(
    fields: Mapping[str, _FieldValue],
    ragged_shape: tf.experimental.DynamicRaggedShape
)
","import pandas as pd
import numpy as np
import tensorflow as tf
s1 = tf.experimental.StructuredTensor.from_pyval(
    {""age"": 82, ""nicknames"": [""Bob"", ""Bobby""]})
s1.shape
TensorShape([])
s1[""age""]
<tf.Tensor: shape=(), dtype=int32, numpy=82>
<tf.Tensor: shape=(), dtype=int32, numpy=82>"
"tf.experimental.StructuredTensor(
    fields: Mapping[str, _FieldValue],
    ragged_shape: tf.experimental.DynamicRaggedShape
)
","import pandas as pd
import numpy as np
import tensorflow as tf
s1 = tf.experimental.StructuredTensor.from_pyval(
    {""age"": 82, ""nicknames"": [""Bob"", ""Bobby""]})
s1.shape
TensorShape([])
s1[""age""]
<tf.Tensor: shape=(), dtype=int32, numpy=82>
<tf.Tensor: shape=(), dtype=int32, numpy=82>"
"tf.experimental.StructuredTensor.Spec(
    _fields, _ragged_shape
)
","import pandas as pd
import numpy as np
import tensorflow as tf
experimental_as_proto() -> struct_pb2.TypeSpecProto
"
"tf.experimental.dispatch_for_binary_elementwise_apis(
    x_type, y_type
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import dispatch_for_binary_elementwise_apis
class MaskedTensor(tf.experimental.ExtensionType):
  values: tf.Tensor
  mask: tf.Tensor
@dispatch_for_binary_elementwise_apis(MaskedTensor, MaskedTensor)
def binary_elementwise_api_handler(api_func, x, y):
  return MaskedTensor(api_func(x.values, y.values), x.mask & y.mask)
a = MaskedTensor([1, 2, 3, 4, 5], [True, True, True, True, False])
b = MaskedTensor([2, 4, 6, 8, 0], [True, True, True, False, True])
c = tf.add(a, b)
print(f""values={c.values.numpy()}, mask={c.mask.numpy()}"")
values=[ 3 6 9 12 5], mask=[ True True True False False]"
"tf.experimental.dispatch_for_binary_elementwise_assert_apis(
    x_type, y_type
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import dispatch_for_binary_elementwise_assert_apis
class MaskedTensor(tf.experimental.ExtensionType):
  values: tf.Tensor
  mask: tf.Tensor
@dispatch_for_binary_elementwise_assert_apis(MaskedTensor, MaskedTensor)
def binary_elementwise_assert_api_handler(assert_func, x, y):
  merged_mask = tf.logical_and(x.mask, y.mask)
  selected_x_values = tf.boolean_mask(x.values, merged_mask)
  selected_y_values = tf.boolean_mask(y.values, merged_mask)
  assert_func(selected_x_values, selected_y_values)
a = MaskedTensor([1, 1, 0, 1, 1], [False, False, True, True, True])
b = MaskedTensor([2, 2, 0, 2, 2], [True, True, True, False, False])"
"tf.experimental.dispatch_for_unary_elementwise_apis(
    x_type
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import dispatch_for_unary_elementwise_apis
class MaskedTensor(tf.experimental.ExtensionType):
  values: tf.Tensor
  mask: tf.Tensor
@dispatch_for_unary_elementwise_apis(MaskedTensor)
def unary_elementwise_api_handler(api_func, x):
  return MaskedTensor(api_func(x.values), x.mask)
mt = MaskedTensor([1, -2, -3], [True, False, True])
abs_mt = tf.abs(mt)
print(f""values={abs_mt.values.numpy()}, mask={abs_mt.mask.numpy()}"")
values=[1 2 3], mask=[ True False True]"
"tf.experimental.dlpack.from_dlpack(
    dlcapsule
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import from_dlpack
  a = tf.experimental.dlpack.from_dlpack(dlcapsule)
"
"tf.experimental.dlpack.to_dlpack(
    tf_tensor
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import to_dlpack
  a = tf.tensor([1, 10])
  dlcapsule = tf.experimental.dlpack.to_dlpack(a)
"
"tf.experimental.dtensor.barrier(
    mesh: tf.experimental.dtensor.Mesh,
    barrier_name: Optional[str] = None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import barrier

x = [1, 2, 3]
x = dtensor.relayout(x, dtensor.Layout.batch_sharded(mesh, 'batch', 1))
dtensor.barrier(mesh)

sys.exit()
"
"tf.experimental.dtensor.device_name() -> str
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import device_name
import tensorflow as tf

with tf.device(dtensor.device_name()):
"
"tf.experimental.dtensor.pack(
    tensors: Sequence[Any],
    layout: tf.experimental.dtensor.Layout
) -> Any
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import pack
* unpack(pack(tensors)) == tensors
* pack(unpack(dtensor)) == dtensor
"
"tf.experimental.dtensor.unpack(
    tensor: Any
) -> Sequence[Any]
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import unpack
* unpack(pack(tensors)) == tensors
* pack(unpack(dtensor)) == dtensor
"
"tf.experimental.numpy.iinfo(
    int_type
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import iinfo
ii16 = np.iinfo(np.int16)
ii16.min
-32768
ii16.max
32767
ii32 = np.iinfo(np.int32)
ii32.min
-2147483648
ii32.max
2147483647"
"tf.experimental.numpy.issubdtype(
    arg1, arg2
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import issubdtype
ints = np.array([1, 2, 3], dtype=np.int32)
np.issubdtype(ints.dtype, np.integer)
True
np.issubdtype(ints.dtype, np.floating)
False"
"tf.Tensor(
    op, value_index, dtype
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Tensor
c = tf.constant([[1.0, 2.0], [3.0, 4.0]])
d = tf.constant([[1.0, 1.0], [0.0, 1.0]])
e = tf.matmul(c, d)
print(e)
tf.Tensor(
[[1. 3.]
 [3. 7.]], shape=(2, 2), dtype=float32)"
"tf.eye(
    num_rows,
    num_columns=None,
    batch_shape=None,
    dtype=tf.dtypes.float32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import eye
tf.eye(2)
==> [[1., 0.],
     [0., 1.]]

batch_identity = tf.eye(2, batch_shape=[3])

tf.eye(2, num_columns=3)
==> [[ 1.,  0.,  0.],
     [ 0.,  1.,  0.]]
"
"tf.feature_column.categorical_column_with_hash_bucket(
    key,
    hash_bucket_size,
    dtype=tf.dtypes.string
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import categorical_column_with_hash_bucket
import tensorflow as tf
keywords = tf.feature_column.categorical_column_with_hash_bucket(""keywords"",
10000)
columns = [keywords]
features = {'keywords': tf.constant([['Tensorflow', 'Keras', 'RNN', 'LSTM',
'CNN'], ['LSTM', 'CNN', 'Tensorflow', 'Keras', 'RNN'], ['CNN', 'Tensorflow',
'LSTM', 'Keras', 'RNN']])}
linear_prediction, _, _ = tf.compat.v1.feature_column.linear_model(features,
columns)

import tensorflow as tf
keywords = tf.feature_column.categorical_column_with_hash_bucket(""keywords"",
10000)
keywords_embedded = tf.feature_column.embedding_column(keywords, 16)
columns = [keywords_embedded]
features = {'keywords': tf.constant([['Tensorflow', 'Keras', 'RNN', 'LSTM',
'CNN'], ['LSTM', 'CNN', 'Tensorflow', 'Keras', 'RNN'], ['CNN', 'Tensorflow',
'LSTM', 'Keras', 'RNN']])}
input_layer = tf.keras.layers.DenseFeatures(columns)
dense_tensor = input_layer(features)
"
"tf.feature_column.categorical_column_with_identity(
    key, num_buckets, default_value=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import categorical_column_with_identity
import tensorflow as tf
video_id = tf.feature_column.categorical_column_with_identity(
    key='video_id', num_buckets=1000000, default_value=0)
columns = [video_id]
features = {'video_id': tf.sparse.from_dense([[2, 85, 0, 0, 0],
[33,78, 2, 73, 1]])}
linear_prediction = tf.compat.v1.feature_column.linear_model(features,
columns)
"
"tf.feature_column.categorical_column_with_vocabulary_file(
    key,
    vocabulary_file,
    vocabulary_size=None,
    dtype=tf.dtypes.string,
    default_value=None,
    num_oov_buckets=0,
    file_format=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import categorical_column_with_vocabulary_file
states = categorical_column_with_vocabulary_file(
    key='states', vocabulary_file='/us/states.txt', vocabulary_size=50,
    num_oov_buckets=5)
columns = [states, ...]
features = tf.io.parse_example(..., features=make_parse_example_spec(columns))
linear_prediction = linear_model(features, columns)
"
"tf.feature_column.categorical_column_with_vocabulary_list(
    key, vocabulary_list, dtype=None, default_value=-1, num_oov_buckets=0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import categorical_column_with_vocabulary_list
colors = categorical_column_with_vocabulary_list(
    key='colors', vocabulary_list=('R', 'G', 'B', 'Y'),
    num_oov_buckets=2)
columns = [colors, ...]
features = tf.io.parse_example(..., features=make_parse_example_spec(columns))
linear_prediction, _, _ = linear_model(features, columns)
"
"tf.feature_column.embedding_column(
    categorical_column,
    dimension,
    combiner='mean',
    initializer=None,
    ckpt_to_load_from=None,
    tensor_name_in_ckpt=None,
    max_norm=None,
    trainable=True,
    use_safe_embedding_lookup=True
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import embedding_column
video_id = categorical_column_with_identity(
    key='video_id', num_buckets=1000000, default_value=0)
columns = [embedding_column(video_id, 9),...]

estimator = tf.estimator.DNNClassifier(feature_columns=columns, ...)

label_column = ...
def input_fn():
  features = tf.io.parse_example(
      ..., features=make_parse_example_spec(columns + [label_column]))
  labels = features.pop(label_column.name)
  return features, labels

estimator.train(input_fn=input_fn, steps=100)
"
"tf.feature_column.indicator_column(
    categorical_column
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import indicator_column
name = indicator_column(categorical_column_with_vocabulary_list(
    'name', ['bob', 'george', 'wanda']))
columns = [name, ...]
features = tf.io.parse_example(..., features=make_parse_example_spec(columns))
dense_tensor = input_layer(features, columns)

"
"tf.feature_column.make_parse_example_spec(
    feature_columns
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import make_parse_example_spec
feature_a = tf.feature_column.categorical_column_with_vocabulary_file(...)
feature_b = tf.feature_column.numeric_column(...)
feature_c_bucketized = tf.feature_column.bucketized_column(
    tf.feature_column.numeric_column(""feature_c""), ...)
feature_a_x_feature_c = tf.feature_column.crossed_column(
    columns=[""feature_a"", feature_c_bucketized], ...)

feature_columns = set(
    [feature_b, feature_c_bucketized, feature_a_x_feature_c])
features = tf.io.parse_example(
    serialized=serialized_examples,
    features=tf.feature_column.make_parse_example_spec(feature_columns))
"
"tf.feature_column.sequence_categorical_column_with_hash_bucket(
    key,
    hash_bucket_size,
    dtype=tf.dtypes.string
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sequence_categorical_column_with_hash_bucket
tokens = sequence_categorical_column_with_hash_bucket(
    'tokens', hash_bucket_size=1000)
tokens_embedding = embedding_column(tokens, dimension=10)
columns = [tokens_embedding]

features = tf.io.parse_example(..., features=make_parse_example_spec(columns))
sequence_feature_layer = SequenceFeatures(columns)
sequence_input, sequence_length = sequence_feature_layer(features)
sequence_length_mask = tf.sequence_mask(sequence_length)

rnn_cell = tf.keras.layers.SimpleRNNCell(hidden_size)
rnn_layer = tf.keras.layers.RNN(rnn_cell)
outputs, state = rnn_layer(sequence_input, mask=sequence_length_mask)
"
"tf.feature_column.sequence_categorical_column_with_identity(
    key, num_buckets, default_value=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sequence_categorical_column_with_identity
watches = sequence_categorical_column_with_identity(
    'watches', num_buckets=1000)
watches_embedding = embedding_column(watches, dimension=10)
columns = [watches_embedding]

features = tf.io.parse_example(..., features=make_parse_example_spec(columns))
sequence_feature_layer = SequenceFeatures(columns)
sequence_input, sequence_length = sequence_feature_layer(features)
sequence_length_mask = tf.sequence_mask(sequence_length)

rnn_cell = tf.keras.layers.SimpleRNNCell(hidden_size)
rnn_layer = tf.keras.layers.RNN(rnn_cell)
outputs, state = rnn_layer(sequence_input, mask=sequence_length_mask)
"
"tf.feature_column.sequence_categorical_column_with_vocabulary_file(
    key,
    vocabulary_file,
    vocabulary_size=None,
    num_oov_buckets=0,
    default_value=None,
    dtype=tf.dtypes.string
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sequence_categorical_column_with_vocabulary_file
states = sequence_categorical_column_with_vocabulary_file(
    key='states', vocabulary_file='/us/states.txt', vocabulary_size=50,
    num_oov_buckets=5)
states_embedding = embedding_column(states, dimension=10)
columns = [states_embedding]

features = tf.io.parse_example(..., features=make_parse_example_spec(columns))
sequence_feature_layer = SequenceFeatures(columns)
sequence_input, sequence_length = sequence_feature_layer(features)
sequence_length_mask = tf.sequence_mask(sequence_length)

rnn_cell = tf.keras.layers.SimpleRNNCell(hidden_size)
rnn_layer = tf.keras.layers.RNN(rnn_cell)
outputs, state = rnn_layer(sequence_input, mask=sequence_length_mask)
"
"tf.feature_column.sequence_categorical_column_with_vocabulary_list(
    key, vocabulary_list, dtype=None, default_value=-1, num_oov_buckets=0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sequence_categorical_column_with_vocabulary_list
colors = sequence_categorical_column_with_vocabulary_list(
    key='colors', vocabulary_list=('R', 'G', 'B', 'Y'),
    num_oov_buckets=2)
colors_embedding = embedding_column(colors, dimension=3)
columns = [colors_embedding]

features = tf.io.parse_example(..., features=make_parse_example_spec(columns))
sequence_feature_layer = SequenceFeatures(columns)
sequence_input, sequence_length = sequence_feature_layer(features)
sequence_length_mask = tf.sequence_mask(sequence_length)

rnn_cell = tf.keras.layers.SimpleRNNCell(hidden_size)
rnn_layer = tf.keras.layers.RNN(rnn_cell)
outputs, state = rnn_layer(sequence_input, mask=sequence_length_mask)
"
"tf.feature_column.sequence_numeric_column(
    key,
    shape=(1,),
    default_value=0.0,
    dtype=tf.dtypes.float32,
    normalizer_fn=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sequence_numeric_column
temperature = sequence_numeric_column('temperature')
columns = [temperature]

features = tf.io.parse_example(..., features=make_parse_example_spec(columns))
sequence_feature_layer = SequenceFeatures(columns)
sequence_input, sequence_length = sequence_feature_layer(features)
sequence_length_mask = tf.sequence_mask(sequence_length)

rnn_cell = tf.keras.layers.SimpleRNNCell(hidden_size)
rnn_layer = tf.keras.layers.RNN(rnn_cell)
outputs, state = rnn_layer(sequence_input, mask=sequence_length_mask)
"
"tf.fill(
    dims, value, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import fill
tf.fill([2, 3], 9)
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
array([[9, 9, 9],
       [9, 9, 9]], dtype=int32)>"
"tf.fingerprint(
    data, method='farmhash64', name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import fingerprint
tf.fingerprint(data) == tf.fingerprint(tf.reshape(data, ...))
tf.fingerprint(data) == tf.fingerprint(tf.bitcast(data, ...))
"
"tf.math.floor(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import floor
x = tf.constant([1.3324, -1.5, 5.555, -2.532, 0.99, float(""inf"")])
tf.floor(x).numpy()
array([ 1., -2.,  5., -3.,  0., inf], dtype=float32)"
"tf.foldl(
    fn,
    elems,
    initializer=None,
    parallel_iterations=10,
    back_prop=True,
    swap_memory=False,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import foldl
elems = tf.constant([1, 2, 3, 4, 5, 6])
sum = tf.foldl(lambda a, x: a + x, elems)
"
"tf.foldr(
    fn,
    elems,
    initializer=None,
    parallel_iterations=10,
    back_prop=True,
    swap_memory=False,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import foldr
elems = [1, 2, 3, 4, 5, 6]
sum = tf.foldr(lambda a, x: a + x, elems)
"
"tf.function(
    func=None,
    input_signature=None,
    autograph=True,
    jit_compile=None,
    reduce_retracing=False,
    experimental_implements=None,
    experimental_autograph_options=None,
    experimental_relax_shapes=None,
    experimental_compile=None,
    experimental_follow_type_hints=None
) -> tf.types.experimental.GenericFunction
","import pandas as pd
import numpy as np
import tensorflow as tf
@tf.function
def f(x, y):
  return x ** 2 + y
x = tf.constant([2, 3])
y = tf.constant([3, -2])
f(x, y)
<tf.Tensor: ... numpy=array([7, 7], ...)>
<tf.Tensor: ... numpy=array([7, 7], ...)>"
"tf.gather(
    params, indices, validate_indices=None, axis=None, batch_dims=0, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import gather
params = tf.constant(['p0', 'p1', 'p2', 'p3', 'p4', 'p5'])
params[3].numpy()
b'p3'
tf.gather(params, 3).numpy()
b'p3'"
"tf.gather_nd(
    params, indices, batch_dims=0, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import gather_nd
tf.gather_nd(
    indices=[[0, 0],
             [1, 1]],
    params = [['a', 'b'],
              ['c', 'd']]).numpy()
array([b'a', b'd'], dtype=object)"
"tf.get_static_value(
    tensor, partial=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get_static_value
a = tf.constant(10)
tf.get_static_value(a)
10
b = tf.constant(20)
tf.get_static_value(tf.add(a, b))
30"
"tf.grad_pass_through(
    f
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import grad_pass_through
x = tf.Variable(1.0, name=""x"")
z = tf.Variable(3.0, name=""z"")

with tf.GradientTape() as tape:
  y = tf.grad_pass_through(x.assign)(z**2)
grads = tape.gradient(y, z)
"
"tf.gradients(
    ys,
    xs,
    grad_ys=None,
    name='gradients',
    gate_gradients=False,
    aggregation_method=None,
    stop_gradients=None,
    unconnected_gradients=tf.UnconnectedGradients.NONE
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import gradients
@tf.function
def example():
  a = tf.constant(0.)
  b = 2 * a
  return tf.gradients(a + b, [a, b], stop_gradients=[a, b])
example()
[<tf.Tensor: shape=(), dtype=float32, numpy=1.0>,
[<tf.Tensor: shape=(), dtype=float32, numpy=1.0>,
<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]
<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]"
"tf.math.greater(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import greater
x = tf.constant([5, 4, 6])
y = tf.constant([5, 2, 5])
tf.math.greater(x, y) ==> [False, True, True]

x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.greater(x, y) ==> [False, False, True]
"
"tf.math.greater_equal(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import greater_equal
x = tf.constant([5, 4, 6, 7])
y = tf.constant([5, 2, 5, 10])
tf.math.greater_equal(x, y) ==> [True, True, True, False]

x = tf.constant([5, 4, 6, 7])
y = tf.constant([5])
tf.math.greater_equal(x, y) ==> [True, False, True, True]
"
"tf.histogram_fixed_width(
    values,
    value_range,
    nbins=100,
    dtype=tf.dtypes.int32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import histogram_fixed_width
nbins = 5
value_range = [0.0, 5.0]
new_values = [-1.0, 0.0, 1.5, 2.0, 5.0, 15]
hist = tf.histogram_fixed_width(new_values, value_range, nbins=5)
hist.numpy()
array([2, 1, 1, 0, 2], dtype=int32)"
"tf.histogram_fixed_width_bins(
    values,
    value_range,
    nbins=100,
    dtype=tf.dtypes.int32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import histogram_fixed_width_bins
nbins = 5
value_range = [0.0, 5.0]
new_values = [-1.0, 0.0, 1.5, 2.0, 5.0, 15]
indices = tf.histogram_fixed_width_bins(new_values, value_range, nbins=5)
indices.numpy()
array([0, 0, 1, 2, 4, 4], dtype=int32)"
"tf.identity(
    input, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import identity
a = tf.constant([0.78])
a_identity = tf.identity(a)
a.numpy()
array([0.78], dtype=float32)
a_identity.numpy()
array([0.78], dtype=float32)"
"tf.identity_n(
    input, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import identity_n
with tf.get_default_graph().gradient_override_map(
    {'IdentityN': 'OverrideGradientWithG'}):
  y, _ = identity_n([f(x), x])

@tf.RegisterGradient('OverrideGradientWithG')
def ApplyG(op, dy, _):
"
"tf.image.adjust_brightness(
    image, delta
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import adjust_brightness
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.adjust_brightness(x, delta=0.1)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 1.1,  2.1,  3.1],
        [ 4.1,  5.1,  6.1]],
       [[ 7.1,  8.1,  9.1],
        [10.1, 11.1, 12.1]]], dtype=float32)>"
"tf.image.adjust_contrast(
    images, contrast_factor
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import adjust_contrast
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.adjust_contrast(x, 2.)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[-3.5, -2.5, -1.5],
        [ 2.5,  3.5,  4.5]],
       [[ 8.5,  9.5, 10.5],
        [14.5, 15.5, 16.5]]], dtype=float32)>"
"tf.image.adjust_gamma(
    image, gamma=1, gain=1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import adjust_gamma
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.adjust_gamma(x, 0.2)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[1.       , 1.1486983, 1.2457309],
        [1.319508 , 1.3797297, 1.4309691]],
       [[1.4757731, 1.5157166, 1.5518456],
        [1.5848932, 1.6153942, 1.6437519]]], dtype=float32)>"
"tf.image.adjust_hue(
    image, delta, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import adjust_hue
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.adjust_hue(x, 0.2)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 2.3999996,  1.       ,  3.       ],
        [ 5.3999996,  4.       ,  6.       ]],
      [[ 8.4      ,  7.       ,  9.       ],
        [11.4      , 10.       , 12.       ]]], dtype=float32)>"
"tf.image.adjust_jpeg_quality(
    image, jpeg_quality, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import adjust_jpeg_quality
x = [[[0.01, 0.02, 0.03],
      [0.04, 0.05, 0.06]],
     [[0.07, 0.08, 0.09],
      [0.10, 0.11, 0.12]]]
x_jpeg = tf.image.adjust_jpeg_quality(x, 75)
x_jpeg.numpy()
array([[[0.00392157, 0.01960784, 0.03137255],
        [0.02745098, 0.04313726, 0.05490196]],
       [[0.05882353, 0.07450981, 0.08627451],
        [0.08235294, 0.09803922, 0.10980393]]], dtype=float32)"
"tf.image.adjust_saturation(
    image, saturation_factor, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import adjust_saturation
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.adjust_saturation(x, 0.5)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 2. ,  2.5,  3. ],
        [ 5. ,  5.5,  6. ]],
       [[ 8. ,  8.5,  9. ],
        [11. , 11.5, 12. ]]], dtype=float32)>"
"tf.image.convert_image_dtype(
    image, dtype, saturate=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import convert_image_dtype
x = [[[1, 2, 3], [4, 5, 6]],
     [[7, 8, 9], [10, 11, 12]]]
x_int8 = tf.convert_to_tensor(x, dtype=tf.int8)
tf.image.convert_image_dtype(x_int8, dtype=tf.float16, saturate=False)
<tf.Tensor: shape=(2, 2, 3), dtype=float16, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=float16, numpy=
array([[[0.00787, 0.01575, 0.02362],
        [0.0315 , 0.03937, 0.04724]],
       [[0.0551 , 0.063  , 0.07086],
        [0.07874, 0.0866 , 0.0945 ]]], dtype=float16)>"
"tf.image.crop_and_resize(
    image,
    boxes,
    box_indices,
    crop_size,
    method='bilinear',
    extrapolation_value=0.0,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import crop_and_resize
import tensorflow as tf
BATCH_SIZE = 1
NUM_BOXES = 5
IMAGE_HEIGHT = 256
IMAGE_WIDTH = 256
CHANNELS = 3
CROP_SIZE = (24, 24)

image = tf.random.normal(shape=(BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WIDTH,
CHANNELS) )
boxes = tf.random.uniform(shape=(NUM_BOXES, 4))
box_indices = tf.random.uniform(shape=(NUM_BOXES,), minval=0,
maxval=BATCH_SIZE, dtype=tf.int32)
output = tf.image.crop_and_resize(image, boxes, box_indices, CROP_SIZE)
output.shape  #=> (5, 24, 24, 3)
"
"tf.image.crop_to_bounding_box(
    image, offset_height, offset_width, target_height, target_width
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import crop_to_bounding_box
image = tf.constant(np.arange(1, 28, dtype=np.float32), shape=[3, 3, 3])
<tf.Tensor: shape=(3, 3), dtype=float32, numpy=
<tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[ 1.,  4.,  7.],
       [10., 13., 16.],
       [19., 22., 25.]], dtype=float32)>
cropped_image = tf.image.crop_to_bounding_box(image, 0, 0, 2, 2)
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[ 1.,  4.],
       [10., 13.]], dtype=float32)>"
"tf.image.draw_bounding_boxes(
    images, boxes, colors, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import draw_bounding_boxes
img = tf.zeros([1, 3, 3, 3])
box = np.array([0, 0, 1, 1])
boxes = box.reshape([1, 1, 4])
colors = np.array([[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]])
tf.image.draw_bounding_boxes(img, boxes, colors)
<tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
<tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.]],
        [[1., 0., 0.],
        [0., 0., 0.],
        [1., 0., 0.]],
        [[1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.]]]], dtype=float32)>"
"tf.image.extract_glimpse(
    input,
    size,
    offsets,
    centered=True,
    normalized=True,
    noise='uniform',
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import extract_glimpse
x = [[[[0.0],
          [1.0],
          [2.0]],
         [[3.0],
          [4.0],
          [5.0]],
         [[6.0],
          [7.0],
          [8.0]]]]
tf.image.extract_glimpse(x, size=(2, 2), offsets=[[1, 1]],
                        centered=False, normalized=False)
<tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=
<tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=
array([[[[4.],
         [5.]],
        [[7.],
         [8.]]]], dtype=float32)>"
"tf.image.extract_patches(
    images, sizes, strides, rates, padding, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import extract_patches
  n = 10
  images = [[[[x * n + y + 1] for y in range(n)] for x in range(n)]]

  tf.image.extract_patches(images=images,
                           sizes=[1, 3, 3, 1],
                           strides=[1, 5, 5, 1],
                           rates=[1, 1, 1, 1],
                           padding='VALID')

  [[[[ 1  2  3 11 12 13 21 22 23]
     [ 6  7  8 16 17 18 26 27 28]]
    [[51 52 53 61 62 63 71 72 73]
     [56 57 58 66 67 68 76 77 78]]]]
"
"tf.image.flip_left_right(
    image
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import flip_left_right
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.flip_left_right(x)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 4.,  5.,  6.],
        [ 1.,  2.,  3.]],
       [[10., 11., 12.],
        [ 7.,  8.,  9.]]], dtype=float32)>"
"tf.image.flip_up_down(
    image
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import flip_up_down
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.flip_up_down(x)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 7.,  8.,  9.],
        [10., 11., 12.]],
       [[ 1.,  2.,  3.],
        [ 4.,  5.,  6.]]], dtype=float32)>"
"tf.image.grayscale_to_rgb(
    images, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import grayscale_to_rgb
original = tf.constant([[[1.0], [2.0], [3.0]]])
converted = tf.image.grayscale_to_rgb(original)
print(converted.numpy())
[[[1. 1. 1.]
  [2. 2. 2.]
  [3. 3. 3.]]]"
"tf.image.image_gradients(
    image
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import image_gradients
BATCH_SIZE = 1
IMAGE_HEIGHT = 5
IMAGE_WIDTH = 5
CHANNELS = 1
image = tf.reshape(tf.range(IMAGE_HEIGHT * IMAGE_WIDTH * CHANNELS,
  delta=1, dtype=tf.float32),
  shape=(BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WIDTH, CHANNELS))
dy, dx = tf.image.image_gradients(image)
print(image[0, :,:,0])
tf.Tensor(
  [[ 0.  1.  2.  3.  4.]
  [ 5.  6.  7.  8.  9.]
  [10. 11. 12. 13. 14.]
  [15. 16. 17. 18. 19.]
  [20. 21. 22. 23. 24.]], shape=(5, 5), dtype=float32)
print(dy[0, :,:,0])
tf.Tensor(
  [[5. 5. 5. 5. 5.]
  [5. 5. 5. 5. 5.]
  [5. 5. 5. 5. 5.]
  [5. 5. 5. 5. 5.]
  [0. 0. 0. 0. 0.]], shape=(5, 5), dtype=float32)
print(dx[0, :,:,0])
tf.Tensor(
  [[1. 1. 1. 1. 0.]
  [1. 1. 1. 1. 0.]
  [1. 1. 1. 1. 0.]
  [1. 1. 1. 1. 0.]
  [1. 1. 1. 1. 0.]], shape=(5, 5), dtype=float32)
"
"tf.image.non_max_suppression(
    boxes,
    scores,
    max_output_size,
    iou_threshold=0.5,
    score_threshold=float('-inf'),
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import non_max_suppression
  selected_indices = tf.image.non_max_suppression(
      boxes, scores, max_output_size, iou_threshold)
  selected_boxes = tf.gather(boxes, selected_indices)
"
"tf.image.non_max_suppression_overlaps(
    overlaps,
    scores,
    max_output_size,
    overlap_threshold=0.5,
    score_threshold=float('-inf'),
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import non_max_suppression_overlaps
  selected_indices = tf.image.non_max_suppression_overlaps(
      overlaps, scores, max_output_size, iou_threshold)
  selected_boxes = tf.gather(boxes, selected_indices)
"
"tf.image.non_max_suppression_padded(
    boxes,
    scores,
    max_output_size,
    iou_threshold=0.5,
    score_threshold=float('-inf'),
    pad_to_max_output_size=False,
    name=None,
    sorted_input=False,
    canonicalized_coordinates=False,
    tile_size=512
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import non_max_suppression_padded
  selected_indices_padded, num_valid = tf.image.non_max_suppression_padded(
      boxes, scores, max_output_size, iou_threshold,
      score_threshold, pad_to_max_output_size=True)
  selected_indices = tf.slice(
      selected_indices_padded, tf.constant([0]), num_valid)
  selected_boxes = tf.gather(boxes, selected_indices)
"
"tf.image.pad_to_bounding_box(
    image, offset_height, offset_width, target_height, target_width
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import pad_to_bounding_box
x = [[[1., 2., 3.],
      [4., 5., 6.]],
      [[7., 8., 9.],
      [10., 11., 12.]]]
padded_image = tf.image.pad_to_bounding_box(x, 1, 1, 4, 4)
padded_image
<tf.Tensor: shape=(4, 4, 3), dtype=float32, numpy=
<tf.Tensor: shape=(4, 4, 3), dtype=float32, numpy=
array([[[ 0.,  0.,  0.],
[ 0.,  0.,  0.],
[ 0.,  0.,  0.],
[ 0.,  0.,  0.]],
[[ 0.,  0.,  0.],
[ 1.,  2.,  3.],
[ 4.,  5.,  6.],
[ 0.,  0.,  0.]],
[[ 0.,  0.,  0.],
[ 7.,  8.,  9.],
[10., 11., 12.],
[ 0.,  0.,  0.]],
[[ 0.,  0.,  0.],
[ 0.,  0.,  0.],
[ 0.,  0.,  0.],
[ 0.,  0.,  0.]]], dtype=float32)>"
"tf.image.per_image_standardization(
    image
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import per_image_standardization
image = tf.constant(np.arange(1, 13, dtype=np.int32), shape=[2, 2, 3])
<tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=
array([[[ 1,  2,  3],
        [ 4,  5,  6]],
       [[ 7,  8,  9],
        [10, 11, 12]]], dtype=int32)>
new_image = tf.image.per_image_standardization(image)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[-1.593255  , -1.3035723 , -1.0138896 ],
        [-0.7242068 , -0.4345241 , -0.14484136]],
       [[ 0.14484136,  0.4345241 ,  0.7242068 ],
        [ 1.0138896 ,  1.3035723 ,  1.593255  ]]], dtype=float32)>"
"tf.image.psnr(
    a, b, max_val, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import psnr
    im1 = tf.decode_png('path/to/im1.png')
    im2 = tf.decode_png('path/to/im2.png')
    psnr1 = tf.image.psnr(im1, im2, max_val=255)

    im1 = tf.image.convert_image_dtype(im1, tf.float32)
    im2 = tf.image.convert_image_dtype(im2, tf.float32)
    psnr2 = tf.image.psnr(im1, im2, max_val=1.0)
"
"tf.image.random_brightness(
    image, max_delta, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import random_brightness
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
     [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.random_brightness(x, 0.2)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=...>
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=...>"
"tf.image.random_contrast(
    image, lower, upper, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import random_contrast
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.random_contrast(x, 0.2, 0.5)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=...>
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=...>"
"tf.image.random_crop(
    value, size, seed=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import random_crop
image = [[1, 2, 3], [4, 5, 6]]
result = tf.image.random_crop(value=image, size=(1, 3))
result.shape.as_list()
[1, 3]"
"tf.image.random_flip_left_right(
    image, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import random_flip_left_right
image = np.array([[[1], [2]], [[3], [4]]])
tf.image.random_flip_left_right(image, 5).numpy().tolist()
[[[2], [1]], [[4], [3]]]"
"tf.image.random_flip_up_down(
    image, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import random_flip_up_down
image = np.array([[[1], [2]], [[3], [4]]])
tf.image.random_flip_up_down(image, 3).numpy().tolist()
[[[3], [4]], [[1], [2]]]"
"tf.image.random_hue(
    image, max_delta, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import random_hue
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.random_hue(x, 0.2)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=...>
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=...>"
"tf.image.random_jpeg_quality(
    image, min_jpeg_quality, max_jpeg_quality, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import random_jpeg_quality
x = tf.constant([[[1, 2, 3],
                  [4, 5, 6]],
                 [[7, 8, 9],
                  [10, 11, 12]]], dtype=tf.uint8)
tf.image.random_jpeg_quality(x, 75, 95)
<tf.Tensor: shape=(2, 2, 3), dtype=uint8, numpy=...>
<tf.Tensor: shape=(2, 2, 3), dtype=uint8, numpy=...>"
"tf.image.random_saturation(
    image, lower, upper, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import random_saturation
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.random_saturation(x, 5, 10)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 0. ,  1.5,  3. ],
        [ 0. ,  3. ,  6. ]],
       [[ 0. ,  4.5,  9. ],
        [ 0. ,  6. , 12. ]]], dtype=float32)>"
"tf.image.resize(
    images,
    size,
    method=ResizeMethod.BILINEAR,
    preserve_aspect_ratio=False,
    antialias=False,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import resize
image = tf.constant([
 [1,0,0,0,0],
 [0,1,0,0,0],
 [0,0,1,0,0],
 [0,0,0,1,0],
 [0,0,0,0,1],
])
image = image[tf.newaxis, ..., tf.newaxis]
[1, 5, 5, 1]
tf.image.resize(image, [3,5])[0,...,0].numpy()
array([[0.6666667, 0.3333333, 0.       , 0.       , 0.       ],
       [0.       , 0.       , 1.       , 0.       , 0.       ],
       [0.       , 0.       , 0.       , 0.3333335, 0.6666665]],
      dtype=float32)"
"tf.image.resize_with_crop_or_pad(
    image, target_height, target_width
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import resize_with_crop_or_pad
array([[ 0,  3,  6,  9, 12],
       [15, 18, 21, 24, 27],
       [30, 33, 36, 39, 42],
       [45, 48, 51, 54, 57],
       [60, 63, 66, 69, 72]])
image[:,:,0]
<tf.Tensor: shape=(3, 3), dtype=int64, numpy=
<tf.Tensor: shape=(3, 3), dtype=int64, numpy=
array([[18, 21, 24],
       [33, 36, 39],
       [48, 51, 54]])>"
"tf.image.rgb_to_grayscale(
    images, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import rgb_to_grayscale
original = tf.constant([[[1.0, 2.0, 3.0]]])
converted = tf.image.rgb_to_grayscale(original)
print(converted.numpy())
[[[1.81...]]]"
"tf.image.rgb_to_hsv(
    images, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import rgb_to_hsv
blue_image = tf.stack([
   tf.zeros([5,5]),
   tf.zeros([5,5]),
   tf.ones([5,5])],
   axis=-1)
blue_hsv_image = tf.image.rgb_to_hsv(blue_image)
blue_hsv_image[0,0].numpy()
array([0.6666667, 1. , 1. ], dtype=float32)"
"tf.image.rgb_to_yiq(
    images
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import rgb_to_yiq
x = tf.constant([[[1.0, 2.0, 3.0]]])
tf.image.rgb_to_yiq(x)
<tf.Tensor: shape=(1, 1, 3), dtype=float32,
<tf.Tensor: shape=(1, 1, 3), dtype=float32,
numpy=array([[[ 1.815     , -0.91724455,  0.09962624]]], dtype=float32)>"
"tf.image.rot90(
    image, k=1, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import rot90
a=tf.constant([[[1],[2]],
               [[3],[4]]])
a_rot=tf.image.rot90(a)
print(a_rot[...,0].numpy())
[[2 4]
 [1 3]]
a_rot=tf.image.rot90(a, k=3)
print(a_rot[...,0].numpy())
[[3 1]
 [4 2]]
a_rot=tf.image.rot90(a, k=-2)
print(a_rot[...,0].numpy())
[[4 3]
 [2 1]]"
"tf.image.sample_distorted_bounding_box(
    image_size,
    bounding_boxes,
    seed=0,
    min_object_covered=0.1,
    aspect_ratio_range=None,
    area_range=None,
    max_attempts=None,
    use_image_if_no_bounding_boxes=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sample_distorted_bounding_box
    begin, size, bbox_for_draw = tf.image.sample_distorted_bounding_box(
        tf.shape(image),
        bounding_boxes=bounding_boxes,
        min_object_covered=0.1)

    image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),
                                                  bbox_for_draw)
    tf.compat.v1.summary.image('images_with_box', image_with_box)

    distorted_image = tf.slice(image, begin, size)
"
"tf.image.ssim(
    img1,
    img2,
    max_val,
    filter_size=11,
    filter_sigma=1.5,
    k1=0.01,
    k2=0.03,
    return_index_map=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ssim
    im1 = tf.image.decode_image(tf.io.read_file('path/to/im1.png'))
    im2 = tf.image.decode_image(tf.io.read_file('path/to/im2.png'))
    im1 = tf.expand_dims(im1, axis=0)
    im2 = tf.expand_dims(im2, axis=0)
    ssim1 = tf.image.ssim(im1, im2, max_val=255, filter_size=11,
                          filter_sigma=1.5, k1=0.01, k2=0.03)

    im1 = tf.image.convert_image_dtype(im1, tf.float32)
    im2 = tf.image.convert_image_dtype(im2, tf.float32)
    ssim2 = tf.image.ssim(im1, im2, max_val=1.0, filter_size=11,
                          filter_sigma=1.5, k1=0.01, k2=0.03)
"
"tf.image.stateless_random_brightness(
    image, max_delta, seed
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_random_brightness
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
     [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
seed = (1, 2)
tf.image.stateless_random_brightness(x, 0.2, seed)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 1.1376241,  2.1376243,  3.1376243],
        [ 4.1376243,  5.1376243,  6.1376243]],
       [[ 7.1376243,  8.137624 ,  9.137624 ],
        [10.137624 , 11.137624 , 12.137624 ]]], dtype=float32)>"
"tf.image.stateless_random_contrast(
    image, lower, upper, seed
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_random_contrast
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
     [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
seed = (1, 2)
tf.image.stateless_random_contrast(x, 0.2, 0.5, seed)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[3.4605184, 4.4605184, 5.4605184],
        [4.820173 , 5.820173 , 6.820173 ]],
       [[6.179827 , 7.179827 , 8.179828 ],
        [7.5394816, 8.539482 , 9.539482 ]]], dtype=float32)>"
"tf.image.stateless_random_crop(
    value, size, seed, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_random_crop
image = [[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]
seed = (1, 2)
tf.image.stateless_random_crop(value=image, size=(1, 2, 3), seed=seed)
<tf.Tensor: shape=(1, 2, 3), dtype=int32, numpy=
<tf.Tensor: shape=(1, 2, 3), dtype=int32, numpy=
array([[[1, 2, 3],
        [4, 5, 6]]], dtype=int32)>"
"tf.image.stateless_random_flip_left_right(
    image, seed
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_random_flip_left_right
image = np.array([[[1], [2]], [[3], [4]]])
seed = (2, 3)
tf.image.stateless_random_flip_left_right(image, seed).numpy().tolist()
[[[2], [1]], [[4], [3]]]"
"tf.image.stateless_random_flip_up_down(
    image, seed
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_random_flip_up_down
image = np.array([[[1], [2]], [[3], [4]]])
seed = (2, 3)
tf.image.stateless_random_flip_up_down(image, seed).numpy().tolist()
[[[3], [4]], [[1], [2]]]"
"tf.image.stateless_random_hue(
    image, max_delta, seed
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_random_hue
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
     [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
seed = (1, 2)
tf.image.stateless_random_hue(x, 0.2, seed)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 1.6514902,  1.       ,  3.       ],
        [ 4.65149  ,  4.       ,  6.       ]],
       [[ 7.65149  ,  7.       ,  9.       ],
        [10.65149  , 10.       , 12.       ]]], dtype=float32)>"
"tf.image.stateless_random_jpeg_quality(
    image, min_jpeg_quality, max_jpeg_quality, seed
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_random_jpeg_quality
x = tf.constant([[[1, 2, 3],
                  [4, 5, 6]],
                 [[7, 8, 9],
                  [10, 11, 12]]], dtype=tf.uint8)
seed = (1, 2)
tf.image.stateless_random_jpeg_quality(x, 75, 95, seed)
<tf.Tensor: shape=(2, 2, 3), dtype=uint8, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=uint8, numpy=
array([[[ 0,  4,  5],
        [ 1,  5,  6]],
       [[ 5,  9, 10],
        [ 5,  9, 10]]], dtype=uint8)>"
"tf.image.stateless_random_saturation(
    image, lower, upper, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_random_saturation
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
     [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
seed = (1, 2)
tf.image.stateless_random_saturation(x, 0.5, 1.0, seed)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 1.1559395,  2.0779698,  3.       ],
        [ 4.1559396,  5.07797  ,  6.       ]],
       [[ 7.1559396,  8.07797  ,  9.       ],
        [10.155939 , 11.07797  , 12.       ]]], dtype=float32)>"
"tf.image.stateless_sample_distorted_bounding_box(
    image_size,
    bounding_boxes,
    seed,
    min_object_covered=0.1,
    aspect_ratio_range=None,
    area_range=None,
    max_attempts=None,
    use_image_if_no_bounding_boxes=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_sample_distorted_bounding_box
image = np.array([[[1], [2], [3]], [[4], [5], [6]], [[7], [8], [9]]])
bbox = tf.constant(
  [0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])
seed = (1, 2)
bbox_begin, bbox_size, bbox_draw = (
  tf.image.stateless_sample_distorted_bounding_box(
    tf.shape(image), bounding_boxes=bbox, seed=seed))
tf.slice(image, bbox_begin, bbox_size)
<tf.Tensor: shape=(2, 2, 1), dtype=int64, numpy=
<tf.Tensor: shape=(2, 2, 1), dtype=int64, numpy=
array([[[1],
        [2]],
       [[4],
        [5]]])>
colors = np.array([[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]])
tf.image.draw_bounding_boxes(
  tf.expand_dims(tf.cast(image, tf.float32),0), bbox_draw, colors)
<tf.Tensor: shape=(1, 3, 3, 1), dtype=float32, numpy=
<tf.Tensor: shape=(1, 3, 3, 1), dtype=float32, numpy=
array([[[[1.],
         [1.],
         [3.]],
        [[1.],
         [1.],
         [6.]],
        [[7.],
         [8.],
         [9.]]]], dtype=float32)>"
"tf.image.transpose(
    image, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import transpose
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.transpose(x)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 1.,  2.,  3.],
        [ 7.,  8.,  9.]],
       [[ 4.,  5.,  6.],
        [10., 11., 12.]]], dtype=float32)>"
"tf.image.yuv_to_rgb(
    images
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import yuv_to_rgb
yuv_images = tf.random.uniform(shape=[100, 64, 64, 3], maxval=255)
last_dimension_axis = len(yuv_images.shape) - 1
yuv_tensor_images = tf.truediv(
    tf.subtract(
        yuv_images,
        tf.reduce_min(yuv_images)
    ),
    tf.subtract(
        tf.reduce_max(yuv_images),
        tf.reduce_min(yuv_images)
     )
)
y, u, v = tf.split(yuv_tensor_images, 3, axis=last_dimension_axis)
target_uv_min, target_uv_max = -0.5, 0.5
u = u * (target_uv_max - target_uv_min) + target_uv_min
v = v * (target_uv_max - target_uv_min) + target_uv_min
preprocessed_yuv_images = tf.concat([y, u, v], axis=last_dimension_axis)
rgb_tensor_images = tf.image.yuv_to_rgb(preprocessed_yuv_images)
"
"tf.keras.initializers.Constant(
    value=0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Constant
initializer = tf.keras.initializers.Constant(3.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.GlorotNormal(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GlorotNormal
initializer = tf.keras.initializers.GlorotNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.GlorotUniform(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GlorotUniform
initializer = tf.keras.initializers.GlorotUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.HeNormal(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import HeNormal
initializer = tf.keras.initializers.HeNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.HeUniform(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import HeUniform
initializer = tf.keras.initializers.HeUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.Identity(
    gain=1.0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Identity
initializer = tf.keras.initializers.Identity()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.LecunNormal(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LecunNormal
initializer = tf.keras.initializers.LecunNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.LecunUniform(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LecunUniform
initializer = tf.keras.initializers.LecunUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.Orthogonal(
    gain=1.0, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Orthogonal
initializer = tf.keras.initializers.Orthogonal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.RandomNormal(
    mean=0.0, stddev=0.05, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RandomNormal
initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.RandomUniform(
    minval=-0.05, maxval=0.05, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RandomUniform
initializer = tf.keras.initializers.RandomUniform(minval=0., maxval=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.TruncatedNormal(
    mean=0.0, stddev=0.05, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TruncatedNormal
initializer = tf.keras.initializers.TruncatedNormal(mean=0., stddev=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.VarianceScaling(
    scale=1.0,
    mode='fan_in',
    distribution='truncated_normal',
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import VarianceScaling
initializer = tf.keras.initializers.VarianceScaling(
scale=0.1, mode='fan_in', distribution='uniform')
values = initializer(shape=(2, 2))"
"tf.keras.initializers.Constant(
    value=0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Constant
initializer = tf.keras.initializers.Constant(3.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.GlorotNormal(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GlorotNormal
initializer = tf.keras.initializers.GlorotNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.GlorotUniform(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GlorotUniform
initializer = tf.keras.initializers.GlorotUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.HeNormal(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import HeNormal
initializer = tf.keras.initializers.HeNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.HeUniform(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import HeUniform
initializer = tf.keras.initializers.HeUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.Identity(
    gain=1.0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Identity
initializer = tf.keras.initializers.Identity()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.LecunNormal(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LecunNormal
initializer = tf.keras.initializers.LecunNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.LecunUniform(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LecunUniform
initializer = tf.keras.initializers.LecunUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.Orthogonal(
    gain=1.0, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Orthogonal
initializer = tf.keras.initializers.Orthogonal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.RandomNormal(
    mean=0.0, stddev=0.05, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RandomNormal
initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.RandomUniform(
    minval=-0.05, maxval=0.05, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RandomUniform
initializer = tf.keras.initializers.RandomUniform(minval=0., maxval=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.TruncatedNormal(
    mean=0.0, stddev=0.05, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TruncatedNormal
initializer = tf.keras.initializers.TruncatedNormal(mean=0., stddev=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.VarianceScaling(
    scale=1.0,
    mode='fan_in',
    distribution='truncated_normal',
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import VarianceScaling
initializer = tf.keras.initializers.VarianceScaling(
scale=0.1, mode='fan_in', distribution='uniform')
values = initializer(shape=(2, 2))"
"tf.io.decode_raw(
    input_bytes, out_type, little_endian=True, fixed_length=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import decode_raw
tf.io.decode_raw(tf.constant(""1""), tf.uint8)
<tf.Tensor: shape=(1,), dtype=uint8, numpy=array([49], dtype=uint8)>
<tf.Tensor: shape=(1,), dtype=uint8, numpy=array([49], dtype=uint8)>
tf.io.decode_raw(tf.constant(""1,2""), tf.uint8)
<tf.Tensor: shape=(3,), dtype=uint8, numpy=array([49, 44, 50], dtype=uint8)>
<tf.Tensor: shape=(3,), dtype=uint8, numpy=array([49, 44, 50], dtype=uint8)>"
"tf.io.gfile.GFile(
    name, mode='r'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GFile
with open(""/tmp/x"", ""w"") as f:
  f.write(""asdf"")
4
with tf.io.gfile.GFile(""/tmp/x"") as f:
  f.read()
'asdf'"
"tf.io.gfile.copy(
    src, dst, overwrite=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import copy
with open(""/tmp/x"", ""w"") as f:
  f.write(""asdf"")
4
tf.io.gfile.exists(""/tmp/x"")
True
tf.io.gfile.copy(""/tmp/x"", ""/tmp/y"")
tf.io.gfile.exists(""/tmp/y"")
True
tf.io.gfile.remove(""/tmp/y"")"
"tf.io.gfile.exists(
    path
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import exists
with open(""/tmp/x"", ""w"") as f:
  f.write(""asdf"")
4
tf.io.gfile.exists(""/tmp/x"")
True"
"tf.io.gfile.glob(
    pattern
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import glob
tf.io.gfile.glob(""*.py"")"
"tf.io.gfile.join(
    path, *paths
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import join
>>> tf.io.gfile.join(""gcs://folder"", ""file.py"")
'gcs://folder/file.py'
"
"tf.io.read_file(
    filename, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import read_file
with open(""/tmp/file.txt"", ""w"") as f:
  f.write(""asdf"")
4
tf.io.read_file(""/tmp/file.txt"")
<tf.Tensor: shape=(), dtype=string, numpy=b'asdf'>
<tf.Tensor: shape=(), dtype=string, numpy=b'asdf'>"
"tf.io.serialize_tensor(
    tensor, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import serialize_tensor
t = tf.constant(1)
tf.io.serialize_tensor(t)
<tf.Tensor: shape=(), dtype=string, numpy=b'\x08...\x00'>
<tf.Tensor: shape=(), dtype=string, numpy=b'\x08...\x00'>"
"tf.io.write_graph(
    graph_or_graph_def, logdir, name, as_text=True
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import write_graph
v = tf.Variable(0, name='my_variable')
sess = tf.compat.v1.Session()
tf.io.write_graph(sess.graph_def, '/tmp/my-model', 'train.pbtxt')
"
"tf.is_tensor(
    x
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import is_tensor
if not tf.is_tensor(t):
  t = tf.convert_to_tensor(t)
return t.shape, t.dtype
"
"tf.keras.Input(
    shape=None,
    batch_size=None,
    name=None,
    dtype=None,
    sparse=None,
    tensor=None,
    ragged=None,
    type_spec=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Input
x = Input(shape=(32,))
y = Dense(16, activation='softmax')(x)
model = Model(x, y)
"
"tf.keras.Model(
    *args, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Model
import tensorflow as tf

inputs = tf.keras.Input(shape=(3,))
x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)
outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)
"
"tf.keras.Sequential(
    layers=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Sequential
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8, input_shape=(16,)))
model.add(tf.keras.layers.Dense(4))

model = tf.keras.Sequential()
model.add(tf.keras.Input(shape=(16,)))
model.add(tf.keras.layers.Dense(8))

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8))
model.add(tf.keras.layers.Dense(4))

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8, input_shape=(16,)))
model.add(tf.keras.layers.Dense(4))
len(model.weights)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8))
model.add(tf.keras.layers.Dense(4))
model.build((None, 16))
len(model.weights)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8))
model.add(tf.keras.layers.Dense(1))
model.compile(optimizer='sgd', loss='mse')
model.fit(x, y, batch_size=32, epochs=10)
"
"tf.keras.activations.deserialize(
    name, custom_objects=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import deserialize
tf.keras.activations.deserialize('linear')
 <function linear at 0x1239596a8>
 <function linear at 0x1239596a8>
tf.keras.activations.deserialize('sigmoid')
 <function sigmoid at 0x123959510>
 <function sigmoid at 0x123959510>
tf.keras.activations.deserialize('abcd')
Traceback (most recent call last):
ValueError: Unknown activation function:abcd"
"tf.keras.activations.elu(
    x, alpha=1.0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
import tensorflow as tf
model = tf.keras.Sequential()
model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='elu',
         input_shape=(28, 28, 1)))
model.add(tf.keras.layers.MaxPooling2D((2, 2)))
model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='elu'))
model.add(tf.keras.layers.MaxPooling2D((2, 2)))
model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='elu'))"
"tf.keras.activations.exponential(
    x
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import exponential
a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)
b = tf.keras.activations.exponential(a)
b.numpy()
array([0.04978707,  0.36787945,  1.,  2.7182817 , 20.085537], dtype=float32)"
"tf.keras.activations.gelu(
    x, approximate=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import gelu
x = tf.constant([-3.0, -1.0, 0.0, 1.0, 3.0], dtype=tf.float32)
y = tf.keras.activations.gelu(x)
y.numpy()
array([-0.00404951, -0.15865529,  0.        ,  0.8413447 ,  2.9959507 ],
    dtype=float32)
y = tf.keras.activations.gelu(x, approximate=True)
y.numpy()
array([-0.00363752, -0.15880796,  0.        ,  0.841192  ,  2.9963627 ],
    dtype=float32)"
"tf.keras.activations.get(
    identifier
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get
tf.keras.activations.get('softmax')
 <function softmax at 0x1222a3d90>
 <function softmax at 0x1222a3d90>
tf.keras.activations.get(tf.keras.activations.softmax)
 <function softmax at 0x1222a3d90>
 <function softmax at 0x1222a3d90>
tf.keras.activations.get(None)
 <function linear at 0x1239596a8>
 <function linear at 0x1239596a8>
tf.keras.activations.get(abs)
 <built-in function abs>
 <built-in function abs>
tf.keras.activations.get('abcd')
Traceback (most recent call last):
ValueError: Unknown activation function:abcd"
"tf.keras.activations.hard_sigmoid(
    x
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import hard_sigmoid
a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)
b = tf.keras.activations.hard_sigmoid(a)
b.numpy()
array([0. , 0.3, 0.5, 0.7, 1. ], dtype=float32)"
"tf.keras.activations.linear(
    x
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import linear
a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)
b = tf.keras.activations.linear(a)
b.numpy()
array([-3., -1.,  0.,  1.,  3.], dtype=float32)"
"tf.keras.activations.relu(
    x, alpha=0.0, max_value=None, threshold=0.0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import relu
foo = tf.constant([-10, -5, 0.0, 5, 10], dtype = tf.float32)
tf.keras.activations.relu(foo).numpy()
array([ 0.,  0.,  0.,  5., 10.], dtype=float32)
tf.keras.activations.relu(foo, alpha=0.5).numpy()
array([-5. , -2.5,  0. ,  5. , 10. ], dtype=float32)
tf.keras.activations.relu(foo, max_value=5.).numpy()
array([0., 0., 0., 5., 5.], dtype=float32)
tf.keras.activations.relu(foo, threshold=5.).numpy()
array([-0., -0.,  0.,  0., 10.], dtype=float32)"
"tf.keras.activations.selu(
    x
)
","import pandas as pd
import numpy as np
import tensorflow as tf
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(64, kernel_initializer='lecun_normal',
                                activation='selu'))
model.add(tf.keras.layers.Dense(32, kernel_initializer='lecun_normal',
                                activation='selu'))
model.add(tf.keras.layers.Dense(16, kernel_initializer='lecun_normal',
                                activation='selu'))
model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))"
"tf.keras.activations.serialize(
    activation
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import serialize
tf.keras.activations.serialize(tf.keras.activations.tanh)
'tanh'
tf.keras.activations.serialize(tf.keras.activations.sigmoid)
'sigmoid'
tf.keras.activations.serialize('abcd')
Traceback (most recent call last):
ValueError: ('Cannot serialize', 'abcd')"
"tf.keras.activations.sigmoid(
    x
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sigmoid
a = tf.constant([-20, -1.0, 0.0, 1.0, 20], dtype = tf.float32)
b = tf.keras.activations.sigmoid(a)
b.numpy()
array([2.0611537e-09, 2.6894143e-01, 5.0000000e-01, 7.3105860e-01,
         1.0000000e+00], dtype=float32)"
"tf.keras.activations.softmax(
    x, axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import softmax
inputs = tf.random.normal(shape=(32, 10))
outputs = tf.keras.activations.softmax(inputs)
<tf.Tensor: shape=(), dtype=float32, numpy=1.0000001>
<tf.Tensor: shape=(), dtype=float32, numpy=1.0000001>"
"tf.keras.activations.softplus(
    x
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import softplus
a = tf.constant([-20, -1.0, 0.0, 1.0, 20], dtype = tf.float32)
b = tf.keras.activations.softplus(a)
b.numpy()
array([2.0611537e-09, 3.1326166e-01, 6.9314718e-01, 1.3132616e+00,
         2.0000000e+01], dtype=float32)"
"tf.keras.activations.softsign(
    x
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import softsign
a = tf.constant([-1.0, 0.0, 1.0], dtype = tf.float32)
b = tf.keras.activations.softsign(a)
b.numpy()
array([-0.5,  0. ,  0.5], dtype=float32)"
"tf.keras.activations.swish(
    x
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import swish
a = tf.constant([-20, -1.0, 0.0, 1.0, 20], dtype = tf.float32)
b = tf.keras.activations.swish(a)
b.numpy()
array([-4.1223075e-08, -2.6894143e-01,  0.0000000e+00,  7.3105860e-01,
          2.0000000e+01], dtype=float32)"
"tf.keras.activations.tanh(
    x
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import tanh
a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)
b = tf.keras.activations.tanh(a)
b.numpy()
array([-0.9950547, -0.7615942,  0.,  0.7615942,  0.9950547], dtype=float32)"
"tf.keras.applications.densenet.preprocess_input(
    x, data_format=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import preprocess_input
i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.imagenet_utils.preprocess_input(
    x, data_format=None, mode='caffe'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import preprocess_input
i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.inception_resnet_v2.preprocess_input(
    x, data_format=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import preprocess_input
i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.inception_v3.preprocess_input(
    x, data_format=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import preprocess_input
i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.mobilenet.preprocess_input(
    x, data_format=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import preprocess_input
i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.mobilenet_v2.preprocess_input(
    x, data_format=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import preprocess_input
i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.nasnet.preprocess_input(
    x, data_format=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import preprocess_input
i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.resnet50.preprocess_input(
    x, data_format=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import preprocess_input
i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.resnet50.preprocess_input(
    x, data_format=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import preprocess_input
i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.resnet_v2.preprocess_input(
    x, data_format=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import preprocess_input
i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.vgg16.preprocess_input(
    x, data_format=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import preprocess_input
i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.vgg19.preprocess_input(
    x, data_format=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import preprocess_input
i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.xception.preprocess_input(
    x, data_format=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import preprocess_input
i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.backend.get_uid(
    prefix=''
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get_uid
get_uid('dense')
1
get_uid('dense')
2"
"tf.keras.backend.is_keras_tensor(
    x
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import is_keras_tensor
np_var = np.array([1, 2])
tf.keras.backend.is_keras_tensor(np_var)
Traceback (most recent call last):
ValueError: Unexpectedly found an instance of type
`<class 'numpy.ndarray'>`.
`<class 'numpy.ndarray'>`.
Expected a symbolic tensor instance.
keras_var = tf.keras.backend.variable(np_var)
tf.keras.backend.is_keras_tensor(keras_var)
False
keras_placeholder = tf.keras.backend.placeholder(shape=(2, 4, 5))
tf.keras.backend.is_keras_tensor(keras_placeholder)
True
keras_input = tf.keras.layers.Input([10])
tf.keras.backend.is_keras_tensor(keras_input)
True
keras_layer_output = tf.keras.layers.Dense(10)(keras_input)
tf.keras.backend.is_keras_tensor(keras_layer_output)
True"
"tf.keras.backend.set_epsilon(
    value
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import set_epsilon
tf.keras.backend.epsilon()
1e-07
tf.keras.backend.set_epsilon(1e-5)
tf.keras.backend.epsilon()
1e-05
tf.keras.backend.set_epsilon(1e-7)"
"tf.keras.backend.set_floatx(
    value
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import set_floatx
tf.keras.backend.floatx()
'float32'
tf.keras.backend.set_floatx('float64')
tf.keras.backend.floatx()
'float64'
tf.keras.backend.set_floatx('float32')"
"tf.keras.backend.set_image_data_format(
    data_format
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import set_image_data_format
tf.keras.backend.image_data_format()
'channels_last'
tf.keras.backend.set_image_data_format('channels_first')
tf.keras.backend.image_data_format()
'channels_first'
tf.keras.backend.set_image_data_format('channels_last')"
"tf.keras.callbacks.BackupAndRestore(
    backup_dir,
    save_freq='epoch',
    delete_checkpoint=True,
    save_before_preemption=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import BackupAndRestore
class InterruptingCallback(tf.keras.callbacks.Callback):
  def on_epoch_begin(self, epoch, logs=None):
    if epoch == 4:
      raise RuntimeError('Interrupting!')
callback = tf.keras.callbacks.BackupAndRestore(backup_dir=""/tmp/backup"")
model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
model.compile(tf.keras.optimizers.SGD(), loss='mse')
try:
  model.fit(np.arange(100).reshape(5, 20), np.zeros(5), epochs=10,
            batch_size=1, callbacks=[callback, InterruptingCallback()],
            verbose=0)
except:
  pass
history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),
                    epochs=10, batch_size=1, callbacks=[callback],
                    verbose=0)
len(history.history['loss'])
6"
"tf.keras.callbacks.CSVLogger(
    filename, separator=',', append=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CSVLogger
csv_logger = CSVLogger('training.log')
model.fit(X_train, Y_train, callbacks=[csv_logger])
"
"tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    min_delta=0,
    patience=0,
    verbose=0,
    mode='auto',
    baseline=None,
    restore_best_weights=False,
    start_from_epoch=0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import EarlyStopping
callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)
model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
model.compile(tf.keras.optimizers.SGD(), loss='mse')
history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),
                    epochs=10, batch_size=1, callbacks=[callback],
                    verbose=0)
4"
"tf.keras.callbacks.LambdaCallback(
    on_epoch_begin=None,
    on_epoch_end=None,
    on_batch_begin=None,
    on_batch_end=None,
    on_train_begin=None,
    on_train_end=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LambdaCallback
batch_print_callback = LambdaCallback(
    on_batch_begin=lambda batch,logs: print(batch))

import json
json_log = open('loss_log.json', mode='wt', buffering=1)
json_logging_callback = LambdaCallback(
    on_epoch_end=lambda epoch, logs: json_log.write(
        json.dumps({'epoch': epoch, 'loss': logs['loss']}) + '\n'),
    on_train_end=lambda logs: json_log.close()
)

processes = ...
cleanup_callback = LambdaCallback(
    on_train_end=lambda logs: [
        p.terminate() for p in processes if p.is_alive()])

model.fit(...,
          callbacks=[batch_print_callback,
                     json_logging_callback,
                     cleanup_callback])
"
"tf.keras.callbacks.ModelCheckpoint(
    filepath,
    monitor: str = 'val_loss',
    verbose: int = 0,
    save_best_only: bool = False,
    save_weights_only: bool = False,
    mode: str = 'auto',
    save_freq='epoch',
    options=None,
    initial_value_threshold=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ModelCheckpoint
model.compile(loss=..., optimizer=...,
              metrics=['accuracy'])

EPOCHS = 10
checkpoint_filepath = '/tmp/checkpoint'
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=True,
    monitor='val_accuracy',
    mode='max',
    save_best_only=True)

model.fit(epochs=EPOCHS, callbacks=[model_checkpoint_callback])

model.load_weights(checkpoint_filepath)
"
"tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.1,
    patience=10,
    verbose=0,
    mode='auto',
    min_delta=0.0001,
    cooldown=0,
    min_lr=0,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ReduceLROnPlateau
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,
                              patience=5, min_lr=0.001)
model.fit(X_train, Y_train, callbacks=[reduce_lr])
"
"tf.keras.datasets.cifar100.load_data(
    label_mode='fine'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import load_data
(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()
assert x_train.shape == (50000, 32, 32, 3)
assert x_test.shape == (10000, 32, 32, 3)
assert y_train.shape == (50000, 1)
assert y_test.shape == (10000, 1)
"
"tf.keras.datasets.imdb.get_word_index(
    path='imdb_word_index.json'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get_word_index
start_char = 1
oov_char = 2
index_from = 3
(x_train, _), _ = keras.datasets.imdb.load_data(
    start_char=start_char, oov_char=oov_char, index_from=index_from
)
word_index = keras.datasets.imdb.get_word_index()
inverted_word_index = dict(
    (i + index_from, word) for (word, i) in word_index.items()
)
inverted_word_index[start_char] = ""[START]""
inverted_word_index[oov_char] = ""[OOV]""
decoded_sequence = "" "".join(inverted_word_index[i] for i in x_train[0])
"
"tf.keras.datasets.mnist.load_data(
    path='mnist.npz'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import load_data
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
assert x_train.shape == (60000, 28, 28)
assert x_test.shape == (10000, 28, 28)
assert y_train.shape == (60000,)
assert y_test.shape == (10000,)
"
"tf.keras.dtensor.experimental.LayoutMap(
    mesh=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LayoutMap
map = LayoutMap(mesh=None)
map['.*dense.*kernel'] = layout_2d
map['.*dense.*bias'] = layout_1d
map['.*conv2d.*kernel'] = layout_4d
map['.*conv2d.*bias'] = layout_1d

"
"tf.keras.dtensor.experimental.optimizers.RMSprop(
    learning_rate=0.001,
    rho=0.9,
    momentum=0.0,
    epsilon=1e-07,
    centered=False,
    gradients_clip_option=None,
    ema_option=None,
    jit_compile=False,
    name='RMSprop',
    mesh=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RMSprop
opt = tf.keras.optimizers.experimental.RMSprop(learning_rate=0.1)
var1 = tf.Variable(10.0)
opt.minimize(loss, [var1])
var1.numpy()
9.683772"
"tf.keras.estimator.model_to_estimator(
    keras_model=None,
    keras_model_path=None,
    custom_objects=None,
    model_dir=None,
    config=None,
    checkpoint_format='checkpoint',
    metric_names_map=None,
    export_outputs=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import model_to_estimator
keras_model = tf.keras.Model(...)
keras_model.compile(...)

estimator = tf.keras.estimator.model_to_estimator(keras_model)

def input_fn():
  return dataset_ops.Dataset.from_tensors(
      ({'features': features, 'sample_weights': sample_weights},
       targets))

estimator.train(input_fn, steps=1)
"
"tf.keras.optimizers.schedules.CosineDecayRestarts(
    initial_learning_rate,
    first_decay_steps,
    t_mul=2.0,
    m_mul=1.0,
    alpha=0.0,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CosineDecayRestarts
first_decay_steps = 1000
lr_decayed_fn = (
  tf.keras.optimizers.schedules.CosineDecayRestarts(
      initial_learning_rate,
      first_decay_steps))
"
"tf.keras.experimental.LinearModel(
    units=1,
    activation=None,
    use_bias=True,
    kernel_initializer='zeros',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearModel
model = LinearModel()
model.compile(optimizer='sgd', loss='mse')
model.fit(x, y, epochs=epochs)
"
"tf.keras.experimental.SequenceFeatures(
    feature_columns, trainable=True, name=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SequenceFeatures

import tensorflow as tf

training = True
rating = tf.feature_column.sequence_numeric_column('rating')
watches = tf.feature_column.sequence_categorical_column_with_identity(
    'watches', num_buckets=1000)
watches_embedding = tf.feature_column.embedding_column(watches,
                                            dimension=10)
columns = [rating, watches_embedding]

features = {
 'rating': tf.sparse.from_dense([[1.0,1.1, 0, 0, 0],
                                             [2.0,2.1,2.2, 2.3, 2.5]]),
 'watches': tf.sparse.from_dense([[2, 85, 0, 0, 0],[33,78, 2, 73, 1]])
}

sequence_input_layer = tf.keras.experimental.SequenceFeatures(columns)
sequence_input, sequence_length = sequence_input_layer(
   features, training=training)
sequence_length_mask = tf.sequence_mask(sequence_length)
hidden_size = 32
rnn_cell = tf.keras.layers.SimpleRNNCell(hidden_size)
rnn_layer = tf.keras.layers.RNN(rnn_cell)
outputs, state = rnn_layer(sequence_input, mask=sequence_length_mask)
"
"tf.keras.experimental.WideDeepModel(
    linear_model, dnn_model, activation=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import WideDeepModel
linear_model = LinearModel()
dnn_model = keras.Sequential([keras.layers.Dense(units=64),
                             keras.layers.Dense(units=1)])
combined_model = WideDeepModel(linear_model, dnn_model)
combined_model.compile(optimizer=['sgd', 'adam'],
                       loss='mse', metrics=['mse'])
combined_model.fit([linear_inputs, dnn_inputs], y, epochs)
dataset = tf.data.Dataset.from_tensors(([linear_inputs, dnn_inputs], y))
combined_model.fit(dataset, epochs)
"
"tf.keras.initializers.Constant(
    value=0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Constant
initializer = tf.keras.initializers.Constant(3.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.GlorotNormal(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GlorotNormal
initializer = tf.keras.initializers.GlorotNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.GlorotUniform(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GlorotUniform
initializer = tf.keras.initializers.GlorotUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.HeNormal(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import HeNormal
initializer = tf.keras.initializers.HeNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.HeUniform(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import HeUniform
initializer = tf.keras.initializers.HeUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.Identity(
    gain=1.0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Identity
initializer = tf.keras.initializers.Identity()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.LecunNormal(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LecunNormal
initializer = tf.keras.initializers.LecunNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.LecunUniform(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LecunUniform
initializer = tf.keras.initializers.LecunUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.Orthogonal(
    gain=1.0, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Orthogonal
initializer = tf.keras.initializers.Orthogonal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.RandomNormal(
    mean=0.0, stddev=0.05, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RandomNormal
initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.RandomUniform(
    minval=-0.05, maxval=0.05, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RandomUniform
initializer = tf.keras.initializers.RandomUniform(minval=0., maxval=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.TruncatedNormal(
    mean=0.0, stddev=0.05, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TruncatedNormal
initializer = tf.keras.initializers.TruncatedNormal(mean=0., stddev=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.VarianceScaling(
    scale=1.0,
    mode='fan_in',
    distribution='truncated_normal',
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import VarianceScaling
initializer = tf.keras.initializers.VarianceScaling(
scale=0.1, mode='fan_in', distribution='uniform')
values = initializer(shape=(2, 2))"
"tf.keras.initializers.Constant(
    value=0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Constant
initializer = tf.keras.initializers.Constant(3.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.GlorotNormal(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GlorotNormal
initializer = tf.keras.initializers.GlorotNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.GlorotUniform(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GlorotUniform
initializer = tf.keras.initializers.GlorotUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.HeNormal(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import HeNormal
initializer = tf.keras.initializers.HeNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.HeUniform(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import HeUniform
initializer = tf.keras.initializers.HeUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.Identity(
    gain=1.0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Identity
initializer = tf.keras.initializers.Identity()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.LecunNormal(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LecunNormal
initializer = tf.keras.initializers.LecunNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.LecunUniform(
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LecunUniform
initializer = tf.keras.initializers.LecunUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.Orthogonal(
    gain=1.0, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Orthogonal
initializer = tf.keras.initializers.Orthogonal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.RandomNormal(
    mean=0.0, stddev=0.05, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RandomNormal
initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.RandomUniform(
    minval=-0.05, maxval=0.05, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RandomUniform
initializer = tf.keras.initializers.RandomUniform(minval=0., maxval=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.TruncatedNormal(
    mean=0.0, stddev=0.05, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TruncatedNormal
initializer = tf.keras.initializers.TruncatedNormal(mean=0., stddev=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.VarianceScaling(
    scale=1.0,
    mode='fan_in',
    distribution='truncated_normal',
    seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import VarianceScaling
initializer = tf.keras.initializers.VarianceScaling(
scale=0.1, mode='fan_in', distribution='uniform')
values = initializer(shape=(2, 2))"
"tf.keras.layers.AbstractRNNCell(
    trainable=True, name=None, dtype=None, dynamic=False, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
  class MinimalRNNCell(AbstractRNNCell):

    def __init__(self, units, **kwargs):
      self.units = units
      super(MinimalRNNCell, self).__init__(**kwargs)

    @property
    def state_size(self):
      return self.units

    def build(self, input_shape):
      self.kernel = self.add_weight(shape=(input_shape[-1], self.units),
                                    initializer='uniform',
                                    name='kernel')
      self.recurrent_kernel = self.add_weight(
          shape=(self.units, self.units),
          initializer='uniform',
          name='recurrent_kernel')
      self.built = True

    def call(self, inputs, states):
      prev_output = states[0]
      h = backend.dot(inputs, self.kernel)
      output = h + backend.dot(prev_output, self.recurrent_kernel)
      return output, output
"
"tf.keras.layers.Activation(
    activation, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Activation
layer = tf.keras.layers.Activation('relu')
output = layer([-3.0, -1.0, 0.0, 2.0])
list(output.numpy())
[0.0, 0.0, 0.0, 2.0]
layer = tf.keras.layers.Activation(tf.nn.relu)
output = layer([-3.0, -1.0, 0.0, 2.0])
list(output.numpy())
[0.0, 0.0, 0.0, 2.0]"
"tf.keras.layers.Add(
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Add
input_shape = (2, 3, 4)
x1 = tf.random.normal(input_shape)
x2 = tf.random.normal(input_shape)
y = tf.keras.layers.Add()([x1, x2])
print(y.shape)
(2, 3, 4)"
"tf.keras.layers.AdditiveAttention(
    use_scale=True, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import AdditiveAttention
query_input = tf.keras.Input(shape=(None,), dtype='int32')
value_input = tf.keras.Input(shape=(None,), dtype='int32')

token_embedding = tf.keras.layers.Embedding(max_tokens, dimension)
query_embeddings = token_embedding(query_input)
value_embeddings = token_embedding(value_input)

cnn_layer = tf.keras.layers.Conv1D(
    filters=100,
    kernel_size=4,
    padding='same')
query_seq_encoding = cnn_layer(query_embeddings)
value_seq_encoding = cnn_layer(value_embeddings)

query_value_attention_seq = tf.keras.layers.AdditiveAttention()(
    [query_seq_encoding, value_seq_encoding])

query_encoding = tf.keras.layers.GlobalAveragePooling1D()(
    query_seq_encoding)
query_value_attention = tf.keras.layers.GlobalAveragePooling1D()(
    query_value_attention_seq)

input_layer = tf.keras.layers.Concatenate()(
    [query_encoding, query_value_attention])

"
"tf.keras.layers.Attention(
    use_scale=False, score_mode='dot', **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Attention
query_input = tf.keras.Input(shape=(None,), dtype='int32')
value_input = tf.keras.Input(shape=(None,), dtype='int32')

token_embedding = tf.keras.layers.Embedding(input_dim=1000, output_dim=64)
query_embeddings = token_embedding(query_input)
value_embeddings = token_embedding(value_input)

cnn_layer = tf.keras.layers.Conv1D(
    filters=100,
    kernel_size=4,
    padding='same')
query_seq_encoding = cnn_layer(query_embeddings)
value_seq_encoding = cnn_layer(value_embeddings)

query_value_attention_seq = tf.keras.layers.Attention()(
    [query_seq_encoding, value_seq_encoding])

query_encoding = tf.keras.layers.GlobalAveragePooling1D()(
    query_seq_encoding)
query_value_attention = tf.keras.layers.GlobalAveragePooling1D()(
    query_value_attention_seq)

input_layer = tf.keras.layers.Concatenate()(
    [query_encoding, query_value_attention])

"
"tf.keras.layers.Average(
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Average
x1 = np.ones((2, 2))
x2 = np.zeros((2, 2))
y = tf.keras.layers.Average()([x1, x2])
y.numpy().tolist()
[[0.5, 0.5], [0.5, 0.5]]"
"tf.keras.layers.AveragePooling1D(
    pool_size=2,
    strides=None,
    padding='valid',
    data_format='channels_last',
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import AveragePooling1D
x = tf.constant([1., 2., 3., 4., 5.])
x = tf.reshape(x, [1, 5, 1])
x
<tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=
<tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=
  array([[[1.],
          [2.],
          [3.],
          [4.],
          [5.]], dtype=float32)>
avg_pool_1d = tf.keras.layers.AveragePooling1D(pool_size=2,
   strides=1, padding='valid')
avg_pool_1d(x)
<tf.Tensor: shape=(1, 4, 1), dtype=float32, numpy=
<tf.Tensor: shape=(1, 4, 1), dtype=float32, numpy=
array([[[1.5],
        [2.5],
        [3.5],
        [4.5]]], dtype=float32)>"
"tf.keras.layers.AveragePooling2D(
    pool_size=(2, 2),
    strides=None,
    padding='valid',
    data_format=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import AveragePooling2D
x = tf.constant([[1., 2., 3.],
                 [4., 5., 6.],
                 [7., 8., 9.]])
x = tf.reshape(x, [1, 3, 3, 1])
avg_pool_2d = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),
   strides=(1, 1), padding='valid')
avg_pool_2d(x)
<tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=
<tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=
  array([[[[3.],
           [4.]],
          [[6.],
           [7.]]]], dtype=float32)>"
"tf.keras.layers.AveragePooling3D(
    pool_size=(2, 2, 2),
    strides=None,
    padding='valid',
    data_format=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import AveragePooling3D
depth = 30
height = 30
width = 30
input_channels = 3

inputs = tf.keras.Input(shape=(depth, height, width, input_channels))
layer = tf.keras.layers.AveragePooling3D(pool_size=3)
"
"tf.keras.layers.AveragePooling1D(
    pool_size=2,
    strides=None,
    padding='valid',
    data_format='channels_last',
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import AveragePooling1D
x = tf.constant([1., 2., 3., 4., 5.])
x = tf.reshape(x, [1, 5, 1])
x
<tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=
<tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=
  array([[[1.],
          [2.],
          [3.],
          [4.],
          [5.]], dtype=float32)>
avg_pool_1d = tf.keras.layers.AveragePooling1D(pool_size=2,
   strides=1, padding='valid')
avg_pool_1d(x)
<tf.Tensor: shape=(1, 4, 1), dtype=float32, numpy=
<tf.Tensor: shape=(1, 4, 1), dtype=float32, numpy=
array([[[1.5],
        [2.5],
        [3.5],
        [4.5]]], dtype=float32)>"
"tf.keras.layers.AveragePooling2D(
    pool_size=(2, 2),
    strides=None,
    padding='valid',
    data_format=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import AveragePooling2D
x = tf.constant([[1., 2., 3.],
                 [4., 5., 6.],
                 [7., 8., 9.]])
x = tf.reshape(x, [1, 3, 3, 1])
avg_pool_2d = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),
   strides=(1, 1), padding='valid')
avg_pool_2d(x)
<tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=
<tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=
  array([[[[3.],
           [4.]],
          [[6.],
           [7.]]]], dtype=float32)>"
"tf.keras.layers.AveragePooling3D(
    pool_size=(2, 2, 2),
    strides=None,
    padding='valid',
    data_format=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import AveragePooling3D
depth = 30
height = 30
width = 30
input_channels = 3

inputs = tf.keras.Input(shape=(depth, height, width, input_channels))
layer = tf.keras.layers.AveragePooling3D(pool_size=3)
"
"tf.keras.layers.Bidirectional(
    layer,
    merge_mode='concat',
    weights=None,
    backward_layer=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Bidirectional
model = Sequential()
model.add(Bidirectional(LSTM(10, return_sequences=True),
                             input_shape=(5, 10)))
model.add(Bidirectional(LSTM(10)))
model.add(Dense(5))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

model = Sequential()
forward_layer = LSTM(10, return_sequences=True)
backward_layer = LSTM(10, activation='relu', return_sequences=True,
                      go_backwards=True)
model.add(Bidirectional(forward_layer, backward_layer=backward_layer,
                        input_shape=(5, 10)))
model.add(Dense(5))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
"
"tf.keras.layers.CategoryEncoding(
    num_tokens=None, output_mode='multi_hot', sparse=False, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CategoryEncoding
layer = tf.keras.layers.CategoryEncoding(
          num_tokens=4, output_mode=""one_hot"")
layer([3, 2, 0, 1])
<tf.Tensor: shape=(4, 4), dtype=float32, numpy=
<tf.Tensor: shape=(4, 4), dtype=float32, numpy=
  array([[0., 0., 0., 1.],
         [0., 0., 1., 0.],
         [1., 0., 0., 0.],
         [0., 1., 0., 0.]], dtype=float32)>"
"tf.keras.layers.Concatenate(
    axis=-1, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Concatenate
x = np.arange(20).reshape(2, 2, 5)
print(x)
[[[ 0  1  2  3  4]
  [ 5  6  7  8  9]]
 [[10 11 12 13 14]
  [15 16 17 18 19]]]
y = np.arange(20, 30).reshape(2, 1, 5)
print(y)
[[[20 21 22 23 24]]
 [[25 26 27 28 29]]]
tf.keras.layers.Concatenate(axis=1)([x, y])
<tf.Tensor: shape=(2, 3, 5), dtype=int64, numpy=
<tf.Tensor: shape=(2, 3, 5), dtype=int64, numpy=
array([[[ 0,  1,  2,  3,  4],
        [ 5,  6,  7,  8,  9],
        [20, 21, 22, 23, 24]],
       [[10, 11, 12, 13, 14],
        [15, 16, 17, 18, 19],
        [25, 26, 27, 28, 29]]])>"
"tf.keras.layers.Conv1D(
    filters,
    kernel_size,
    strides=1,
    padding='valid',
    data_format='channels_last',
    dilation_rate=1,
    groups=1,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Conv1D
input_shape = (4, 10, 128)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv1D(
32, 3, activation='relu',input_shape=input_shape[1:])(x)
print(y.shape)
(4, 8, 32)"
"tf.keras.layers.Conv2D(
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    data_format=None,
    dilation_rate=(1, 1),
    groups=1,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Conv2D
input_shape = (4, 28, 28, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv2D(
2, 3, activation='relu', input_shape=input_shape[1:])(x)
print(y.shape)
(4, 26, 26, 2)"
"tf.keras.layers.Conv3D(
    filters,
    kernel_size,
    strides=(1, 1, 1),
    padding='valid',
    data_format=None,
    dilation_rate=(1, 1, 1),
    groups=1,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Conv3D
input_shape =(4, 28, 28, 28, 1)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv3D(
2, 3, activation='relu', input_shape=input_shape[1:])(x)
print(y.shape)
(4, 26, 26, 26, 2)"
"tf.keras.layers.Conv1D(
    filters,
    kernel_size,
    strides=1,
    padding='valid',
    data_format='channels_last',
    dilation_rate=1,
    groups=1,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Conv1D
input_shape = (4, 10, 128)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv1D(
32, 3, activation='relu',input_shape=input_shape[1:])(x)
print(y.shape)
(4, 8, 32)"
"tf.keras.layers.Conv2D(
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    data_format=None,
    dilation_rate=(1, 1),
    groups=1,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Conv2D
input_shape = (4, 28, 28, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv2D(
2, 3, activation='relu', input_shape=input_shape[1:])(x)
print(y.shape)
(4, 26, 26, 2)"
"tf.keras.layers.Conv3D(
    filters,
    kernel_size,
    strides=(1, 1, 1),
    padding='valid',
    data_format=None,
    dilation_rate=(1, 1, 1),
    groups=1,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Conv3D
input_shape =(4, 28, 28, 28, 1)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv3D(
2, 3, activation='relu', input_shape=input_shape[1:])(x)
print(y.shape)
(4, 26, 26, 26, 2)"
"tf.keras.layers.Cropping1D(
    cropping=(1, 1), **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Cropping1D
input_shape = (2, 3, 2)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
print(x)
[[[ 0  1]
  [ 2  3]
  [ 4  5]]
 [[ 6  7]
  [ 8  9]
  [10 11]]]
y = tf.keras.layers.Cropping1D(cropping=1)(x)
print(y)
tf.Tensor(
  [[[2 3]]
   [[8 9]]], shape=(2, 1, 2), dtype=int64)"
"tf.keras.layers.Cropping2D(
    cropping=((0, 0), (0, 0)), data_format=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Cropping2D
input_shape = (2, 28, 28, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
y = tf.keras.layers.Cropping2D(cropping=((2, 2), (4, 4)))(x)
print(y.shape)
(2, 24, 20, 3)"
"tf.keras.layers.Cropping3D(
    cropping=((1, 1), (1, 1), (1, 1)), data_format=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Cropping3D
input_shape = (2, 28, 28, 10, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
y = tf.keras.layers.Cropping3D(cropping=(2, 4, 2))(x)
print(y.shape)
(2, 24, 20, 6, 3)"
"tf.keras.layers.Dense(
    units,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Dense
model = tf.keras.models.Sequential()
model.add(tf.keras.Input(shape=(16,)))
model.add(tf.keras.layers.Dense(32, activation='relu'))
model.add(tf.keras.layers.Dense(32))
model.output_shape
(None, 32)"
"tf.keras.layers.DenseFeatures(
    feature_columns, trainable=True, name=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import DenseFeatures
price = tf.feature_column.numeric_column('price')
keywords_embedded = tf.feature_column.embedding_column(
    tf.feature_column.categorical_column_with_hash_bucket(""keywords"",
                                                          10000),
    dimensions=16)
columns = [price, keywords_embedded, ...]
feature_layer = tf.keras.layers.DenseFeatures(columns)

features = tf.io.parse_example(
    ..., features=tf.feature_column.make_parse_example_spec(columns))
dense_tensor = feature_layer(features)
for units in [128, 64, 32]:
  dense_tensor = tf.keras.layers.Dense(units, activation='relu')(
    dense_tensor)
prediction = tf.keras.layers.Dense(1)(dense_tensor)
"
"tf.keras.layers.Discretization(
    bin_boundaries=None,
    num_bins=None,
    epsilon=0.01,
    output_mode='int',
    sparse=False,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Discretization
>>> input = np.array([[-1.5, 1.0, 3.4, .5], [0.0, 3.0, 1.3, 0.0]])
>>> layer = tf.keras.layers.Discretization(bin_boundaries=[0., 1., 2.])
>>> layer(input)
<tf.Tensor: shape=(2, 4), dtype=int64, numpy=
<tf.Tensor: shape=(2, 4), dtype=int64, numpy=
array([[0, 2, 3, 1],
       [1, 3, 2, 1]])>
"
"tf.keras.layers.Dot(
    axes, normalize=False, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Dot
x = np.arange(10).reshape(1, 5, 2)
print(x)
[[[0 1]
  [2 3]
  [4 5]
  [6 7]
  [8 9]]]
y = np.arange(10, 20).reshape(1, 2, 5)
print(y)
[[[10 11 12 13 14]
  [15 16 17 18 19]]]
tf.keras.layers.Dot(axes=(1, 2))([x, y])
<tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=
<tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=
array([[[260, 360],
        [320, 445]]])>"
"tf.keras.layers.Dropout(
    rate, noise_shape=None, seed=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Dropout
tf.random.set_seed(0)
layer = tf.keras.layers.Dropout(.2, input_shape=(2,))
data = np.arange(10).reshape(5, 2).astype(np.float32)
print(data)
[[0. 1.]
 [2. 3.]
 [4. 5.]
 [6. 7.]
 [8. 9.]]
outputs = layer(data, training=True)
print(outputs)
tf.Tensor(
[[ 0.    1.25]
 [ 2.5   3.75]
 [ 5.    6.25]
 [ 7.5   8.75]
 [10.    0.  ]], shape=(5, 2), dtype=float32)"
"tf.keras.layers.EinsumDense(
    equation,
    output_shape,
    activation=None,
    bias_axes=None,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import EinsumDense
layer = tf.keras.layers.EinsumDense(""ab,bc->ac"",
                                    output_shape=64,
                                    bias_axes=""c"")
input_tensor = tf.keras.Input(shape=[32])
output_tensor = layer(input_tensor)
output_tensor
<... shape=(None, 64) dtype=...>
<... shape=(None, 64) dtype=...>"
"tf.keras.layers.Embedding(
    input_dim,
    output_dim,
    embeddings_initializer='uniform',
    embeddings_regularizer=None,
    activity_regularizer=None,
    embeddings_constraint=None,
    mask_zero=False,
    input_length=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Embedding
model = tf.keras.Sequential()
model.add(tf.keras.layers.Embedding(1000, 64, input_length=10))
input_array = np.random.randint(1000, size=(32, 10))
model.compile('rmsprop', 'mse')
output_array = model.predict(input_array)
print(output_array.shape)
(32, 10, 64)"
"tf.keras.layers.GRU(
    units,
    activation='tanh',
    recurrent_activation='sigmoid',
    use_bias=True,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros',
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    dropout=0.0,
    recurrent_dropout=0.0,
    return_sequences=False,
    return_state=False,
    go_backwards=False,
    stateful=False,
    unroll=False,
    time_major=False,
    reset_after=True,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GRU
inputs = tf.random.normal([32, 10, 8])
gru = tf.keras.layers.GRU(4)
output = gru(inputs)
print(output.shape)
(32, 4)
gru = tf.keras.layers.GRU(4, return_sequences=True, return_state=True)
whole_sequence_output, final_state = gru(inputs)
print(whole_sequence_output.shape)
(32, 10, 4)
print(final_state.shape)
(32, 4)"
"tf.keras.layers.GRUCell(
    units,
    activation='tanh',
    recurrent_activation='sigmoid',
    use_bias=True,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros',
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    dropout=0.0,
    recurrent_dropout=0.0,
    reset_after=True,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GRUCell
inputs = tf.random.normal([32, 10, 8])
rnn = tf.keras.layers.RNN(tf.keras.layers.GRUCell(4))
output = rnn(inputs)
print(output.shape)
(32, 4)
rnn = tf.keras.layers.RNN(
   tf.keras.layers.GRUCell(4),
   return_sequences=True,
   return_state=True)
whole_sequence_output, final_state = rnn(inputs)
print(whole_sequence_output.shape)
(32, 10, 4)
print(final_state.shape)
(32, 4)"
"tf.keras.layers.GlobalAveragePooling1D(
    data_format='channels_last', **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GlobalAveragePooling1D
input_shape = (2, 3, 4)
x = tf.random.normal(input_shape)
y = tf.keras.layers.GlobalAveragePooling1D()(x)
print(y.shape)
(2, 4)"
"tf.keras.layers.GlobalAveragePooling2D(
    data_format=None, keepdims=False, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GlobalAveragePooling2D
input_shape = (2, 4, 5, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.GlobalAveragePooling2D()(x)
print(y.shape)
(2, 3)"
"tf.keras.layers.GlobalAveragePooling1D(
    data_format='channels_last', **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GlobalAveragePooling1D
input_shape = (2, 3, 4)
x = tf.random.normal(input_shape)
y = tf.keras.layers.GlobalAveragePooling1D()(x)
print(y.shape)
(2, 4)"
"tf.keras.layers.GlobalAveragePooling2D(
    data_format=None, keepdims=False, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GlobalAveragePooling2D
input_shape = (2, 4, 5, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.GlobalAveragePooling2D()(x)
print(y.shape)
(2, 3)"
"tf.keras.layers.GlobalMaxPool2D(
    data_format=None, keepdims=False, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GlobalMaxPool2D
input_shape = (2, 4, 5, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.GlobalMaxPool2D()(x)
print(y.shape)
(2, 3)"
"tf.keras.layers.GlobalMaxPool2D(
    data_format=None, keepdims=False, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import GlobalMaxPool2D
input_shape = (2, 4, 5, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.GlobalMaxPool2D()(x)
print(y.shape)
(2, 3)"
"tf.keras.layers.Hashing(
    num_bins,
    mask_value=None,
    salt=None,
    output_mode='int',
    sparse=False,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Hashing
layer = tf.keras.layers.Hashing(num_bins=3)
inp = [['A'], ['B'], ['C'], ['D'], ['E']]
layer(inp)
<tf.Tensor: shape=(5, 1), dtype=int64, numpy=
<tf.Tensor: shape=(5, 1), dtype=int64, numpy=
  array([[1],
         [0],
         [1],
         [1],
         [2]])>"
"tf.keras.Input(
    shape=None,
    batch_size=None,
    name=None,
    dtype=None,
    sparse=None,
    tensor=None,
    ragged=None,
    type_spec=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Input
x = Input(shape=(32,))
y = Dense(16, activation='softmax')(x)
model = Model(x, y)
"
"tf.keras.layers.InputLayer(
    input_shape=None,
    batch_size=None,
    dtype=None,
    input_tensor=None,
    sparse=None,
    name=None,
    ragged=None,
    type_spec=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import InputLayer
model = tf.keras.Sequential([
  tf.keras.layers.InputLayer(input_shape=(4,)),
  tf.keras.layers.Dense(8)])
model.compile(tf.keras.optimizers.RMSprop(0.001), loss='mse')
model.fit(np.zeros((10, 4)),
          np.ones((10, 8)))

model = tf.keras.Sequential([
  tf.keras.layers.Dense(8, input_shape=(4,))])
model.compile(tf.keras.optimizers.RMSprop(0.001), loss='mse')
model.fit(np.zeros((10, 4)),
          np.ones((10, 8)))
"
"tf.keras.layers.InputSpec(
    dtype=None,
    shape=None,
    ndim=None,
    max_ndim=None,
    min_ndim=None,
    axes=None,
    allow_last_axis_squeeze=False,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import InputSpec
class MyLayer(Layer):
    def __init__(self):
        super(MyLayer, self).__init__()
        self.input_spec = InputSpec(
            shape=(None, 28, 28, 1),
            allow_last_axis_squeeze=True)
"
"tf.keras.layers.IntegerLookup(
    max_tokens=None,
    num_oov_indices=1,
    mask_token=None,
    oov_token=-1,
    vocabulary=None,
    vocabulary_dtype='int64',
    idf_weights=None,
    invert=False,
    output_mode='int',
    sparse=False,
    pad_to_max_tokens=False,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import IntegerLookup
vocab = [12, 36, 1138, 42]
layer = tf.keras.layers.IntegerLookup(vocabulary=vocab)
layer(data)
<tf.Tensor: shape=(2, 3), dtype=int64, numpy=
<tf.Tensor: shape=(2, 3), dtype=int64, numpy=
array([[1, 3, 4],
       [4, 0, 2]])>"
"tf.keras.layers.LSTM(
    units,
    activation='tanh',
    recurrent_activation='sigmoid',
    use_bias=True,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros',
    unit_forget_bias=True,
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    dropout=0.0,
    recurrent_dropout=0.0,
    return_sequences=False,
    return_state=False,
    go_backwards=False,
    stateful=False,
    time_major=False,
    unroll=False,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LSTM
inputs = tf.random.normal([32, 10, 8])
lstm = tf.keras.layers.LSTM(4)
output = lstm(inputs)
print(output.shape)
(32, 4)
lstm = tf.keras.layers.LSTM(4, return_sequences=True, return_state=True)
whole_seq_output, final_memory_state, final_carry_state = lstm(inputs)
print(whole_seq_output.shape)
(32, 10, 4)
print(final_memory_state.shape)
(32, 4)
print(final_carry_state.shape)
(32, 4)"
"tf.keras.layers.LSTMCell(
    units,
    activation='tanh',
    recurrent_activation='sigmoid',
    use_bias=True,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros',
    unit_forget_bias=True,
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    dropout=0.0,
    recurrent_dropout=0.0,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LSTMCell
inputs = tf.random.normal([32, 10, 8])
rnn = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(4))
output = rnn(inputs)
print(output.shape)
(32, 4)
rnn = tf.keras.layers.RNN(
   tf.keras.layers.LSTMCell(4),
   return_sequences=True,
   return_state=True)
whole_seq_output, final_memory_state, final_carry_state = rnn(inputs)
print(whole_seq_output.shape)
(32, 10, 4)
print(final_memory_state.shape)
(32, 4)
print(final_carry_state.shape)
(32, 4)"
"tf.keras.layers.Lambda(
    function, output_shape=None, mask=None, arguments=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Lambda
model.add(Lambda(lambda x: x ** 2))
"
"tf.keras.layers.Layer(
    trainable=True, name=None, dtype=None, dynamic=False, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
class SimpleDense(Layer):

  def __init__(self, units=32):
      super(SimpleDense, self).__init__()
      self.units = units

    w_init = tf.random_normal_initializer()
    self.w = tf.Variable(
        initial_value=w_init(shape=(input_shape[-1], self.units),
                             dtype='float32'),
        trainable=True)
    b_init = tf.zeros_initializer()
    self.b = tf.Variable(
        initial_value=b_init(shape=(self.units,), dtype='float32'),
        trainable=True)

      return tf.matmul(inputs, self.w) + self.b

linear_layer = SimpleDense(4)

y = linear_layer(tf.ones((2, 2)))
assert len(linear_layer.weights) == 2

assert len(linear_layer.trainable_weights) == 2
"
"tf.keras.layers.LocallyConnected1D(
    filters,
    kernel_size,
    strides=1,
    padding='valid',
    data_format=None,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    implementation=1,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LocallyConnected1D
    model = Sequential()
    model.add(LocallyConnected1D(64, 3, input_shape=(10, 32)))
    model.add(LocallyConnected1D(32, 3))
"
"tf.keras.layers.LocallyConnected2D(
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    data_format=None,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    implementation=1,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LocallyConnected2D
    32x32 image
    model = Sequential()
    model.add(LocallyConnected2D(64, (3, 3), input_shape=(32, 32, 3)))
    parameters

    model.add(LocallyConnected2D(32, (3, 3)))
"
"tf.keras.layers.Masking(
    mask_value=0.0, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Masking
samples, timesteps, features = 32, 10, 8
inputs = np.random.random([samples, timesteps, features]).astype(np.float32)
inputs[:, 3, :] = 0.
inputs[:, 5, :] = 0.

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Masking(mask_value=0.,
                                  input_shape=(timesteps, features)))
model.add(tf.keras.layers.LSTM(32))

output = model(inputs)
"
"tf.keras.layers.Maximum(
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Maximum
tf.keras.layers.Maximum()([np.arange(5).reshape(5, 1),
                           np.arange(5, 10).reshape(5, 1)])
<tf.Tensor: shape=(5, 1), dtype=int64, numpy=
<tf.Tensor: shape=(5, 1), dtype=int64, numpy=
array([[5],
     [6],
     [7],
     [8],
     [9]])>"
"tf.keras.layers.Minimum(
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Minimum
tf.keras.layers.Minimum()([np.arange(5).reshape(5, 1),
                           np.arange(5, 10).reshape(5, 1)])
<tf.Tensor: shape=(5, 1), dtype=int64, numpy=
<tf.Tensor: shape=(5, 1), dtype=int64, numpy=
array([[0],
     [1],
     [2],
     [3],
     [4]])>"
"tf.keras.layers.MultiHeadAttention(
    num_heads,
    key_dim,
    value_dim=None,
    dropout=0.0,
    use_bias=True,
    output_shape=None,
    attention_axes=None,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MultiHeadAttention
layer = MultiHeadAttention(num_heads=2, key_dim=2)
target = tf.keras.Input(shape=[8, 16])
source = tf.keras.Input(shape=[4, 16])
output_tensor, weights = layer(target, source,
                               return_attention_scores=True)
print(output_tensor.shape)
(None, 8, 16)
print(weights.shape)
(None, 2, 8, 4)"
"tf.keras.layers.Multiply(
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Multiply
tf.keras.layers.Multiply()([np.arange(5).reshape(5, 1),
                            np.arange(5, 10).reshape(5, 1)])
<tf.Tensor: shape=(5, 1), dtype=int64, numpy=
<tf.Tensor: shape=(5, 1), dtype=int64, numpy=
array([[ 0],
     [ 6],
     [14],
     [24],
     [36]])>"
"tf.keras.layers.Normalization(
    axis=-1, mean=None, variance=None, invert=False, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Normalization
adapt_data = np.array([1., 2., 3., 4., 5.], dtype='float32')
input_data = np.array([1., 2., 3.], dtype='float32')
layer = tf.keras.layers.Normalization(axis=None)
layer.adapt(adapt_data)
layer(input_data)
<tf.Tensor: shape=(3,), dtype=float32, numpy=
<tf.Tensor: shape=(3,), dtype=float32, numpy=
array([-1.4142135, -0.70710677, 0.], dtype=float32)>"
"tf.keras.layers.Permute(
    dims, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Permute
model = Sequential()
model.add(Permute((2, 1), input_shape=(10, 64)))
"
"tf.keras.layers.RandomBrightness(
    factor, value_range=(0, 255), seed=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RandomBrightness
random_bright = tf.keras.layers.RandomBrightness(factor=0.2)

image = [[[1, 2, 3], [4 ,5 ,6]], [[7, 8, 9], [10, 11, 12]]]

output = random_bright(image, training=True)

tf.Tensor([[[26.5, 27.5, 28.5]
            [29.5, 30.5, 31.5]]
           [[32.5, 33.5, 34.5]
            [35.5, 36.5, 37.5]]],
          shape=(2, 2, 3), dtype=int64)
"
"tf.keras.layers.RandomZoom(
    height_factor,
    width_factor=None,
    fill_mode='reflect',
    interpolation='bilinear',
    seed=None,
    fill_value=0.0,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RandomZoom
input_img = np.random.random((32, 224, 224, 3))
layer = tf.keras.layers.RandomZoom(.5, .2)
out_img = layer(input_img)
out_img.shape
TensorShape([32, 224, 224, 3])"
"tf.keras.layers.RepeatVector(
    n, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RepeatVector
model = Sequential()
model.add(Dense(32, input_dim=32))

model.add(RepeatVector(3))
"
"tf.keras.layers.Reshape(
    target_shape, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Reshape
model = tf.keras.Sequential()
model.add(tf.keras.layers.Reshape((3, 4), input_shape=(12,)))
model.output_shape
(None, 3, 4)"
"tf.keras.layers.SimpleRNN(
    units,
    activation='tanh',
    use_bias=True,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros',
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    dropout=0.0,
    recurrent_dropout=0.0,
    return_sequences=False,
    return_state=False,
    go_backwards=False,
    stateful=False,
    unroll=False,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SimpleRNN
inputs = np.random.random([32, 10, 8]).astype(np.float32)
simple_rnn = tf.keras.layers.SimpleRNN(4)


simple_rnn = tf.keras.layers.SimpleRNN(
    4, return_sequences=True, return_state=True)

whole_sequence_output, final_state = simple_rnn(inputs)
"
"tf.keras.layers.SimpleRNNCell(
    units,
    activation='tanh',
    use_bias=True,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros',
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    dropout=0.0,
    recurrent_dropout=0.0,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SimpleRNNCell
inputs = np.random.random([32, 10, 8]).astype(np.float32)
rnn = tf.keras.layers.RNN(tf.keras.layers.SimpleRNNCell(4))


rnn = tf.keras.layers.RNN(
    tf.keras.layers.SimpleRNNCell(4),
    return_sequences=True,
    return_state=True)

whole_sequence_output, final_state = rnn(inputs)
"
"tf.keras.layers.Softmax(
    axis=-1, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Softmax
inp = np.asarray([1., 2., 1.])
layer = tf.keras.layers.Softmax()
layer(inp).numpy()
array([0.21194157, 0.5761169 , 0.21194157], dtype=float32)
mask = np.asarray([True, False, True], dtype=bool)
layer(inp, mask).numpy()
array([0.5, 0. , 0.5], dtype=float32)"
"tf.keras.layers.StackedRNNCells(
    cells, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import StackedRNNCells
batch_size = 3
sentence_max_length = 5
n_features = 2
new_shape = (batch_size, sentence_max_length, n_features)
x = tf.constant(np.reshape(np.arange(30), new_shape), dtype = tf.float32)

rnn_cells = [tf.keras.layers.LSTMCell(128) for _ in range(2)]
stacked_lstm = tf.keras.layers.StackedRNNCells(rnn_cells)
lstm_layer = tf.keras.layers.RNN(stacked_lstm)

result = lstm_layer(x)
"
"tf.keras.layers.StringLookup(
    max_tokens=None,
    num_oov_indices=1,
    mask_token=None,
    oov_token='[UNK]',
    vocabulary=None,
    idf_weights=None,
    encoding='utf-8',
    invert=False,
    output_mode='int',
    sparse=False,
    pad_to_max_tokens=False,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import StringLookup
vocab = [""a"", ""b"", ""c"", ""d""]
data = tf.constant([[""a"", ""c"", ""d""], [""d"", ""z"", ""b""]])
layer = tf.keras.layers.StringLookup(vocabulary=vocab)
layer(data)
<tf.Tensor: shape=(2, 3), dtype=int64, numpy=
<tf.Tensor: shape=(2, 3), dtype=int64, numpy=
array([[1, 3, 4],
       [4, 0, 2]])>"
"tf.keras.layers.Subtract(
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Subtract
    import keras

    input1 = keras.layers.Input(shape=(16,))
    x1 = keras.layers.Dense(8, activation='relu')(input1)
    input2 = keras.layers.Input(shape=(32,))
    x2 = keras.layers.Dense(8, activation='relu')(input2)
    subtracted = keras.layers.Subtract()([x1, x2])

    out = keras.layers.Dense(4)(subtracted)
    model = keras.models.Model(inputs=[input1, input2], outputs=out)
"
"tf.keras.layers.TextVectorization(
    max_tokens=None,
    standardize='lower_and_strip_punctuation',
    split='whitespace',
    ngrams=None,
    output_mode='int',
    output_sequence_length=None,
    pad_to_max_tokens=False,
    vocabulary=None,
    idf_weights=None,
    sparse=False,
    ragged=False,
    encoding='utf-8',
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TextVectorization
text_dataset = tf.data.Dataset.from_tensor_slices([""foo"", ""bar"", ""baz""])
vectorize_layer = tf.keras.layers.TextVectorization(
 max_tokens=max_features,
 output_mode='int',
 output_sequence_length=max_len)
vectorize_layer.adapt(text_dataset.batch(64))
model = tf.keras.models.Sequential()
model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
model.add(vectorize_layer)
input_data = [[""foo qux bar""], [""qux baz""]]
model.predict(input_data)
array([[2, 1, 4, 0],
       [1, 3, 0, 0]])"
"tf.keras.layers.TimeDistributed(
    layer, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TimeDistributed
inputs = tf.keras.Input(shape=(10, 128, 128, 3))
conv_2d_layer = tf.keras.layers.Conv2D(64, (3, 3))
outputs = tf.keras.layers.TimeDistributed(conv_2d_layer)(inputs)
outputs.shape
TensorShape([None, 10, 126, 126, 64])"
"tf.keras.layers.UnitNormalization(
    axis=-1, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import UnitNormalization
data = tf.constant(np.arange(6).reshape(2, 3), dtype=tf.float32)
normalized_data = tf.keras.layers.UnitNormalization()(data)
print(tf.reduce_sum(normalized_data[0, :] ** 2).numpy())
1.0"
"tf.keras.layers.UpSampling1D(
    size=2, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import UpSampling1D
input_shape = (2, 2, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
print(x)
[[[ 0  1  2]
  [ 3  4  5]]
 [[ 6  7  8]
  [ 9 10 11]]]
y = tf.keras.layers.UpSampling1D(size=2)(x)
print(y)
tf.Tensor(
  [[[ 0  1  2]
    [ 0  1  2]
    [ 3  4  5]
    [ 3  4  5]]
   [[ 6  7  8]
    [ 6  7  8]
    [ 9 10 11]
    [ 9 10 11]]], shape=(2, 4, 3), dtype=int64)"
"tf.keras.layers.UpSampling2D(
    size=(2, 2), data_format=None, interpolation='nearest', **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import UpSampling2D
input_shape = (2, 2, 1, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
print(x)
[[[[ 0  1  2]]
  [[ 3  4  5]]]
 [[[ 6  7  8]]
  [[ 9 10 11]]]]
y = tf.keras.layers.UpSampling2D(size=(1, 2))(x)
print(y)
tf.Tensor(
  [[[[ 0  1  2]
     [ 0  1  2]]
    [[ 3  4  5]
     [ 3  4  5]]]
   [[[ 6  7  8]
     [ 6  7  8]]
    [[ 9 10 11]
     [ 9 10 11]]]], shape=(2, 2, 2, 3), dtype=int64)"
"tf.keras.layers.UpSampling3D(
    size=(2, 2, 2), data_format=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import UpSampling3D
input_shape = (2, 1, 2, 1, 3)
x = tf.constant(1, shape=input_shape)
y = tf.keras.layers.UpSampling3D(size=2)(x)
print(y.shape)
(2, 2, 4, 2, 3)"
"tf.keras.layers.ZeroPadding1D(
    padding=1, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ZeroPadding1D
input_shape = (2, 2, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
print(x)
[[[ 0  1  2]
  [ 3  4  5]]
 [[ 6  7  8]
  [ 9 10 11]]]
y = tf.keras.layers.ZeroPadding1D(padding=2)(x)
print(y)
tf.Tensor(
  [[[ 0  0  0]
    [ 0  0  0]
    [ 0  1  2]
    [ 3  4  5]
    [ 0  0  0]
    [ 0  0  0]]
   [[ 0  0  0]
    [ 0  0  0]
    [ 6  7  8]
    [ 9 10 11]
    [ 0  0  0]
    [ 0  0  0]]], shape=(2, 6, 3), dtype=int64)"
"tf.keras.layers.ZeroPadding2D(
    padding=(1, 1), data_format=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ZeroPadding2D
input_shape = (1, 1, 2, 2)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
print(x)
[[[[0 1]
   [2 3]]]]
y = tf.keras.layers.ZeroPadding2D(padding=1)(x)
print(y)
tf.Tensor(
  [[[[0 0]
     [0 0]
     [0 0]
     [0 0]]
    [[0 0]
     [0 1]
     [2 3]
     [0 0]]
    [[0 0]
     [0 0]
     [0 0]
     [0 0]]]], shape=(1, 3, 4, 2), dtype=int64)"
"tf.keras.layers.ZeroPadding3D(
    padding=(1, 1, 1), data_format=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ZeroPadding3D
input_shape = (1, 1, 2, 2, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
y = tf.keras.layers.ZeroPadding3D(padding=2)(x)
print(y.shape)
(1, 5, 6, 6, 3)"
"tf.keras.layers.add(
    inputs, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import add
input_shape = (2, 3, 4)
x1 = tf.random.normal(input_shape)
x2 = tf.random.normal(input_shape)
y = tf.keras.layers.add([x1, x2])
print(y.shape)
(2, 3, 4)"
"tf.keras.layers.concatenate(
    inputs, axis=-1, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import concatenate
x = np.arange(20).reshape(2, 2, 5)
print(x)
[[[ 0  1  2  3  4]
  [ 5  6  7  8  9]]
 [[10 11 12 13 14]
  [15 16 17 18 19]]]
y = np.arange(20, 30).reshape(2, 1, 5)
print(y)
[[[20 21 22 23 24]]
 [[25 26 27 28 29]]]
tf.keras.layers.concatenate([x, y],
                            axis=1)
<tf.Tensor: shape=(2, 3, 5), dtype=int64, numpy=
<tf.Tensor: shape=(2, 3, 5), dtype=int64, numpy=
array([[[ 0,  1,  2,  3,  4],
      [ 5,  6,  7,  8,  9],
      [20, 21, 22, 23, 24]],
     [[10, 11, 12, 13, 14],
      [15, 16, 17, 18, 19],
      [25, 26, 27, 28, 29]]])>"
"tf.keras.layers.deserialize(
    config, custom_objects=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import deserialize
config = {
  'class_name': 'Dense',
  'config': {
    'activation': 'relu',
    'activity_regularizer': None,
    'bias_constraint': None,
    'bias_initializer': {'class_name': 'Zeros', 'config': {} },
    'bias_regularizer': None,
    'dtype': 'float32',
    'kernel_constraint': None,
    'kernel_initializer': {'class_name': 'GlorotUniform',
                           'config': {'seed': None} },
    'kernel_regularizer': None,
    'name': 'dense',
    'trainable': True,
    'units': 32,
    'use_bias': True
  }
}
dense_layer = tf.keras.layers.deserialize(config)
"
"tf.keras.layers.EinsumDense(
    equation,
    output_shape,
    activation=None,
    bias_axes=None,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import EinsumDense
layer = tf.keras.layers.EinsumDense(""ab,bc->ac"",
                                    output_shape=64,
                                    bias_axes=""c"")
input_tensor = tf.keras.Input(shape=[32])
output_tensor = layer(input_tensor)
output_tensor
<... shape=(None, 64) dtype=...>
<... shape=(None, 64) dtype=...>"
"tf.keras.layers.experimental.RandomFourierFeatures(
    output_dim,
    kernel_initializer='gaussian',
    scale=None,
    trainable=False,
    name=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RandomFourierFeatures
model = keras.Sequential([
  keras.Input(shape=(784,)),
  RandomFourierFeatures(
      output_dim=4096,
      scale=10.,
      kernel_initializer='gaussian'),
  layers.Dense(units=10, activation='softmax'),
])
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['categorical_accuracy']
)
"
"tf.keras.layers.experimental.SyncBatchNormalization(
    axis=-1,
    momentum=0.99,
    epsilon=0.001,
    center=True,
    scale=True,
    beta_initializer='zeros',
    gamma_initializer='ones',
    moving_mean_initializer='zeros',
    moving_variance_initializer='ones',
    beta_regularizer=None,
    gamma_regularizer=None,
    beta_constraint=None,
    gamma_constraint=None,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SyncBatchNormalization
strategy = tf.distribute.MirroredStrategy()

with strategy.scope():
  model = tf.keras.Sequential()
  model.add(tf.keras.layers.Dense(16))
  model.add(tf.keras.layers.experimental.SyncBatchNormalization())
"
"tf.keras.layers.CategoryEncoding(
    num_tokens=None, output_mode='multi_hot', sparse=False, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CategoryEncoding
layer = tf.keras.layers.CategoryEncoding(
          num_tokens=4, output_mode=""one_hot"")
layer([3, 2, 0, 1])
<tf.Tensor: shape=(4, 4), dtype=float32, numpy=
<tf.Tensor: shape=(4, 4), dtype=float32, numpy=
  array([[0., 0., 0., 1.],
         [0., 0., 1., 0.],
         [1., 0., 0., 0.],
         [0., 1., 0., 0.]], dtype=float32)>"
"tf.keras.layers.Discretization(
    bin_boundaries=None,
    num_bins=None,
    epsilon=0.01,
    output_mode='int',
    sparse=False,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Discretization
>>> input = np.array([[-1.5, 1.0, 3.4, .5], [0.0, 3.0, 1.3, 0.0]])
>>> layer = tf.keras.layers.Discretization(bin_boundaries=[0., 1., 2.])
>>> layer(input)
<tf.Tensor: shape=(2, 4), dtype=int64, numpy=
<tf.Tensor: shape=(2, 4), dtype=int64, numpy=
array([[0, 2, 3, 1],
       [1, 3, 2, 1]])>
"
"tf.keras.layers.experimental.preprocessing.HashedCrossing(
    num_bins, output_mode='int', sparse=False, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import HashedCrossing
layer = tf.keras.layers.experimental.preprocessing.HashedCrossing(
    num_bins=5)
feat1 = tf.constant(['A', 'B', 'A', 'B', 'A'])
feat2 = tf.constant([101, 101, 101, 102, 102])
layer((feat1, feat2))
<tf.Tensor: shape=(5,), dtype=int64, numpy=array([1, 4, 1, 1, 3])>
<tf.Tensor: shape=(5,), dtype=int64, numpy=array([1, 4, 1, 1, 3])>"
"tf.keras.layers.Hashing(
    num_bins,
    mask_value=None,
    salt=None,
    output_mode='int',
    sparse=False,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Hashing
layer = tf.keras.layers.Hashing(num_bins=3)
inp = [['A'], ['B'], ['C'], ['D'], ['E']]
layer(inp)
<tf.Tensor: shape=(5, 1), dtype=int64, numpy=
<tf.Tensor: shape=(5, 1), dtype=int64, numpy=
  array([[1],
         [0],
         [1],
         [1],
         [2]])>"
"tf.keras.layers.IntegerLookup(
    max_tokens=None,
    num_oov_indices=1,
    mask_token=None,
    oov_token=-1,
    vocabulary=None,
    vocabulary_dtype='int64',
    idf_weights=None,
    invert=False,
    output_mode='int',
    sparse=False,
    pad_to_max_tokens=False,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import IntegerLookup
vocab = [12, 36, 1138, 42]
layer = tf.keras.layers.IntegerLookup(vocabulary=vocab)
layer(data)
<tf.Tensor: shape=(2, 3), dtype=int64, numpy=
<tf.Tensor: shape=(2, 3), dtype=int64, numpy=
array([[1, 3, 4],
       [4, 0, 2]])>"
"tf.keras.layers.Normalization(
    axis=-1, mean=None, variance=None, invert=False, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Normalization
adapt_data = np.array([1., 2., 3., 4., 5.], dtype='float32')
input_data = np.array([1., 2., 3.], dtype='float32')
layer = tf.keras.layers.Normalization(axis=None)
layer.adapt(adapt_data)
layer(input_data)
<tf.Tensor: shape=(3,), dtype=float32, numpy=
<tf.Tensor: shape=(3,), dtype=float32, numpy=
array([-1.4142135, -0.70710677, 0.], dtype=float32)>"
"tf.keras.layers.RandomZoom(
    height_factor,
    width_factor=None,
    fill_mode='reflect',
    interpolation='bilinear',
    seed=None,
    fill_value=0.0,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RandomZoom
input_img = np.random.random((32, 224, 224, 3))
layer = tf.keras.layers.RandomZoom(.5, .2)
out_img = layer(input_img)
out_img.shape
TensorShape([32, 224, 224, 3])"
"tf.keras.layers.StringLookup(
    max_tokens=None,
    num_oov_indices=1,
    mask_token=None,
    oov_token='[UNK]',
    vocabulary=None,
    idf_weights=None,
    encoding='utf-8',
    invert=False,
    output_mode='int',
    sparse=False,
    pad_to_max_tokens=False,
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import StringLookup
vocab = [""a"", ""b"", ""c"", ""d""]
data = tf.constant([[""a"", ""c"", ""d""], [""d"", ""z"", ""b""]])
layer = tf.keras.layers.StringLookup(vocabulary=vocab)
layer(data)
<tf.Tensor: shape=(2, 3), dtype=int64, numpy=
<tf.Tensor: shape=(2, 3), dtype=int64, numpy=
array([[1, 3, 4],
       [4, 0, 2]])>"
"tf.keras.layers.TextVectorization(
    max_tokens=None,
    standardize='lower_and_strip_punctuation',
    split='whitespace',
    ngrams=None,
    output_mode='int',
    output_sequence_length=None,
    pad_to_max_tokens=False,
    vocabulary=None,
    idf_weights=None,
    sparse=False,
    ragged=False,
    encoding='utf-8',
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TextVectorization
text_dataset = tf.data.Dataset.from_tensor_slices([""foo"", ""bar"", ""baz""])
vectorize_layer = tf.keras.layers.TextVectorization(
 max_tokens=max_features,
 output_mode='int',
 output_sequence_length=max_len)
vectorize_layer.adapt(text_dataset.batch(64))
model = tf.keras.models.Sequential()
model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
model.add(vectorize_layer)
input_data = [[""foo qux bar""], [""qux baz""]]
model.predict(input_data)
array([[2, 1, 4, 0],
       [1, 3, 0, 0]])"
"tf.keras.layers.maximum(
    inputs, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import maximum
input1 = tf.keras.layers.Input(shape=(16,))
x1 = tf.keras.layers.Dense(8, activation='relu')(input1) #shape=(None, 8)
input2 = tf.keras.layers.Input(shape=(32,))
x2 = tf.keras.layers.Dense(8, activation='relu')(input2) #shape=(None, 8)
max_inp=tf.keras.layers.maximum([x1,x2]) #shape=(None, 8)
out = tf.keras.layers.Dense(4)(max_inp)
model = tf.keras.models.Model(inputs=[input1, input2], outputs=out)
"
"tf.keras.layers.multiply(
    inputs, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import multiply
x1 = np.arange(3.0)
x2 = np.arange(3.0)
tf.keras.layers.multiply([x1, x2])
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 1., 4.], ...)>
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 1., 4.], ...)>"
"tf.keras.layers.serialize(
    layer
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import serialize
from pprint import pprint
model = tf.keras.models.Sequential()
model.add(tf.keras.Input(shape=(16,)))
model.add(tf.keras.layers.Dense(32, activation='relu'))

pprint(tf.keras.layers.serialize(model))
"
"tf.keras.layers.subtract(
    inputs, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import subtract
    import keras

    input1 = keras.layers.Input(shape=(16,))
    x1 = keras.layers.Dense(8, activation='relu')(input1)
    input2 = keras.layers.Input(shape=(32,))
    x2 = keras.layers.Dense(8, activation='relu')(input2)
    subtracted = keras.layers.subtract([x1, x2])

    out = keras.layers.Dense(4)(subtracted)
    model = keras.models.Model(inputs=[input1, input2], outputs=out)
"
"tf.keras.losses.BinaryCrossentropy(
    from_logits=False,
    label_smoothing=0.0,
    axis=-1,
    reduction=losses_utils.ReductionV2.AUTO,
    name='binary_crossentropy'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import BinaryCrossentropy
model.compile(
  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
  ....
)
"
"tf.keras.losses.BinaryFocalCrossentropy(
    apply_class_balancing=False,
    alpha=0.25,
    gamma=2.0,
    from_logits=False,
    label_smoothing=0.0,
    axis=-1,
    reduction=losses_utils.ReductionV2.AUTO,
    name='binary_focal_crossentropy'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import BinaryFocalCrossentropy
model.compile(
  loss=tf.keras.losses.BinaryFocalCrossentropy(gamma=2.0, from_logits=True),
  ....
)
"
"tf.keras.losses.CategoricalCrossentropy(
    from_logits=False,
    label_smoothing=0.0,
    axis=-1,
    reduction=losses_utils.ReductionV2.AUTO,
    name='categorical_crossentropy'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CategoricalCrossentropy
y_true = [[0, 1, 0], [0, 0, 1]]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
cce = tf.keras.losses.CategoricalCrossentropy()
cce(y_true, y_pred).numpy()
1.177"
"tf.keras.losses.CategoricalHinge(
    reduction=losses_utils.ReductionV2.AUTO, name='categorical_hinge'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CategoricalHinge
y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
h = tf.keras.losses.CategoricalHinge()
h(y_true, y_pred).numpy()
1.4"
"tf.keras.losses.CosineSimilarity(
    axis=-1,
    reduction=losses_utils.ReductionV2.AUTO,
    name='cosine_similarity'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CosineSimilarity
y_true = [[0., 1.], [1., 1.]]
y_pred = [[1., 0.], [1., 1.]]
cosine_loss = tf.keras.losses.CosineSimilarity(axis=1)
cosine_loss(y_true, y_pred).numpy()
-0.5"
"tf.keras.losses.Hinge(
    reduction=losses_utils.ReductionV2.AUTO, name='hinge'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Hinge
y_true = [[0., 1.], [0., 0.]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
h = tf.keras.losses.Hinge()
h(y_true, y_pred).numpy()
1.3"
"tf.keras.losses.KLDivergence(
    reduction=losses_utils.ReductionV2.AUTO, name='kl_divergence'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import KLDivergence
y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
kl = tf.keras.losses.KLDivergence()
kl(y_true, y_pred).numpy()
0.458"
"tf.keras.losses.LogCosh(
    reduction=losses_utils.ReductionV2.AUTO, name='log_cosh'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LogCosh
y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [0., 0.]]
l = tf.keras.losses.LogCosh()
l(y_true, y_pred).numpy()
0.108"
"tf.keras.losses.Loss(
    reduction=losses_utils.ReductionV2.AUTO, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
class MeanSquaredError(Loss):

  def call(self, y_true, y_pred):
    return tf.reduce_mean(tf.math.square(y_pred - y_true), axis=-1)
"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_percentage_error
y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_logarithmic_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.losses.MeanAbsoluteError(
    reduction=losses_utils.ReductionV2.AUTO,
    name='mean_absolute_error'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanAbsoluteError
y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [1., 0.]]
mae = tf.keras.losses.MeanAbsoluteError()
mae(y_true, y_pred).numpy()
0.5"
"tf.keras.losses.MeanAbsolutePercentageError(
    reduction=losses_utils.ReductionV2.AUTO,
    name='mean_absolute_percentage_error'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanAbsolutePercentageError
y_true = [[2., 1.], [2., 3.]]
y_pred = [[1., 1.], [1., 0.]]
mape = tf.keras.losses.MeanAbsolutePercentageError()
mape(y_true, y_pred).numpy()
50."
"tf.keras.losses.MeanSquaredError(
    reduction=losses_utils.ReductionV2.AUTO,
    name='mean_squared_error'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanSquaredError
y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [1., 0.]]
mse = tf.keras.losses.MeanSquaredError()
mse(y_true, y_pred).numpy()
0.5"
"tf.keras.losses.MeanSquaredLogarithmicError(
    reduction=losses_utils.ReductionV2.AUTO,
    name='mean_squared_logarithmic_error'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanSquaredLogarithmicError
y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [1., 0.]]
msle = tf.keras.losses.MeanSquaredLogarithmicError()
msle(y_true, y_pred).numpy()
0.240"
"tf.keras.losses.Poisson(
    reduction=losses_utils.ReductionV2.AUTO, name='poisson'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Poisson
y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [0., 0.]]
p = tf.keras.losses.Poisson()
p(y_true, y_pred).numpy()
0.5"
"tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=False,
    ignore_class=None,
    reduction=losses_utils.ReductionV2.AUTO,
    name='sparse_categorical_crossentropy'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SparseCategoricalCrossentropy
y_true = [1, 2]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
scce = tf.keras.losses.SparseCategoricalCrossentropy()
scce(y_true, y_pred).numpy()
1.177"
"tf.keras.losses.SquaredHinge(
    reduction=losses_utils.ReductionV2.AUTO, name='squared_hinge'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SquaredHinge
y_true = [[0., 1.], [0., 0.]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
h = tf.keras.losses.SquaredHinge()
h(y_true, y_pred).numpy()
1.86"
"tf.keras.metrics.binary_crossentropy(
    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import binary_crossentropy
y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.916 , 0.714], dtype=float32)"
"tf.keras.metrics.binary_focal_crossentropy(
    y_true,
    y_pred,
    apply_class_balancing=False,
    alpha=0.25,
    gamma=2.0,
    from_logits=False,
    label_smoothing=0.0,
    axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import binary_focal_crossentropy
y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
loss = tf.keras.losses.binary_focal_crossentropy(y_true, y_pred,
                                                 gamma=2)
assert loss.shape == (2,)
loss.numpy()
array([0.330, 0.206], dtype=float32)"
"tf.keras.metrics.categorical_crossentropy(
    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import categorical_crossentropy
y_true = [[0, 1, 0], [0, 0, 1]]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.0513, 2.303], dtype=float32)"
"tf.keras.losses.categorical_hinge(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import categorical_hinge
y_true = np.random.randint(0, 3, size=(2,))
y_true = tf.keras.utils.to_categorical(y_true, num_classes=3)
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.categorical_hinge(y_true, y_pred)
assert loss.shape == (2,)
pos = np.sum(y_true * y_pred, axis=-1)
neg = np.amax((1. - y_true) * y_pred, axis=-1)
assert np.array_equal(loss.numpy(), np.maximum(0., neg - pos + 1.))"
"tf.keras.losses.cosine_similarity(
    y_true, y_pred, axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import cosine_similarity
y_true = [[0., 1.], [1., 1.], [1., 1.]]
y_pred = [[1., 0.], [1., 1.], [-1., -1.]]
loss = tf.keras.losses.cosine_similarity(y_true, y_pred, axis=1)
loss.numpy()
array([-0., -0.999, 0.999], dtype=float32)"
"tf.keras.losses.get(
    identifier
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get
loss = tf.keras.losses.get(""categorical_crossentropy"")
type(loss)
<class 'function'>
<class 'function'>
loss = tf.keras.losses.get(""CategoricalCrossentropy"")
type(loss)
<class '...keras.losses.CategoricalCrossentropy'>
<class '...keras.losses.CategoricalCrossentropy'>"
"tf.keras.metrics.hinge(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import hinge
y_true = np.random.choice([-1, 1], size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.hinge(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    np.mean(np.maximum(1. - y_true * y_pred, 0.), axis=-1))"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_percentage_error
y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_percentage_error
y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_logarithmic_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_logarithmic_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.poisson(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import poisson
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.poisson(y_true, y_pred)
assert loss.shape == (2,)
y_pred = y_pred + 1e-7
assert np.allclose(
    loss.numpy(), np.mean(y_pred - y_true * np.log(y_pred), axis=-1),
    atol=1e-5)"
"tf.keras.metrics.sparse_categorical_crossentropy(
    y_true, y_pred, from_logits=False, axis=-1, ignore_class=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sparse_categorical_crossentropy
y_true = [1, 2]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.0513, 2.303], dtype=float32)"
"tf.keras.metrics.squared_hinge(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import squared_hinge
y_true = np.random.choice([-1, 1], size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.squared_hinge(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    np.mean(np.square(np.maximum(1. - y_true * y_pred, 0.)), axis=-1))"
"tf.keras.metrics.AUC(
    num_thresholds=200,
    curve='ROC',
    summation_method='interpolation',
    name=None,
    dtype=None,
    thresholds=None,
    multi_label=False,
    num_labels=None,
    label_weights=None,
    from_logits=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import AUC
m = tf.keras.metrics.AUC(num_thresholds=3)
m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])
m.result().numpy()
0.75"
"tf.keras.metrics.Accuracy(
    name='accuracy', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Accuracy
m = tf.keras.metrics.Accuracy()
m.update_state([[1], [2], [3], [4]], [[0], [2], [3], [4]])
m.result().numpy()
0.75"
"tf.keras.metrics.BinaryAccuracy(
    name='binary_accuracy', dtype=None, threshold=0.5
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import BinaryAccuracy
m = tf.keras.metrics.BinaryAccuracy()
m.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]])
m.result().numpy()
0.75"
"tf.keras.metrics.BinaryCrossentropy(
    name='binary_crossentropy',
    dtype=None,
    from_logits=False,
    label_smoothing=0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import BinaryCrossentropy
m = tf.keras.metrics.BinaryCrossentropy()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
0.81492424"
"tf.keras.metrics.CategoricalAccuracy(
    name='categorical_accuracy', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CategoricalAccuracy
m = tf.keras.metrics.CategoricalAccuracy()
m.update_state([[0, 0, 1], [0, 1, 0]], [[0.1, 0.9, 0.8],
                [0.05, 0.95, 0]])
m.result().numpy()
0.5"
"tf.keras.metrics.CategoricalCrossentropy(
    name='categorical_crossentropy',
    dtype=None,
    from_logits=False,
    label_smoothing=0,
    axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CategoricalCrossentropy
m = tf.keras.metrics.CategoricalCrossentropy()
m.update_state([[0, 1, 0], [0, 0, 1]],
               [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])
m.result().numpy()
1.1769392"
"tf.keras.metrics.CategoricalHinge(
    name='categorical_hinge', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CategoricalHinge
m = tf.keras.metrics.CategoricalHinge()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
1.4000001"
"tf.keras.metrics.CosineSimilarity(
    name='cosine_similarity', dtype=None, axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CosineSimilarity
m = tf.keras.metrics.CosineSimilarity(axis=1)
m.update_state([[0., 1.], [1., 1.]], [[1., 0.], [1., 1.]])
m.result().numpy()
0.49999997"
"tf.keras.metrics.FalseNegatives(
    thresholds=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import FalseNegatives
m = tf.keras.metrics.FalseNegatives()
m.update_state([0, 1, 1, 1], [0, 1, 0, 0])
m.result().numpy()
2.0"
"tf.keras.metrics.FalsePositives(
    thresholds=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import FalsePositives
m = tf.keras.metrics.FalsePositives()
m.update_state([0, 1, 0, 0], [0, 0, 1, 1])
m.result().numpy()
2.0"
"tf.keras.metrics.Hinge(
    name='hinge', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Hinge
m = tf.keras.metrics.Hinge()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
1.3"
"tf.keras.metrics.KLDivergence(
    name='kullback_leibler_divergence', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import KLDivergence
m = tf.keras.metrics.KLDivergence()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
0.45814306"
"tf.keras.metrics.LogCoshError(
    name='logcosh', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LogCoshError
m = tf.keras.metrics.LogCoshError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.10844523"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_percentage_error
y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_logarithmic_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.Mean(
    name='mean', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Mean
m = tf.keras.metrics.Mean()
m.update_state([1, 3, 5, 7])
m.result().numpy()
4.0
m.reset_state()
m.update_state([1, 3, 5, 7], sample_weight=[1, 1, 0, 0])
m.result().numpy()
2.0"
"tf.keras.metrics.MeanAbsoluteError(
    name='mean_absolute_error', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanAbsoluteError
m = tf.keras.metrics.MeanAbsoluteError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.25"
"tf.keras.metrics.MeanAbsolutePercentageError(
    name='mean_absolute_percentage_error', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanAbsolutePercentageError
m = tf.keras.metrics.MeanAbsolutePercentageError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
250000000.0"
"tf.keras.metrics.MeanMetricWrapper(
    fn, name=None, dtype=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanMetricWrapper
def accuracy(y_true, y_pred):
  return tf.cast(tf.math.equal(y_true, y_pred), tf.float32)

accuracy_metric = tf.keras.metrics.MeanMetricWrapper(fn=accuracy)

keras_model.compile(..., metrics=accuracy_metric)
"
"tf.keras.metrics.MeanRelativeError(
    normalizer, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanRelativeError
m = tf.keras.metrics.MeanRelativeError(normalizer=[1, 3, 2, 3])
m.update_state([1, 3, 2, 3], [2, 4, 6, 8])"
"tf.keras.metrics.MeanSquaredError(
    name='mean_squared_error', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanSquaredError
m = tf.keras.metrics.MeanSquaredError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.25"
"tf.keras.metrics.MeanSquaredLogarithmicError(
    name='mean_squared_logarithmic_error', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanSquaredLogarithmicError
m = tf.keras.metrics.MeanSquaredLogarithmicError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.12011322"
"tf.keras.metrics.MeanTensor(
    name='mean_tensor', dtype=None, shape=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanTensor
m = tf.keras.metrics.MeanTensor()
m.update_state([0, 1, 2, 3])
m.update_state([4, 5, 6, 7])
m.result().numpy()
array([2., 3., 4., 5.], dtype=float32)"
"tf.keras.metrics.Metric(
    name=None, dtype=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Metric
m = SomeMetric(...)
for input in ...:
  m.update_state(input)
print('Final result: ', m.result().numpy())
"
"tf.keras.metrics.Poisson(
    name='poisson', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Poisson
m = tf.keras.metrics.Poisson()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.49999997"
"tf.keras.metrics.Precision(
    thresholds=None, top_k=None, class_id=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Precision
m = tf.keras.metrics.Precision()
m.update_state([0, 1, 1, 1], [1, 0, 1, 1])
m.result().numpy()
0.6666667"
"tf.keras.metrics.PrecisionAtRecall(
    recall, num_thresholds=200, class_id=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import PrecisionAtRecall
m = tf.keras.metrics.PrecisionAtRecall(0.5)
m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])
m.result().numpy()
0.5"
"tf.keras.metrics.Recall(
    thresholds=None, top_k=None, class_id=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Recall
m = tf.keras.metrics.Recall()
m.update_state([0, 1, 1, 1], [1, 0, 1, 1])
m.result().numpy()
0.6666667"
"tf.keras.metrics.RecallAtPrecision(
    precision, num_thresholds=200, class_id=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RecallAtPrecision
m = tf.keras.metrics.RecallAtPrecision(0.8)
m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])
m.result().numpy()
0.5"
"tf.keras.metrics.RootMeanSquaredError(
    name='root_mean_squared_error', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RootMeanSquaredError
m = tf.keras.metrics.RootMeanSquaredError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.5"
"tf.keras.metrics.SensitivityAtSpecificity(
    specificity, num_thresholds=200, class_id=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SensitivityAtSpecificity
m = tf.keras.metrics.SensitivityAtSpecificity(0.5)
m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])
m.result().numpy()
0.5"
"tf.keras.metrics.SparseCategoricalCrossentropy(
    name: str = 'sparse_categorical_crossentropy',
    dtype: Optional[Union[str, tf.dtypes.DType]] = None,
    from_logits: bool = False,
    ignore_class: Optional[int] = None,
    axis: int = -1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SparseCategoricalCrossentropy
m = tf.keras.metrics.SparseCategoricalCrossentropy()
m.update_state([1, 2],
               [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])
m.result().numpy()
1.1769392"
"tf.keras.metrics.SparseTopKCategoricalAccuracy(
    k=5, name='sparse_top_k_categorical_accuracy', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SparseTopKCategoricalAccuracy
m = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)
m.update_state([2, 1], [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])
m.result().numpy()
0.5"
"tf.keras.metrics.SpecificityAtSensitivity(
    sensitivity, num_thresholds=200, class_id=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SpecificityAtSensitivity
m = tf.keras.metrics.SpecificityAtSensitivity(0.5)
m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])
m.result().numpy()
0.66666667"
"tf.keras.metrics.SquaredHinge(
    name='squared_hinge', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SquaredHinge
m = tf.keras.metrics.SquaredHinge()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
1.86"
"tf.keras.metrics.Sum(
    name='sum', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Sum
m = tf.keras.metrics.Sum()
m.update_state([1, 3, 5, 7])
m.result().numpy()
16.0"
"tf.keras.metrics.TopKCategoricalAccuracy(
    k=5, name='top_k_categorical_accuracy', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TopKCategoricalAccuracy
m = tf.keras.metrics.TopKCategoricalAccuracy(k=1)
m.update_state([[0, 0, 1], [0, 1, 0]],
               [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])
m.result().numpy()
0.5"
"tf.keras.metrics.TrueNegatives(
    thresholds=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TrueNegatives
m = tf.keras.metrics.TrueNegatives()
m.update_state([0, 1, 0, 0], [1, 1, 0, 0])
m.result().numpy()
2.0"
"tf.keras.metrics.TruePositives(
    thresholds=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TruePositives
m = tf.keras.metrics.TruePositives()
m.update_state([0, 1, 1, 1], [1, 0, 1, 1])
m.result().numpy()
2.0"
"tf.keras.metrics.binary_accuracy(
    y_true, y_pred, threshold=0.5
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import binary_accuracy
y_true = [[1], [1], [0], [0]]
y_pred = [[1], [1], [0], [0]]
m = tf.keras.metrics.binary_accuracy(y_true, y_pred)
assert m.shape == (4,)
m.numpy()
array([1., 1., 1., 1.], dtype=float32)"
"tf.keras.metrics.binary_crossentropy(
    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import binary_crossentropy
y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.916 , 0.714], dtype=float32)"
"tf.keras.metrics.binary_focal_crossentropy(
    y_true,
    y_pred,
    apply_class_balancing=False,
    alpha=0.25,
    gamma=2.0,
    from_logits=False,
    label_smoothing=0.0,
    axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import binary_focal_crossentropy
y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
loss = tf.keras.losses.binary_focal_crossentropy(y_true, y_pred,
                                                 gamma=2)
assert loss.shape == (2,)
loss.numpy()
array([0.330, 0.206], dtype=float32)"
"tf.keras.metrics.categorical_accuracy(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import categorical_accuracy
y_true = [[0, 0, 1], [0, 1, 0]]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.categorical_accuracy(y_true, y_pred)
assert m.shape == (2,)
m.numpy()
array([0., 1.], dtype=float32)"
"tf.keras.metrics.categorical_crossentropy(
    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import categorical_crossentropy
y_true = [[0, 1, 0], [0, 0, 1]]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.0513, 2.303], dtype=float32)"
"tf.keras.metrics.get(
    identifier
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get
metric = tf.keras.metrics.get(""categorical_crossentropy"")
type(metric)
<class 'function'>
<class 'function'>
metric = tf.keras.metrics.get(""CategoricalCrossentropy"")
type(metric)
<class '...metrics.CategoricalCrossentropy'>
<class '...metrics.CategoricalCrossentropy'>"
"tf.keras.metrics.hinge(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import hinge
y_true = np.random.choice([-1, 1], size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.hinge(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    np.mean(np.maximum(1. - y_true * y_pred, 0.), axis=-1))"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_percentage_error
y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_percentage_error
y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_logarithmic_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_logarithmic_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.poisson(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import poisson
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.poisson(y_true, y_pred)
assert loss.shape == (2,)
y_pred = y_pred + 1e-7
assert np.allclose(
    loss.numpy(), np.mean(y_pred - y_true * np.log(y_pred), axis=-1),
    atol=1e-5)"
"tf.keras.metrics.sparse_categorical_accuracy(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sparse_categorical_accuracy
y_true = [2, 1]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)
assert m.shape == (2,)
m.numpy()
array([0., 1.], dtype=float32)"
"tf.keras.metrics.sparse_categorical_crossentropy(
    y_true, y_pred, from_logits=False, axis=-1, ignore_class=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sparse_categorical_crossentropy
y_true = [1, 2]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.0513, 2.303], dtype=float32)"
"tf.keras.metrics.sparse_top_k_categorical_accuracy(
    y_true, y_pred, k=5
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sparse_top_k_categorical_accuracy
y_true = [2, 1]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.sparse_top_k_categorical_accuracy(
    y_true, y_pred, k=3)
assert m.shape == (2,)
m.numpy()
array([1., 1.], dtype=float32)"
"tf.keras.metrics.squared_hinge(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import squared_hinge
y_true = np.random.choice([-1, 1], size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.squared_hinge(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    np.mean(np.square(np.maximum(1. - y_true * y_pred, 0.)), axis=-1))"
"tf.keras.metrics.top_k_categorical_accuracy(
    y_true, y_pred, k=5
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import top_k_categorical_accuracy
y_true = [[0, 0, 1], [0, 1, 0]]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.top_k_categorical_accuracy(y_true, y_pred, k=3)
assert m.shape == (2,)
m.numpy()
array([1., 1.], dtype=float32)"
"tf.keras.mixed_precision.Policy(
    name
)
","import pandas as pd
import numpy as np
import tensorflow as tf
tf.keras.mixed_precision.set_global_policy('mixed_float16')
layer1 = tf.keras.layers.Dense(10)
<Policy ""mixed_float16"">
<Policy ""mixed_float16"">
layer2 = tf.keras.layers.Dense(10, dtype='float32')
layer2.dtype_policy
<Policy ""float32"">
<Policy ""float32"">
tf.keras.mixed_precision.set_global_policy('float32')"
"tf.keras.mixed_precision.set_global_policy(
    policy
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import set_global_policy
tf.keras.mixed_precision.set_global_policy('mixed_float16')
tf.keras.mixed_precision.global_policy()
<Policy ""mixed_float16"">
<Policy ""mixed_float16"">
tf.keras.layers.Dense(10).dtype_policy
<Policy ""mixed_float16"">
<Policy ""mixed_float16"">
tf.keras.layers.Dense(10, dtype='float64').dtype_policy
<Policy ""float64"">
<Policy ""float64"">
tf.keras.mixed_precision.set_global_policy('float32')"
"tf.keras.Model(
    *args, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Model
import tensorflow as tf

inputs = tf.keras.Input(shape=(3,))
x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)
outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)
"
"tf.keras.Sequential(
    layers=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Sequential
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8, input_shape=(16,)))
model.add(tf.keras.layers.Dense(4))

model = tf.keras.Sequential()
model.add(tf.keras.Input(shape=(16,)))
model.add(tf.keras.layers.Dense(8))

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8))
model.add(tf.keras.layers.Dense(4))

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8, input_shape=(16,)))
model.add(tf.keras.layers.Dense(4))
len(model.weights)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8))
model.add(tf.keras.layers.Dense(4))
model.build((None, 16))
len(model.weights)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8))
model.add(tf.keras.layers.Dense(1))
model.compile(optimizer='sgd', loss='mse')
model.fit(x, y, batch_size=32, epochs=10)
"
"tf.keras.models.clone_model(
    model, input_tensors=None, clone_function=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import clone_model
model = keras.Sequential([
    keras.Input(shape=(728,)),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid'),
])
new_model = clone_model(model)
"
"tf.keras.models.load_model(
    filepath, custom_objects=None, compile=True, options=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import load_model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(5, input_shape=(3,)),
    tf.keras.layers.Softmax()])
model.save('/tmp/model')
loaded_model = tf.keras.models.load_model('/tmp/model')
x = tf.random.uniform((10, 3))
assert np.allclose(model.predict(x), loaded_model.predict(x))"
"tf.keras.models.model_from_json(
    json_string, custom_objects=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import model_from_json
model = tf.keras.Sequential([
    tf.keras.layers.Dense(5, input_shape=(3,)),
    tf.keras.layers.Softmax()])
config = model.to_json()
loaded_model = tf.keras.models.model_from_json(config)"
"tf.keras.optimizers.experimental.RMSprop(
    learning_rate=0.001,
    rho=0.9,
    momentum=0.0,
    epsilon=1e-07,
    centered=False,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=100,
    jit_compile=True,
    name='RMSprop',
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RMSprop
opt = tf.keras.optimizers.experimental.RMSprop(learning_rate=0.1)
var1 = tf.Variable(10.0)
opt.minimize(loss, [var1])
var1.numpy()
9.683772"
"tf.keras.optimizers.experimental.RMSprop(
    learning_rate=0.001,
    rho=0.9,
    momentum=0.0,
    epsilon=1e-07,
    centered=False,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=100,
    jit_compile=True,
    name='RMSprop',
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RMSprop
opt = tf.keras.optimizers.experimental.RMSprop(learning_rate=0.1)
var1 = tf.Variable(10.0)
opt.minimize(loss, [var1])
var1.numpy()
9.683772"
"tf.keras.optimizers.legacy.Adam(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    amsgrad=False,
    name='Adam',
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Adam
opt = tf.keras.optimizers.legacy.Adam(learning_rate=0.1)
var1 = tf.Variable(10.0)
step_count = opt.minimize(loss, [var1]).numpy()
var1.numpy()
9.9"
"tf.keras.optimizers.legacy.Nadam(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    name='Nadam',
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Nadam
>>> opt = tf.keras.optimizers.legacy.Nadam(learning_rate=0.2)
>>> var1 = tf.Variable(10.0)
>>> loss = lambda: (var1 ** 2) / 2.0
>>> step_count = opt.minimize(loss, [var1]).numpy()
>>> ""{:.1f}"".format(var1.numpy())
9.8
"
"tf.keras.optimizers.legacy.RMSprop(
    learning_rate=0.001,
    rho=0.9,
    momentum=0.0,
    epsilon=1e-07,
    centered=False,
    name='RMSprop',
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RMSprop
opt = tf.keras.optimizers.legacy.RMSprop(learning_rate=0.1)
var1 = tf.Variable(10.0)
step_count = opt.minimize(loss, [var1]).numpy()
var1.numpy()
9.683772"
"tf.keras.optimizers.schedules.CosineDecayRestarts(
    initial_learning_rate,
    first_decay_steps,
    t_mul=2.0,
    m_mul=1.0,
    alpha=0.0,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CosineDecayRestarts
first_decay_steps = 1000
lr_decayed_fn = (
  tf.keras.optimizers.schedules.CosineDecayRestarts(
      initial_learning_rate,
      first_decay_steps))
"
"tf.keras.optimizers.schedules.PiecewiseConstantDecay(
    boundaries, values, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import PiecewiseConstantDecay
step = tf.Variable(0, trainable=False)
boundaries = [100000, 110000]
values = [1.0, 0.5, 0.1]
learning_rate_fn = keras.optimizers.schedules.PiecewiseConstantDecay(
    boundaries, values)

learning_rate = learning_rate_fn(step)
"
"tf.keras.optimizers.schedules.deserialize(
    config, custom_objects=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import deserialize
config = {
  'class_name': 'PolynomialDecay',
  'config': {'cycle': False,
    'decay_steps': 10000,
    'end_learning_rate': 0.01,
    'initial_learning_rate': 0.1,
    'name': None,
    'power': 0.5} }
lr_schedule = tf.keras.optimizers.schedules.deserialize(config)
"
"tf.keras.optimizers.schedules.serialize(
    learning_rate_schedule
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import serialize
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
  0.1, decay_steps=100000, decay_rate=0.96, staircase=True)
tf.keras.optimizers.schedules.serialize(lr_schedule)
{'class_name': 'ExponentialDecay', 'config': {...} }"
"tf.keras.optimizers.serialize(
    optimizer
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import serialize
tf.keras.optimizers.serialize(tf.keras.optimizers.legacy.SGD())
{'class_name': 'SGD', 'config': {'name': 'SGD', 'learning_rate': 0.01,
                                 'decay': 0.0, 'momentum': 0.0,
                                 'nesterov': False} }"
"tf.keras.preprocessing.image.ImageDataGenerator(
    featurewise_center=False,
    samplewise_center=False,
    featurewise_std_normalization=False,
    samplewise_std_normalization=False,
    zca_whitening=False,
    zca_epsilon=1e-06,
    rotation_range=0,
    width_shift_range=0.0,
    height_shift_range=0.0,
    brightness_range=None,
    shear_range=0.0,
    zoom_range=0.0,
    channel_shift_range=0.0,
    fill_mode='nearest',
    cval=0.0,
    horizontal_flip=False,
    vertical_flip=False,
    rescale=None,
    preprocessing_function=None,
    data_format=None,
    validation_split=0.0,
    interpolation_order=1,
    dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ImageDataGenerator
(x_train, y_train), (x_test, y_test) = cifar10.load_data()
y_train = utils.to_categorical(y_train, num_classes)
y_test = utils.to_categorical(y_test, num_classes)
datagen = ImageDataGenerator(
    featurewise_center=True,
    featurewise_std_normalization=True,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True,
    validation_split=0.2)
datagen.fit(x_train)
model.fit(datagen.flow(x_train, y_train, batch_size=32,
         subset='training'),
         validation_data=datagen.flow(x_train, y_train,
         batch_size=8, subset='validation'),
         steps_per_epoch=len(x_train) / 32, epochs=epochs)
for e in range(epochs):
    print('Epoch', e)
    batches = 0
    for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=32):
        model.fit(x_batch, y_batch)
        batches += 1
        if batches >= len(x_train) / 32:
            break
"
"tf.keras.utils.array_to_img(
    x, data_format=None, scale=True, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import array_to_img
from PIL import Image
img = np.random.random(size=(100, 100, 3))
pil_img = tf.keras.utils.array_to_img(img)
"
"tf.keras.utils.img_to_array(
    img, data_format=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import img_to_array
from PIL import Image
img_data = np.random.random(size=(100, 100, 3))
img = tf.keras.utils.array_to_img(img_data)
array = tf.keras.utils.image.img_to_array(img)
"
"tf.keras.utils.load_img(
    path,
    grayscale=False,
    color_mode='rgb',
    target_size=None,
    interpolation='nearest',
    keep_aspect_ratio=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import load_img
image = tf.keras.utils.load_img(image_path)
input_arr = tf.keras.utils.img_to_array(image)
predictions = model.predict(input_arr)
"
"tf.keras.preprocessing.sequence.TimeseriesGenerator(
    data,
    targets,
    length,
    sampling_rate=1,
    stride=1,
    start_index=0,
    end_index=None,
    shuffle=False,
    reverse=False,
    batch_size=128
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TimeseriesGenerator
from keras.preprocessing.sequence import TimeseriesGenerator
import numpy as np
data = np.array([[i] for i in range(50)])
targets = np.array([[i] for i in range(50)])
data_gen = TimeseriesGenerator(data, targets,
                               length=10, sampling_rate=2,
                               batch_size=2)
assert len(data_gen) == 20
batch_0 = data_gen[0]
x, y = batch_0
assert np.array_equal(x,
                      np.array([[[0], [2], [4], [6], [8]],
                                [[1], [3], [5], [7], [9]]]))
assert np.array_equal(y,
                      np.array([[10], [11]]))
"
"tf.keras.utils.pad_sequences(
    sequences,
    maxlen=None,
    dtype='int32',
    padding='pre',
    truncating='pre',
    value=0.0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import pad_sequences
sequence = [[1], [2, 3], [4, 5, 6]]
tf.keras.preprocessing.sequence.pad_sequences(sequence)
array([[0, 0, 1],
       [0, 2, 3],
       [4, 5, 6]], dtype=int32)"
"tf.keras.preprocessing.text.text_to_word_sequence(
    input_text,
    filters='!""#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n',
    lower=True,
    split=' '
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import text_to_word_sequence
sample_text = 'This is a sample sentence.'
tf.keras.preprocessing.text.text_to_word_sequence(sample_text)
['this', 'is', 'a', 'sample', 'sentence']"
"tf.keras.regularizers.OrthogonalRegularizer(
    factor=0.01, mode='rows'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import OrthogonalRegularizer
regularizer = tf.keras.regularizers.OrthogonalRegularizer(factor=0.01)
layer = tf.keras.layers.Dense(units=4, kernel_regularizer=regularizer)"
"tf.keras.regularizers.OrthogonalRegularizer(
    factor=0.01, mode='rows'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import OrthogonalRegularizer
regularizer = tf.keras.regularizers.OrthogonalRegularizer(factor=0.01)
layer = tf.keras.layers.Dense(units=4, kernel_regularizer=regularizer)"
"tf.keras.utils.custom_object_scope(
    *args
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import custom_object_scope
layer = Dense(3, kernel_regularizer=my_regularizer)
config = layer.get_config()
...
with custom_object_scope({'my_regularizer': my_regularizer}):
  layer = Dense.from_config(config)
"
"tf.keras.utils.SequenceEnqueuer(
    sequence, use_multiprocessing=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SequenceEnqueuer
    enqueuer = SequenceEnqueuer(...)
    enqueuer.start()
    datas = enqueuer.get()
    for data in datas:
    enqueuer.stop()
"
"tf.keras.utils.SidecarEvaluator(
    model,
    data,
    checkpoint_dir,
    steps=None,
    max_evaluations=None,
    callbacks=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SidecarEvaluator
model = tf.keras.models.Sequential(...)
model.compile(metrics=tf.keras.metrics.SparseCategoricalAccuracy(
    name=""eval_metrics""))
data = tf.data.Dataset.from_tensor_slices(...)

tf.keras.SidecarEvaluator(
    model=model,
    data=data,
    checkpoint_dir='/tmp/checkpoint_dir',
    callbacks=[tf.keras.callbacks.TensorBoard(log_dir='/tmp/log_dir')]
).start()
"
"tf.keras.utils.array_to_img(
    x, data_format=None, scale=True, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import array_to_img
from PIL import Image
img = np.random.random(size=(100, 100, 3))
pil_img = tf.keras.utils.array_to_img(img)
"
"tf.keras.utils.custom_object_scope(
    *args
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import custom_object_scope
layer = Dense(3, kernel_regularizer=my_regularizer)
config = layer.get_config()
...
with custom_object_scope({'my_regularizer': my_regularizer}):
  layer = Dense.from_config(config)
"
"tf.keras.utils.deserialize_keras_object(
    identifier,
    module_objects=None,
    custom_objects=None,
    printable_module_name='object'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import deserialize_keras_object
def deserialize(config, custom_objects=None):
   return deserialize_keras_object(
     identifier,
     module_objects=globals(),
     custom_objects=custom_objects,
     name=""MyObjectType"",
   )
"
"tf.keras.utils.experimental.DatasetCreator(
    dataset_fn, input_options=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import DatasetCreator
model = tf.keras.Sequential([tf.keras.layers.Dense(10)])
model.compile(tf.keras.optimizers.SGD(), loss=""mse"")

def dataset_fn(input_context):
  global_batch_size = 64
  batch_size = input_context.get_per_replica_batch_size(global_batch_size)
  dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat()
  dataset = dataset.shard(
      input_context.num_input_pipelines, input_context.input_pipeline_id)
  dataset = dataset.batch(batch_size)
  dataset = dataset.prefetch(2)
  return dataset

input_options = tf.distribute.InputOptions(
    experimental_fetch_to_device=True,
    experimental_per_replica_buffer_size=2)
model.fit(tf.keras.utils.experimental.DatasetCreator(
    dataset_fn, input_options=input_options), epochs=10, steps_per_epoch=10)
"
"tf.keras.utils.get_file(
    fname=None,
    origin=None,
    untar=False,
    md5_hash=None,
    file_hash=None,
    cache_subdir='datasets',
    hash_algorithm='auto',
    extract=False,
    archive_format='auto',
    cache_dir=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get_file
path_to_downloaded_file = tf.keras.utils.get_file(
    ""flower_photos"",
    ""https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"",
    untar=True)
"
"tf.keras.utils.get_registered_object(
    name, custom_objects=None, module_objects=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get_registered_object
def from_config(cls, config, custom_objects=None):
  if 'my_custom_object_name' in config:
    config['hidden_cls'] = tf.keras.utils.get_registered_object(
        config['my_custom_object_name'], custom_objects=custom_objects)
"
"tf.keras.utils.img_to_array(
    img, data_format=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import img_to_array
from PIL import Image
img_data = np.random.random(size=(100, 100, 3))
img = tf.keras.utils.array_to_img(img_data)
array = tf.keras.utils.image.img_to_array(img)
"
"tf.keras.utils.load_img(
    path,
    grayscale=False,
    color_mode='rgb',
    target_size=None,
    interpolation='nearest',
    keep_aspect_ratio=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import load_img
image = tf.keras.utils.load_img(image_path)
input_arr = tf.keras.utils.img_to_array(image)
predictions = model.predict(input_arr)
"
"tf.keras.utils.pack_x_y_sample_weight(
    x, y=None, sample_weight=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import pack_x_y_sample_weight
x = tf.ones((10, 1))
data = tf.keras.utils.pack_x_y_sample_weight(x)
isinstance(data, tf.Tensor)
True
y = tf.ones((10, 1))
data = tf.keras.utils.pack_x_y_sample_weight(x, y)
isinstance(data, tuple)
True
x, y = data"
"tf.keras.utils.pad_sequences(
    sequences,
    maxlen=None,
    dtype='int32',
    padding='pre',
    truncating='pre',
    value=0.0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import pad_sequences
sequence = [[1], [2, 3], [4, 5, 6]]
tf.keras.preprocessing.sequence.pad_sequences(sequence)
array([[0, 0, 1],
       [0, 2, 3],
       [4, 5, 6]], dtype=int32)"
"tf.keras.utils.plot_model(
    model,
    to_file='model.png',
    show_shapes=False,
    show_dtype=False,
    show_layer_names=True,
    rankdir='TB',
    expand_nested=False,
    dpi=96,
    layer_range=None,
    show_layer_activations=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import plot_model
input = tf.keras.Input(shape=(100,), dtype='int32', name='input')
x = tf.keras.layers.Embedding(
    output_dim=512, input_dim=10000, input_length=100)(input)
x = tf.keras.layers.LSTM(32)(x)
x = tf.keras.layers.Dense(64, activation='relu')(x)
x = tf.keras.layers.Dense(64, activation='relu')(x)
x = tf.keras.layers.Dense(64, activation='relu')(x)
output = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(x)
model = tf.keras.Model(inputs=[input], outputs=[output])
dot_img_file = '/tmp/model_1.png'
tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)
"
"tf.keras.utils.register_keras_serializable(
    package='Custom', name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import register_keras_serializable
@keras.utils.register_keras_serializable('my_package')
class MyDense(keras.layers.Dense):
  pass

assert keras.utils.get_registered_object('my_package>MyDense') == MyDense
assert keras.utils.get_registered_name(MyDense) == 'my_package>MyDense'
"
"tf.keras.utils.split_dataset(
    dataset, left_size=None, right_size=None, shuffle=False, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import split_dataset
data = np.random.random(size=(1000, 4))
left_ds, right_ds = tf.keras.utils.split_dataset(data, left_size=0.8)
int(left_ds.cardinality())
800
int(right_ds.cardinality())
200"
"tf.keras.utils.to_categorical(
    y, num_classes=None, dtype='float32'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import to_categorical
a = tf.keras.utils.to_categorical([0, 1, 2, 3], num_classes=4)
a = tf.constant(a, shape=[4, 4])
print(a)
tf.Tensor(
  [[1. 0. 0. 0.]
   [0. 1. 0. 0.]
   [0. 0. 1. 0.]
   [0. 0. 0. 1.]], shape=(4, 4), dtype=float32)"
"tf.keras.utils.unpack_x_y_sample_weight(
    data
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import unpack_x_y_sample_weight
features_batch = tf.ones((10, 5))
labels_batch = tf.zeros((10, 5))
data = (features_batch, labels_batch)
x, y, sample_weight = tf.keras.utils.unpack_x_y_sample_weight(data)
sample_weight is None
True"
"tf.keras.utils.warmstart_embedding_matrix(
    base_vocabulary,
    new_vocabulary,
    base_embeddings,
    new_embeddings_initializer='uniform'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import warmstart_embedding_matrix
>>> import keras
>>> vocab_base = tf.convert_to_tensor([""unk"", ""a"", ""b"", ""c""])
>>> vocab_new = tf.convert_to_tensor(
...        [""unk"", ""unk"", ""a"", ""b"", ""c"", ""d"", ""e""])
>>> vectorized_vocab_base = np.random.rand(vocab_base.shape[0], 3)
>>> vectorized_vocab_new = np.random.rand(vocab_new.shape[0], 3)
>>> warmstarted_embedding_matrix = warmstart_embedding_matrix(
...       base_vocabulary=vocab_base,
...       new_vocabulary=vocab_new,
...       base_embeddings=vectorized_vocab_base,
...       new_embeddings_initializer=keras.initializers.Constant(
...         vectorized_vocab_new))
"
"tf.math.less(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import less
x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.less(x, y) ==> [False, True, False]

x = tf.constant([5, 4, 6])
y = tf.constant([5, 6, 7])
tf.math.less(x, y) ==> [False, True, True]
"
"tf.math.less_equal(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import less_equal
x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.less_equal(x, y) ==> [True, True, False]

x = tf.constant([5, 4, 6])
y = tf.constant([5, 6, 6])
tf.math.less_equal(x, y) ==> [True, True, True]
"
"tf.linalg.LinearOperatorAdjoint(
    operator,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearOperatorAdjoint
operator = LinearOperatorFullMatrix([[1 - i., 3.], [0., 1. + i]])
operator_adjoint = LinearOperatorAdjoint(operator)

operator_adjoint.to_dense()
==> [[1. + i, 0.]
     [3., 1 - i]]

operator_adjoint.shape
==> [2, 2]

operator_adjoint.log_abs_determinant()
==> - log(2)

x = ... Shape [2, 4] Tensor
operator_adjoint.matmul(x)
==> Shape [2, 4] Tensor, equal to operator.matmul(x, adjoint=True)
"
"tf.linalg.LinearOperatorBlockDiag(
    operators,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=True,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearOperatorBlockDiag
operator_1 = LinearOperatorFullMatrix([[1., 2.], [3., 4.]])
operator_2 = LinearOperatorFullMatrix([[1., 0.], [0., 1.]])
operator = LinearOperatorBlockDiag([operator_1, operator_2])

operator.to_dense()
==> [[1., 2., 0., 0.],
     [3., 4., 0., 0.],
     [0., 0., 1., 0.],
     [0., 0., 0., 1.]]

operator.shape
==> [4, 4]

operator.log_abs_determinant()
==> scalar Tensor

operator.matmul(x)
==> tf.concat([operator_1.matmul(x1), operator_2.matmul(x2)])

operator_1 = LinearOperatorFullMatrix([[1.], [3.]])
operator_2 = LinearOperatorFullMatrix([[1., 6.]])
operator_3 = LinearOperatorFullMatrix([[2.], [7.]])
operator = LinearOperatorBlockDiag([operator_1, operator_2, operator_3])

operator.to_dense()
==> [[1., 0., 0., 0.],
     [3., 0., 0., 0.],
     [0., 1., 6., 0.],
     [0., 0., 0., 2.]]
     [0., 0., 0., 7.]]

operator.shape
==> [5, 4]


matrix_44 = tf.random.normal(shape=[2, 3, 4, 4])
operator_44 = LinearOperatorFullMatrix(matrix)

matrix_55 = tf.random.normal(shape=[1, 3, 5, 5])
operator_55 = LinearOperatorFullMatrix(matrix_55)

operator_99 = LinearOperatorBlockDiag([operator_44, operator_55])

x = tf.random.normal(shape=[2, 3, 9])
operator_99.matmul(x)
==> Shape [2, 3, 9] Tensor

x = [tf.random.normal(shape=[2, 3, 4]), tf.random.normal(shape=[2, 3, 5])]
operator_99.matmul(x)
==> [Shape [2, 3, 4] Tensor, Shape [2, 3, 5] Tensor]
"
"tf.linalg.LinearOperatorBlockLowerTriangular(
    operators,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name='LinearOperatorBlockLowerTriangular'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearOperatorBlockLowerTriangular
>>> operator_0 = tf.linalg.LinearOperatorFullMatrix([[1., 2.], [3., 4.]])
>>> operator_1 = tf.linalg.LinearOperatorFullMatrix([[1., 0.], [0., 1.]])
>>> operator_2 = tf.linalg.LinearOperatorLowerTriangular([[5., 6.], [7., 8]])
>>> operator = LinearOperatorBlockLowerTriangular(
...   [[operator_0], [operator_1, operator_2]])
"
"tf.linalg.LinearOperatorDiag(
    diag,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name='LinearOperatorDiag'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearOperatorDiag
diag = [1., -1.]
operator = LinearOperatorDiag(diag)

operator.to_dense()
==> [[1.,  0.]
     [0., -1.]]

operator.shape
==> [2, 2]

operator.log_abs_determinant()
==> scalar Tensor

x = ... Shape [2, 4] Tensor
operator.matmul(x)
==> Shape [2, 4] Tensor

diag = tf.random.normal(shape=[2, 3, 4])
operator = LinearOperatorDiag(diag)

y = tf.random.normal(shape=[2, 1, 4, 2])
x = operator.solve(y)
==> operator.matmul(x) = y
"
"tf.linalg.LinearOperatorFullMatrix(
    matrix,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name='LinearOperatorFullMatrix'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearOperatorFullMatrix
matrix = [[1., 2.], [3., 4.]]
operator = LinearOperatorFullMatrix(matrix)

operator.to_dense()
==> [[1., 2.]
     [3., 4.]]

operator.shape
==> [2, 2]

operator.log_abs_determinant()
==> scalar Tensor

x = ... Shape [2, 4] Tensor
operator.matmul(x)
==> Shape [2, 4] Tensor

matrix = tf.random.normal(shape=[2, 3, 4, 4])
operator = LinearOperatorFullMatrix(matrix)
"
"tf.linalg.LinearOperatorHouseholder(
    reflection_axis,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name='LinearOperatorHouseholder'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearOperatorHouseholder
vec = [1 / np.sqrt(2), 1. / np.sqrt(2)]
operator = LinearOperatorHouseholder(vec)

operator.to_dense()
==> [[0.,  -1.]
     [-1., -0.]]

operator.shape
==> [2, 2]

operator.log_abs_determinant()
==> scalar Tensor

x = ... Shape [2, 4] Tensor
operator.matmul(x)
==> Shape [2, 4] Tensor
"
"tf.linalg.LinearOperatorIdentity(
    num_rows,
    batch_shape=None,
    dtype=None,
    is_non_singular=True,
    is_self_adjoint=True,
    is_positive_definite=True,
    is_square=True,
    assert_proper_shapes=False,
    name='LinearOperatorIdentity'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearOperatorIdentity
operator = LinearOperatorIdentity(num_rows=2, dtype=tf.float32)

operator.to_dense()
==> [[1., 0.]
     [0., 1.]]

operator.shape
==> [2, 2]

operator.log_abs_determinant()
==> 0.

x = ... Shape [2, 4] Tensor
operator.matmul(x)
==> Shape [2, 4] Tensor, same as x.

y = tf.random.normal(shape=[3, 2, 4])
x = operator.solve(y)
==> Shape [3, 2, 4] Tensor, same as y.

operator = LinearOperatorIdentity(num_rows=2, batch_shape=[2])
operator.to_dense()
==> [[[1., 0.]
      [0., 1.]],
     [[1., 0.]
      [0., 1.]]]

x = ... Shape [2, 2, 3]
operator.matmul(x)
==> Shape [2, 2, 3] Tensor, same as x

x = ... Shape [1, 2, 3]
operator.matmul(x)
==> Shape [2, 2, 3] Tensor, equal to [x, x]
"
"tf.linalg.LinearOperatorInversion(
    operator,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearOperatorInversion
operator = LinearOperatorFullMatrix([[1., 0.], [0., 2.]])
operator_inv = LinearOperatorInversion(operator)

operator_inv.to_dense()
==> [[1., 0.]
     [0., 0.5]]

operator_inv.shape
==> [2, 2]

operator_inv.log_abs_determinant()
==> - log(2)

x = ... Shape [2, 4] Tensor
operator_inv.matmul(x)
==> Shape [2, 4] Tensor, equal to operator.solve(x)
"
"tf.linalg.LinearOperatorKronecker(
    operators,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearOperatorKronecker
operator_1 = LinearOperatorFullMatrix([[1., 2.], [3., 4.]])
operator_2 = LinearOperatorFullMatrix([[1., 0.], [2., 1.]])
operator = LinearOperatorKronecker([operator_1, operator_2])

operator.to_dense()
==> [[1., 0., 2., 0.],
     [2., 1., 4., 2.],
     [3., 0., 4., 0.],
     [6., 3., 8., 4.]]

operator.shape
==> [4, 4]

operator.log_abs_determinant()
==> scalar Tensor

x = ... Shape [4, 2] Tensor
operator.matmul(x)
==> Shape [4, 2] Tensor

matrix_45 = tf.random.normal(shape=[2, 3, 4, 5])
operator_45 = LinearOperatorFullMatrix(matrix)

matrix_56 = tf.random.normal(shape=[2, 3, 5, 6])
operator_56 = LinearOperatorFullMatrix(matrix_56)

operator_large = LinearOperatorKronecker([operator_45, operator_56])

x = tf.random.normal(shape=[2, 3, 6, 2])
operator_large.matmul(x)
==> Shape [2, 3, 30, 2] Tensor
"
"tf.linalg.LinearOperatorLowerTriangular(
    tril,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name='LinearOperatorLowerTriangular'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearOperatorLowerTriangular
tril = [[1., 2.], [3., 4.]]
operator = LinearOperatorLowerTriangular(tril)

operator.to_dense()
==> [[1., 0.]
     [3., 4.]]

operator.shape
==> [2, 2]

operator.log_abs_determinant()
==> scalar Tensor

x = ... Shape [2, 4] Tensor
operator.matmul(x)
==> Shape [2, 4] Tensor

tril = tf.random.normal(shape=[2, 3, 4, 4])
operator = LinearOperatorLowerTriangular(tril)
"
"tf.linalg.LinearOperatorPermutation(
    perm,
    dtype=tf.dtypes.float32,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name='LinearOperatorPermutation'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearOperatorPermutation
vec = [0, 2, 1]
operator = LinearOperatorPermutation(vec)

operator.to_dense()
==> [[1., 0., 0.]
     [0., 0., 1.]
     [0., 1., 0.]]

operator.shape
==> [3, 3]

operator.log_abs_determinant()
==> scalar Tensor

x = ... Shape [3, 4] Tensor
operator.matmul(x)
==> Shape [3, 4] Tensor
"
"tf.linalg.LinearOperatorTridiag(
    diagonals,
    diagonals_format=_COMPACT,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name='LinearOperatorTridiag'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearOperatorTridiag
superdiag = [3., 4., 5.]
diag = [1., -1., 2.]
subdiag = [6., 7., 8]
operator = tf.linalg.LinearOperatorTridiag(
   [superdiag, diag, subdiag],
   diagonals_format='sequence')
operator.to_dense()
<tf.Tensor: shape=(3, 3), dtype=float32, numpy=
<tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[ 1.,  3.,  0.],
       [ 7., -1.,  4.],
       [ 0.,  8.,  2.]], dtype=float32)>
operator.shape
TensorShape([3, 3])"
"tf.linalg.LinearOperatorZeros(
    num_rows,
    num_columns=None,
    batch_shape=None,
    dtype=None,
    is_non_singular=False,
    is_self_adjoint=True,
    is_positive_definite=False,
    is_square=True,
    assert_proper_shapes=False,
    name='LinearOperatorZeros'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LinearOperatorZeros
operator = LinearOperatorZero(num_rows=2, dtype=tf.float32)

operator.to_dense()
==> [[0., 0.]
     [0., 0.]]

operator.shape
==> [2, 2]

operator.determinant()
==> 0.

x = ... Shape [2, 4] Tensor
operator.matmul(x)
==> Shape [2, 4] Tensor, same as x.

operator = LinearOperatorZeros(num_rows=2, batch_shape=[2])
operator.to_dense()
==> [[[0., 0.]
      [0., 0.]],
     [[0., 0.]
      [0., 0.]]]

x = ... Shape [2, 2, 3]
operator.matmul(x)
==> Shape [2, 2, 3] Tensor, same as tf.zeros_like(x)

x = ... Shape [1, 2, 3]
operator.matmul(x)
==> Shape [2, 2, 3] Tensor, equal to tf.zeros_like([x, x])
"
"tf.linalg.adjoint(
    matrix, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import adjoint
x = tf.constant([[1 + 1j, 2 + 2j, 3 + 3j],
                 [4 + 4j, 5 + 5j, 6 + 6j]])
"
"tf.linalg.band_part(
    input, num_lower, num_upper, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import band_part

tf.linalg.band_part(input, 1, -1) ==> [[ 0,  1,  2, 3]
                                       [-1,  0,  1, 2]
                                       [ 0, -1,  0, 1]
                                       [ 0,  0, -1, 0]],

tf.linalg.band_part(input, 2, 1) ==> [[ 0,  1,  0, 0]
                                      [-1,  0,  1, 0]
                                      [-2, -1,  0, 1]
                                      [ 0, -2, -1, 0]]
"
"tf.linalg.banded_triangular_solve(
    bands, rhs, lower=True, adjoint=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import banded_triangular_solve
x = [[2., 3., 4.], [1., 2., 3.]]
x2 = [[2., 3., 4.], [10000., 2., 3.]]
y = tf.zeros([3, 3])
z = tf.linalg.set_diag(y, x, align='LEFT_RIGHT', k=(-1, 0))
z
<tf.Tensor: shape=(3, 3), dtype=float32, numpy=
<tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[2., 0., 0.],
       [2., 3., 0.],
       [0., 3., 4.]], dtype=float32)>
soln = tf.linalg.banded_triangular_solve(x, tf.ones([3, 1]))
soln
<tf.Tensor: shape=(3, 1), dtype=float32, numpy=
<tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[0.5 ],
       [0.  ],
       [0.25]], dtype=float32)>
are_equal = soln == tf.linalg.banded_triangular_solve(x2, tf.ones([3, 1]))
tf.reduce_all(are_equal).numpy()
True
are_equal = soln == tf.linalg.triangular_solve(z, tf.ones([3, 1]))
tf.reduce_all(are_equal).numpy()
True"
"tf.linalg.cholesky_solve(
    chol, rhs, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import cholesky_solve

...
"
"tf.linalg.diag(
    diagonal,
    name='diag',
    k=0,
    num_rows=-1,
    num_cols=-1,
    padding_value=0,
    align='RIGHT_LEFT'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
output[i, j, ..., l, m, n]
  = diagonal[i, j, ..., l, n-max(d_upper, 0)] ; if n - m == d_upper
    padding_value                             ; otherwise
"
"tf.linalg.eigh_tridiagonal(
    alpha,
    beta,
    eigvals_only=True,
    select='a',
    select_range=None,
    tol=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import eigh_tridiagonal
import numpy
eigvals = tf.linalg.eigh_tridiagonal([0.0, 0.0, 0.0], [1.0, 1.0])
eigvals_expected = [-numpy.sqrt(2.0), 0.0, numpy.sqrt(2.0)]
tf.assert_near(eigvals_expected, eigvals)
"
"tf.eye(
    num_rows,
    num_columns=None,
    batch_shape=None,
    dtype=tf.dtypes.float32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import eye
tf.eye(2)
==> [[1., 0.],
     [0., 1.]]

batch_identity = tf.eye(2, batch_shape=[3])

tf.eye(2, num_columns=3)
==> [[ 1.,  0.,  0.],
     [ 0.,  1.,  0.]]
"
"tf.linalg.logdet(
    matrix, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import logdet
underflow:
"
"tf.linalg.lu_matrix_inverse(
    lower_upper, perm, validate_args=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import lu_matrix_inverse
inv_X = tf.lu_matrix_inverse(*tf.linalg.lu(X))
tf.assert_near(tf.matrix_inverse(X), inv_X)
"
"tf.linalg.lu_reconstruct(
    lower_upper, perm, validate_args=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import lu_reconstruct
import numpy as np
import tensorflow as tf
import tensorflow_probability as tfp

x = [[[3., 4], [1, 2]],
     [[7., 8], [3, 4]]]
x_reconstructed = tf.linalg.lu_reconstruct(*tf.linalg.lu(x))
tf.assert_near(x, x_reconstructed)
"
"tf.linalg.lu_solve(
    lower_upper, perm, rhs, validate_args=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import lu_solve
import numpy as np
import tensorflow as tf
import tensorflow_probability as tfp

x = [[[1., 2],
      [3, 4]],
     [[7, 8],
      [3, 4]]]
inv_x = tf.linalg.lu_solve(*tf.linalg.lu(x), rhs=tf.eye(2))
tf.assert_near(tf.matrix_inverse(x), inv_x)
"
"tf.linalg.matmul(
    a,
    b,
    transpose_a=False,
    transpose_b=False,
    adjoint_a=False,
    adjoint_b=False,
    a_is_sparse=False,
    b_is_sparse=False,
    output_type=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import matmul
a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
array([[1, 2, 3],
       [4, 5, 6]], dtype=int32)>
b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
array([[ 7,  8],
       [ 9, 10],
       [11, 12]], dtype=int32)>
c = tf.matmul(a, b)
<tf.Tensor: shape=(2, 2), dtype=int32, numpy=
<tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[ 58,  64],
       [139, 154]], dtype=int32)>"
"tf.linalg.matrix_transpose(
    a, name='matrix_transpose', conjugate=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import matrix_transpose
x = tf.constant([[1, 2, 3], [4, 5, 6]])

x = tf.constant([[1 + 1j, 2 + 2j, 3 + 3j],
                 [4 + 4j, 5 + 5j, 6 + 6j]])

"
"tf.linalg.matvec(
    a,
    b,
    transpose_a=False,
    adjoint_a=False,
    a_is_sparse=False,
    b_is_sparse=False,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import matvec
a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])

b = tf.constant([7, 9, 11], shape=[3])

c = tf.linalg.matvec(a, b)


a = tf.constant(np.arange(1, 13, dtype=np.int32),
                shape=[2, 2, 3])

b = tf.constant(np.arange(13, 19, dtype=np.int32),
                shape=[2, 3])

c = tf.linalg.matvec(a, b)
"
"tf.linalg.pinv(
    a, rcond=None, validate_args=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import pinv
import tensorflow as tf
import tensorflow_probability as tfp

a = tf.constant([[1.,  0.4,  0.5],
                 [0.4, 0.2,  0.25],
                 [0.5, 0.25, 0.35]])
tf.matmul(tf.linalg.pinv(a), a)
             [0., 1., 0.],
             [0., 0., 1.]], dtype=float32)

a = tf.constant([[1.,  0.4,  0.5,  1.],
                 [0.4, 0.2,  0.25, 2.],
                 [0.5, 0.25, 0.35, 3.]])
tf.matmul(tf.linalg.pinv(a), a)
             [ 0.37,  0.43, -0.33,  0.02],
             [ 0.21, -0.33,  0.81,  0.01],
             [-0.02,  0.02,  0.01,  1.  ]], dtype=float32)
"
"tf.linalg.qr(
    input, full_matrices=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import qr
q, r = qr(a)
q_full, r_full = qr(a, full_matrices=True)
"
"tf.linalg.svd(
    tensor, full_matrices=False, compute_uv=True, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import svd
s, u, v = svd(a)
s = svd(a, compute_uv=False)
"
"tf.linalg.tensor_diag_part(
    input, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import tensor_diag_part
x = [[[[1111,1112],[1121,1122]],
      [[1211,1212],[1221,1222]]],
     [[[2111, 2112], [2121, 2122]],
      [[2211, 2212], [2221, 2222]]]
     ]
tf.linalg.tensor_diag_part(x)
<tf.Tensor: shape=(2, 2), dtype=int32, numpy=
<tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[1111, 1212],
       [2121, 2222]], dtype=int32)>
tf.linalg.diag_part(x).shape
TensorShape([2, 2, 2])"
"tf.linalg.trace(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import trace
x = tf.constant([[1, 2], [3, 4]])

x = tf.constant([[1, 2, 3],
                 [4, 5, 6],
                 [7, 8, 9]])

x = tf.constant([[[1, 2, 3],
                  [4, 5, 6],
                  [7, 8, 9]],
                 [[-1, -2, -3],
                  [-4, -5, -6],
                  [-7, -8, -9]]])
"
"tf.linalg.tridiagonal_matmul(
    diagonals, rhs, diagonals_format='compact', name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import tridiagonal_matmul
superdiag = tf.constant([-1, -1, 0], dtype=tf.float64)
maindiag = tf.constant([2, 2, 2], dtype=tf.float64)
subdiag = tf.constant([0, -1, -1], dtype=tf.float64)
diagonals = [superdiag, maindiag, subdiag]
rhs = tf.constant([[1, 1], [1, 1], [1, 1]], dtype=tf.float64)
x = tf.linalg.tridiagonal_matmul(diagonals, rhs, diagonals_format='sequence')
"
"tf.linalg.tridiagonal_solve(
    diagonals,
    rhs,
    diagonals_format='compact',
    transpose_rhs=False,
    conjugate_rhs=False,
    name=None,
    partial_pivoting=True,
    perturb_singular=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import tridiagonal_solve
rhs = tf.constant([...])
matrix = tf.constant([[...]])
m = matrix.shape[0]
diagonals=tf.gather_nd(matrix, indices)
x = tf.linalg.tridiagonal_solve(diagonals, rhs)
"
"tf.linspace(
    start, stop, num, name=None, axis=0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import linspace
tf.linspace(10.0, 12.0, 3, name=""linspace"") => [ 10.0  11.0  12.0]
"
"tf.lite.TFLiteConverter(
    funcs, trackable_obj=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
  converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
  tflite_model = converter.convert()

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

converter = tf.lite.TFLiteConverter.from_concrete_functions([func], model)
tflite_model = converter.convert()

converter = tf.lite.TFLiteConverter.experimental_from_jax([func], [[
    ('input1', input1), ('input2', input2)]])
tflite_model = converter.convert()
"
"tf.lite.experimental.authoring.compatible(
    target=None, converter_target_spec=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
@tf.lite.experimental.authoring.compatible
@tf.function(input_signature=[
    tf.TensorSpec(shape=[None], dtype=tf.float32)
])
def f(x):
    return tf.cosh(x)

result = f(tf.constant([0.0]))
#   - <stdin>:6
"
"tf.lite.experimental.load_delegate(
    library, options=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import load_delegate
import tensorflow as tf

try:
  delegate = tf.lite.experimental.load_delegate('delegate.so')
except ValueError:
  // Fallback to CPU

if delegate:
  interpreter = tf.lite.Interpreter(
      model_path='model.tflite',
      experimental_delegates=[delegate])
else:
  interpreter = tf.lite.Interpreter(model_path='model.tflite')
"
"tf.math.logical_and(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import logical_and
>>> a = tf.constant([True])
>>> b = tf.constant([False])
>>> tf.math.logical_and(a, b)
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>
>>> a & b
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>
"
"tf.math.logical_not(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import logical_not
tf.math.logical_not(tf.constant([True, False]))
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>"
"tf.math.logical_or(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import logical_or
>>> a = tf.constant([True])
>>> b = tf.constant([False])
>>> tf.math.logical_or(a, b)
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>
>>> a | b
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>
"
"tf.lookup.KeyValueTensorInitializer(
    keys, values, key_dtype=None, value_dtype=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import KeyValueTensorInitializer
keys_tensor = tf.constant(['a', 'b', 'c'])
vals_tensor = tf.constant([7, 8, 9])
input_tensor = tf.constant(['a', 'f'])
init = tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor)
table = tf.lookup.StaticHashTable(
    init,
    default_value=-1)
table.lookup(input_tensor).numpy()
array([ 7, -1], dtype=int32)"
"tf.lookup.StaticHashTable(
    initializer, default_value, name=None, experimental_is_anonymous=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import StaticHashTable
keys_tensor = tf.constant(['a', 'b', 'c'])
vals_tensor = tf.constant([7, 8, 9])
input_tensor = tf.constant(['a', 'f'])
table = tf.lookup.StaticHashTable(
    tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor),
    default_value=-1)
table.lookup(input_tensor).numpy()
array([ 7, -1], dtype=int32)"
"tf.lookup.StaticVocabularyTable(
    initializer,
    num_oov_buckets,
    lookup_key_dtype=None,
    name=None,
    experimental_is_anonymous=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import StaticVocabularyTable
init = tf.lookup.KeyValueTensorInitializer(
    keys=tf.constant(['emerson', 'lake', 'palmer']),
    values=tf.constant([0, 1, 2], dtype=tf.int64))
table = tf.lookup.StaticVocabularyTable(
   init,
   num_oov_buckets=5)"
"tf.lookup.experimental.DenseHashTable(
    key_dtype,
    value_dtype,
    default_value,
    empty_key,
    deleted_key,
    initial_num_buckets=None,
    name='MutableDenseHashTable',
    checkpoint=True,
    experimental_is_anonymous=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import DenseHashTable
table = tf.lookup.experimental.DenseHashTable(
    key_dtype=tf.string,
    value_dtype=tf.int64,
    default_value=-1,
    empty_key='',
    deleted_key='$')
keys = tf.constant(['a', 'b', 'c'])
values = tf.constant([0, 1, 2], dtype=tf.int64)
table.insert(keys, values)
table.remove(tf.constant(['c']))
table.lookup(tf.constant(['a', 'b', 'c','d'])).numpy()
array([ 0,  1, -1, -1])"
"tf.lookup.experimental.MutableHashTable(
    key_dtype,
    value_dtype,
    default_value,
    name='MutableHashTable',
    checkpoint=True,
    experimental_is_anonymous=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MutableHashTable
table = tf.lookup.experimental.MutableHashTable(key_dtype=tf.string,
                                                value_dtype=tf.int64,
                                                default_value=-1)
keys_tensor = tf.constant(['a', 'b', 'c'])
vals_tensor = tf.constant([7, 8, 9], dtype=tf.int64)
input_tensor = tf.constant(['a', 'f'])
table.insert(keys_tensor, vals_tensor)
table.lookup(input_tensor).numpy()
array([ 7, -1])
table.remove(tf.constant(['c']))
table.lookup(keys_tensor).numpy()
array([ 7, 8, -1])
sorted(table.export()[0].numpy())
[b'a', b'b']
sorted(table.export()[1].numpy())
[7, 8]"
"tf.keras.losses.BinaryCrossentropy(
    from_logits=False,
    label_smoothing=0.0,
    axis=-1,
    reduction=losses_utils.ReductionV2.AUTO,
    name='binary_crossentropy'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import BinaryCrossentropy
model.compile(
  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
  ....
)
"
"tf.keras.losses.BinaryFocalCrossentropy(
    apply_class_balancing=False,
    alpha=0.25,
    gamma=2.0,
    from_logits=False,
    label_smoothing=0.0,
    axis=-1,
    reduction=losses_utils.ReductionV2.AUTO,
    name='binary_focal_crossentropy'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import BinaryFocalCrossentropy
model.compile(
  loss=tf.keras.losses.BinaryFocalCrossentropy(gamma=2.0, from_logits=True),
  ....
)
"
"tf.keras.losses.CategoricalCrossentropy(
    from_logits=False,
    label_smoothing=0.0,
    axis=-1,
    reduction=losses_utils.ReductionV2.AUTO,
    name='categorical_crossentropy'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CategoricalCrossentropy
y_true = [[0, 1, 0], [0, 0, 1]]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
cce = tf.keras.losses.CategoricalCrossentropy()
cce(y_true, y_pred).numpy()
1.177"
"tf.keras.losses.CategoricalHinge(
    reduction=losses_utils.ReductionV2.AUTO, name='categorical_hinge'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CategoricalHinge
y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
h = tf.keras.losses.CategoricalHinge()
h(y_true, y_pred).numpy()
1.4"
"tf.keras.losses.CosineSimilarity(
    axis=-1,
    reduction=losses_utils.ReductionV2.AUTO,
    name='cosine_similarity'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CosineSimilarity
y_true = [[0., 1.], [1., 1.]]
y_pred = [[1., 0.], [1., 1.]]
cosine_loss = tf.keras.losses.CosineSimilarity(axis=1)
cosine_loss(y_true, y_pred).numpy()
-0.5"
"tf.keras.losses.Hinge(
    reduction=losses_utils.ReductionV2.AUTO, name='hinge'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Hinge
y_true = [[0., 1.], [0., 0.]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
h = tf.keras.losses.Hinge()
h(y_true, y_pred).numpy()
1.3"
"tf.keras.losses.KLDivergence(
    reduction=losses_utils.ReductionV2.AUTO, name='kl_divergence'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import KLDivergence
y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
kl = tf.keras.losses.KLDivergence()
kl(y_true, y_pred).numpy()
0.458"
"tf.keras.losses.LogCosh(
    reduction=losses_utils.ReductionV2.AUTO, name='log_cosh'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LogCosh
y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [0., 0.]]
l = tf.keras.losses.LogCosh()
l(y_true, y_pred).numpy()
0.108"
"tf.keras.losses.Loss(
    reduction=losses_utils.ReductionV2.AUTO, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
class MeanSquaredError(Loss):

  def call(self, y_true, y_pred):
    return tf.reduce_mean(tf.math.square(y_pred - y_true), axis=-1)
"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_percentage_error
y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_logarithmic_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.losses.MeanAbsoluteError(
    reduction=losses_utils.ReductionV2.AUTO,
    name='mean_absolute_error'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanAbsoluteError
y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [1., 0.]]
mae = tf.keras.losses.MeanAbsoluteError()
mae(y_true, y_pred).numpy()
0.5"
"tf.keras.losses.MeanAbsolutePercentageError(
    reduction=losses_utils.ReductionV2.AUTO,
    name='mean_absolute_percentage_error'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanAbsolutePercentageError
y_true = [[2., 1.], [2., 3.]]
y_pred = [[1., 1.], [1., 0.]]
mape = tf.keras.losses.MeanAbsolutePercentageError()
mape(y_true, y_pred).numpy()
50."
"tf.keras.losses.MeanSquaredError(
    reduction=losses_utils.ReductionV2.AUTO,
    name='mean_squared_error'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanSquaredError
y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [1., 0.]]
mse = tf.keras.losses.MeanSquaredError()
mse(y_true, y_pred).numpy()
0.5"
"tf.keras.losses.MeanSquaredLogarithmicError(
    reduction=losses_utils.ReductionV2.AUTO,
    name='mean_squared_logarithmic_error'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanSquaredLogarithmicError
y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [1., 0.]]
msle = tf.keras.losses.MeanSquaredLogarithmicError()
msle(y_true, y_pred).numpy()
0.240"
"tf.keras.losses.Poisson(
    reduction=losses_utils.ReductionV2.AUTO, name='poisson'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Poisson
y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [0., 0.]]
p = tf.keras.losses.Poisson()
p(y_true, y_pred).numpy()
0.5"
"tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=False,
    ignore_class=None,
    reduction=losses_utils.ReductionV2.AUTO,
    name='sparse_categorical_crossentropy'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SparseCategoricalCrossentropy
y_true = [1, 2]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
scce = tf.keras.losses.SparseCategoricalCrossentropy()
scce(y_true, y_pred).numpy()
1.177"
"tf.keras.losses.SquaredHinge(
    reduction=losses_utils.ReductionV2.AUTO, name='squared_hinge'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SquaredHinge
y_true = [[0., 1.], [0., 0.]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
h = tf.keras.losses.SquaredHinge()
h(y_true, y_pred).numpy()
1.86"
"tf.keras.metrics.binary_crossentropy(
    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import binary_crossentropy
y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.916 , 0.714], dtype=float32)"
"tf.keras.metrics.binary_focal_crossentropy(
    y_true,
    y_pred,
    apply_class_balancing=False,
    alpha=0.25,
    gamma=2.0,
    from_logits=False,
    label_smoothing=0.0,
    axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import binary_focal_crossentropy
y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
loss = tf.keras.losses.binary_focal_crossentropy(y_true, y_pred,
                                                 gamma=2)
assert loss.shape == (2,)
loss.numpy()
array([0.330, 0.206], dtype=float32)"
"tf.keras.metrics.categorical_crossentropy(
    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import categorical_crossentropy
y_true = [[0, 1, 0], [0, 0, 1]]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.0513, 2.303], dtype=float32)"
"tf.keras.losses.categorical_hinge(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import categorical_hinge
y_true = np.random.randint(0, 3, size=(2,))
y_true = tf.keras.utils.to_categorical(y_true, num_classes=3)
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.categorical_hinge(y_true, y_pred)
assert loss.shape == (2,)
pos = np.sum(y_true * y_pred, axis=-1)
neg = np.amax((1. - y_true) * y_pred, axis=-1)
assert np.array_equal(loss.numpy(), np.maximum(0., neg - pos + 1.))"
"tf.keras.losses.cosine_similarity(
    y_true, y_pred, axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import cosine_similarity
y_true = [[0., 1.], [1., 1.], [1., 1.]]
y_pred = [[1., 0.], [1., 1.], [-1., -1.]]
loss = tf.keras.losses.cosine_similarity(y_true, y_pred, axis=1)
loss.numpy()
array([-0., -0.999, 0.999], dtype=float32)"
"tf.keras.losses.get(
    identifier
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get
loss = tf.keras.losses.get(""categorical_crossentropy"")
type(loss)
<class 'function'>
<class 'function'>
loss = tf.keras.losses.get(""CategoricalCrossentropy"")
type(loss)
<class '...keras.losses.CategoricalCrossentropy'>
<class '...keras.losses.CategoricalCrossentropy'>"
"tf.keras.metrics.hinge(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import hinge
y_true = np.random.choice([-1, 1], size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.hinge(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    np.mean(np.maximum(1. - y_true * y_pred, 0.), axis=-1))"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_percentage_error
y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_percentage_error
y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_logarithmic_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_logarithmic_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.poisson(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import poisson
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.poisson(y_true, y_pred)
assert loss.shape == (2,)
y_pred = y_pred + 1e-7
assert np.allclose(
    loss.numpy(), np.mean(y_pred - y_true * np.log(y_pred), axis=-1),
    atol=1e-5)"
"tf.keras.metrics.sparse_categorical_crossentropy(
    y_true, y_pred, from_logits=False, axis=-1, ignore_class=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sparse_categorical_crossentropy
y_true = [1, 2]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.0513, 2.303], dtype=float32)"
"tf.keras.metrics.squared_hinge(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import squared_hinge
y_true = np.random.choice([-1, 1], size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.squared_hinge(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    np.mean(np.square(np.maximum(1. - y_true * y_pred, 0.)), axis=-1))"
"tf.make_ndarray(
    tensor
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import make_ndarray
a = tf.constant([[1,2,3],[4,5,6]])
"
"tf.make_tensor_proto(
    values, dtype=None, shape=None, verify_shape=False, allow_broadcast=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import make_tensor_proto
  request = tensorflow_serving.apis.predict_pb2.PredictRequest()
  request.model_spec.name = ""my_model""
  request.model_spec.signature_name = ""serving_default""
  request.inputs[""images""].CopyFrom(tf.make_tensor_proto(X_new))
"
"tf.map_fn(
    fn,
    elems,
    dtype=None,
    parallel_iterations=None,
    back_prop=True,
    swap_memory=False,
    infer_shape=True,
    name=None,
    fn_output_signature=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import map_fn
tf.map_fn(fn=lambda t: tf.range(t, t + 3), elems=tf.constant([3, 5, 2]))
<tf.Tensor: shape=(3, 3), dtype=int32, numpy=
<tf.Tensor: shape=(3, 3), dtype=int32, numpy=
  array([[3, 4, 5],
         [5, 6, 7],
         [2, 3, 4]], dtype=int32)>"
"tf.math.abs(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import abs
x = tf.constant([-2.25, 3.25])
tf.abs(x)
<tf.Tensor: shape=(2,), dtype=float32,
<tf.Tensor: shape=(2,), dtype=float32,
numpy=array([2.25, 3.25], dtype=float32)>"
"tf.math.accumulate_n(
    inputs, shape=None, tensor_dtype=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import accumulate_n
a = tf.constant([[1, 2], [3, 4]])
b = tf.constant([[5, 0], [0, 6]])
tf.math.accumulate_n([a, b, a]).numpy()
array([[ 7, 4],
       [ 6, 14]], dtype=int32)"
"tf.math.acos(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import acos
x = tf.constant([1.0, -0.5, 3.4, 0.2, 0.0, -2], dtype = tf.float32)
tf.math.acos(x)
<tf.Tensor: shape=(6,), dtype=float32,
<tf.Tensor: shape=(6,), dtype=float32,
numpy= array([0. , 2.0943952, nan, 1.3694383, 1.5707964, nan],
dtype=float32)>"
"tf.math.acosh(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import acosh
x = tf.constant([-2, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
tf.math.acosh(x) ==> [nan nan 0. 0.62236255 5.9914584 9.903487 inf]
"
"tf.math.add(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import add
x = [1, 2, 3, 4, 5]
y = 1
tf.add(x, y)
<tf.Tensor: shape=(5,), dtype=int32, numpy=array([2, 3, 4, 5, 6],
<tf.Tensor: shape=(5,), dtype=int32, numpy=array([2, 3, 4, 5, 6],
dtype=int32)>"
"tf.math.add_n(
    inputs, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import add_n
a = tf.constant([[3, 5], [4, 8]])
b = tf.constant([[1, 6], [2, 9]])
tf.math.add_n([a, b, a]).numpy()
array([[ 7, 16],
       [10, 25]], dtype=int32)"
"tf.math.angle(
    input, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import angle
input = tf.constant([-2.25 + 4.75j, 3.25 + 5.75j], dtype=tf.complex64)
tf.math.angle(input).numpy()
"
"tf.math.approx_max_k(
    operand,
    k,
    reduction_dimension=-1,
    recall_target=0.95,
    reduction_input_size_override=-1,
    aggregate_to_topk=True,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import approx_max_k
import tensorflow as tf
@tf.function(jit_compile=True)
def mips(qy, db, k=10, recall_target=0.95):
  dists = tf.einsum('ik,jk->ij', qy, db)
  return tf.nn.approx_max_k(dists, k=k, recall_target=recall_target)
qy = tf.random.uniform((256,128))
db = tf.random.uniform((2048,128))
dot_products, neighbors = mips(qy, db, k=20)"
"tf.math.approx_min_k(
    operand,
    k,
    reduction_dimension=-1,
    recall_target=0.95,
    reduction_input_size_override=-1,
    aggregate_to_topk=True,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import approx_min_k
import tensorflow as tf
@tf.function(jit_compile=True)
def l2_ann(qy, db, half_db_norms, k=10, recall_target=0.95):
  dists = half_db_norms - tf.einsum('ik,jk->ij', qy, db)
  return tf.nn.approx_min_k(dists, k=k, recall_target=recall_target)
qy = tf.random.uniform((256,128))
db = tf.random.uniform((2048,128))
half_db_norms = tf.norm(db, axis=1) / 2
dists, neighbors = l2_ann(qy, db, half_db_norms)"
"tf.math.argmax(
    input,
    axis=None,
    output_type=tf.dtypes.int64,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import argmax
A = tf.constant([2, 20, 30, 3, 6])
<tf.Tensor: shape=(), dtype=int64, numpy=2>
<tf.Tensor: shape=(), dtype=int64, numpy=2>
B = tf.constant([[2, 20, 30, 3, 6], [3, 11, 16, 1, 8],
                 [14, 45, 23, 5, 27]])
tf.math.argmax(B, 0)
<tf.Tensor: shape=(5,), dtype=int64, numpy=array([2, 2, 0, 2, 2])>
<tf.Tensor: shape=(5,), dtype=int64, numpy=array([2, 2, 0, 2, 2])>
tf.math.argmax(B, 1)
<tf.Tensor: shape=(3,), dtype=int64, numpy=array([2, 2, 1])>
<tf.Tensor: shape=(3,), dtype=int64, numpy=array([2, 2, 1])>
C = tf.constant([0, 0, 0, 0])
<tf.Tensor: shape=(), dtype=int64, numpy=0>
<tf.Tensor: shape=(), dtype=int64, numpy=0>"
"tf.math.argmin(
    input,
    axis=None,
    output_type=tf.dtypes.int64,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import argmin
import tensorflow as tf
a = [1, 10, 26.9, 2.8, 166.32, 62.3]
b = tf.math.argmin(input = a)
c = tf.keras.backend.eval(b)
"
"tf.math.asin(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import asin
x = tf.constant([1.047, 0.785])

"
"tf.math.asinh(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import asinh
  x = tf.constant([-float(""inf""), -2, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
  tf.math.asinh(x) ==> [-inf -1.4436355 -0.4812118 0.8813736 1.0159732 5.991471 9.903487 inf]
"
"tf.math.atan(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import atan
x = tf.constant([1.047, 0.785])

"
"tf.math.atan2(
    y, x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import atan2
x = [1., 1.]
y = [1., -1.]
print((tf.math.atan2(y,x) * (180 / np.pi)).numpy())
[ 45. -45.]"
"tf.math.atanh(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import atanh
  x = tf.constant([-float(""inf""), -1, -0.5, 1, 0, 0.5, 10, float(""inf"")])
  tf.math.atanh(x) ==> [nan -inf -0.54930615 inf  0. 0.54930615 nan nan]
"
"tf.math.bessel_i0(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bessel_i0
tf.math.special.bessel_i0([-1., -0.5, 0.5, 1.]).numpy()
array([1.26606588, 1.06348337, 1.06348337, 1.26606588], dtype=float32)"
"tf.math.bessel_i0e(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bessel_i0e
tf.math.special.bessel_i0e([-1., -0.5, 0.5, 1.]).numpy()
array([0.46575961, 0.64503527, 0.64503527, 0.46575961], dtype=float32)"
"tf.math.bessel_i1(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bessel_i1
tf.math.special.bessel_i1([-1., -0.5, 0.5, 1.]).numpy()
array([-0.5651591 , -0.25789431,  0.25789431,  0.5651591 ], dtype=float32)"
"tf.math.bessel_i1e(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bessel_i1e
tf.math.special.bessel_i1e([-1., -0.5, 0.5, 1.]).numpy()
array([-0.20791042, -0.15642083,  0.15642083,  0.20791042], dtype=float32)"
"tf.math.bincount(
    arr,
    weights=None,
    minlength=None,
    maxlength=None,
    dtype=tf.dtypes.int32,
    name=None,
    axis=None,
    binary_output=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bincount
values = tf.constant([1,1,2,3,2,4,4,5])
tf.math.bincount(values) #[0 2 2 1 2 1]
"
"tf.math.ceil(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ceil
tf.math.ceil([-1.7, -1.5, -0.2, 0.2, 1.5, 1.7, 2.0])
<tf.Tensor: shape=(7,), dtype=float32,
<tf.Tensor: shape=(7,), dtype=float32,
numpy=array([-1., -1., -0.,  1.,  2.,  2.,  2.], dtype=float32)>"
"tf.math.confusion_matrix(
    labels,
    predictions,
    num_classes=None,
    weights=None,
    dtype=tf.dtypes.int32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import confusion_matrix
  tf.math.confusion_matrix([1, 2, 4], [2, 2, 4]) ==>
      [[0 0 0 0 0]
       [0 0 1 0 0]
       [0 0 1 0 0]
       [0 0 0 0 0]
       [0 0 0 0 1]]
"
"tf.math.conj(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import conj
x = tf.constant([-2.25 + 4.75j, 3.25 + 5.75j])
tf.math.conj(x)
<tf.Tensor: shape=(2,), dtype=complex128,
<tf.Tensor: shape=(2,), dtype=complex128,
numpy=array([-2.25-4.75j,  3.25-5.75j])>"
"tf.math.cos(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import cos
  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
  tf.math.cos(x) ==> [nan -0.91113025 0.87758255 0.5403023 0.36235774 0.48718765 -0.95215535 nan]
"
"tf.math.cosh(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import cosh
  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 2, 10, float(""inf"")])
  tf.math.cosh(x) ==> [inf 4.0515420e+03 1.1276259e+00 1.5430807e+00 1.8106556e+00 3.7621956e+00 1.1013233e+04 inf]
"
"tf.math.count_nonzero(
    input,
    axis=None,
    keepdims=None,
    dtype=tf.dtypes.int64,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import count_nonzero
x = tf.constant([[0, 1, 0], [1, 1, 0]])
"
"tf.math.cumprod(
    x, axis=0, exclusive=False, reverse=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import cumprod
"
"tf.math.cumsum(
    x, axis=0, exclusive=False, reverse=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import cumsum
x = tf.constant([2, 4, 6, 8])
tf.cumsum(x)
<tf.Tensor: shape=(4,), dtype=int32,
<tf.Tensor: shape=(4,), dtype=int32,
numpy=array([ 2,  6, 12, 20], dtype=int32)>"
"tf.math.divide(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import divide
x = tf.constant([16, 12, 11])
y = tf.constant([4, 6, 2])
tf.divide(x,y)
<tf.Tensor: shape=(3,), dtype=float64,
<tf.Tensor: shape=(3,), dtype=float64,
numpy=array([4. , 2. , 5.5])>"
"tf.math.divide_no_nan(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import divide_no_nan
tf.constant(3.0) / 0.0
<tf.Tensor: shape=(), dtype=float32, numpy=inf>
<tf.Tensor: shape=(), dtype=float32, numpy=inf>
tf.math.divide_no_nan(3.0, 0.0)
<tf.Tensor: shape=(), dtype=float32, numpy=0.0>
<tf.Tensor: shape=(), dtype=float32, numpy=0.0>"
"tf.math.equal(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import equal
x = tf.constant([2, 4])
y = tf.constant(2)
tf.math.equal(x, y)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  False])>
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  False])>"
"tf.math.erf(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import erf
tf.math.erf([[1.0, 2.0, 3.0], [0.0, -1.0, -2.0]])
<tf.Tensor: shape=(2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 3), dtype=float32, numpy=
array([[ 0.8427007,  0.9953223,  0.999978 ],
       [ 0.       , -0.8427007, -0.9953223]], dtype=float32)>"
"tf.math.erfcinv(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import erfcinv
tf.math.erfcinv([0., 0.2, 1., 1.5, 2.])
<tf.Tensor: shape=(5,), dtype=float32, numpy=
<tf.Tensor: shape=(5,), dtype=float32, numpy=
array([       inf,  0.9061935, -0.       , -0.4769363,       -inf],
      dtype=float32)>"
"tf.math.exp(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import exp
x = tf.constant(2.0)
tf.math.exp(x)
<tf.Tensor: shape=(), dtype=float32, numpy=7.389056>
<tf.Tensor: shape=(), dtype=float32, numpy=7.389056>"
"tf.math.expm1(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import expm1
  x = tf.constant(2.0)
  tf.math.expm1(x) ==> 6.389056

  x = tf.constant([2.0, 8.0])
  tf.math.expm1(x) ==> array([6.389056, 2979.958], dtype=float32)

  x = tf.constant(1 + 1j)
  tf.math.expm1(x) ==> (0.46869393991588515+2.2873552871788423j)
"
"tf.math.floor(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import floor
x = tf.constant([1.3324, -1.5, 5.555, -2.532, 0.99, float(""inf"")])
tf.floor(x).numpy()
array([ 1., -2.,  5., -3.,  0., inf], dtype=float32)"
"tf.math.greater(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import greater
x = tf.constant([5, 4, 6])
y = tf.constant([5, 2, 5])
tf.math.greater(x, y) ==> [False, True, True]

x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.greater(x, y) ==> [False, False, True]
"
"tf.math.greater_equal(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import greater_equal
x = tf.constant([5, 4, 6, 7])
y = tf.constant([5, 2, 5, 10])
tf.math.greater_equal(x, y) ==> [True, True, True, False]

x = tf.constant([5, 4, 6, 7])
y = tf.constant([5])
tf.math.greater_equal(x, y) ==> [True, False, True, True]
"
"tf.math.imag(
    input, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import imag
x = tf.constant([-2.25 + 4.75j, 3.25 + 5.75j])
"
"tf.math.invert_permutation(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import invert_permutation
invert_permutation(x) ==> [2, 4, 3, 0, 1]
"
"tf.math.is_finite(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import is_finite
x = tf.constant([5.0, 4.8, 6.8, np.inf, np.nan])
tf.math.is_finite(x) ==> [True, True, True, False, False]
"
"tf.math.is_inf(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import is_inf
x = tf.constant([5.0, np.inf, 6.8, np.inf])
tf.math.is_inf(x) ==> [False, True, False, True]
"
"tf.math.is_nan(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import is_nan
x = tf.constant([5.0, np.nan, 6.8, np.nan, np.inf])
tf.math.is_nan(x) ==> [False, True, False, True, False]
"
"tf.math.is_non_decreasing(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import is_non_decreasing
x1 = tf.constant([1.0, 1.0, 3.0])
tf.math.is_non_decreasing(x1)
<tf.Tensor: shape=(), dtype=bool, numpy=True>
<tf.Tensor: shape=(), dtype=bool, numpy=True>
x2 = tf.constant([3.0, 1.0, 2.0])
tf.math.is_non_decreasing(x2)
<tf.Tensor: shape=(), dtype=bool, numpy=False>
<tf.Tensor: shape=(), dtype=bool, numpy=False>"
"tf.math.is_strictly_increasing(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import is_strictly_increasing
x1 = tf.constant([1.0, 2.0, 3.0])
tf.math.is_strictly_increasing(x1)
<tf.Tensor: shape=(), dtype=bool, numpy=True>
<tf.Tensor: shape=(), dtype=bool, numpy=True>
x2 = tf.constant([3.0, 1.0, 2.0])
tf.math.is_strictly_increasing(x2)
<tf.Tensor: shape=(), dtype=bool, numpy=False>
<tf.Tensor: shape=(), dtype=bool, numpy=False>"
"tf.math.less(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import less
x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.less(x, y) ==> [False, True, False]

x = tf.constant([5, 4, 6])
y = tf.constant([5, 6, 7])
tf.math.less(x, y) ==> [False, True, True]
"
"tf.math.less_equal(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import less_equal
x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.less_equal(x, y) ==> [True, True, False]

x = tf.constant([5, 4, 6])
y = tf.constant([5, 6, 6])
tf.math.less_equal(x, y) ==> [True, True, True]
"
"tf.math.lgamma(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import lgamma
x = tf.constant([0, 0.5, 1, 4.5, -4, -5.6])
tf.math.lgamma(x) ==> [inf, 0.5723649, 0., 2.4537368, inf, -4.6477685]
"
"tf.math.log(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import log
x = tf.constant([0, 0.5, 1, 5])
tf.math.log(x)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([      -inf, -0.6931472,  0.       ,  1.609438 ], dtype=float32)>
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([      -inf, -0.6931472,  0.       ,  1.609438 ], dtype=float32)>"
"tf.math.log1p(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import log1p
x = tf.constant([0, 0.5, 1, 5])
tf.math.log1p(x)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.       , 0.4054651, 0.6931472, 1.7917595], dtype=float32)>
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.       , 0.4054651, 0.6931472, 1.7917595], dtype=float32)>"
"tf.math.log_sigmoid(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import log_sigmoid
x = tf.constant([0.0, 1.0, 50.0, 100.0])
tf.math.log_sigmoid(x)
<tf.Tensor: shape=(4,), dtype=float32, numpy=
<tf.Tensor: shape=(4,), dtype=float32, numpy=
array([-6.9314718e-01, -3.1326169e-01, -1.9287499e-22, -0.0000000e+00],
      dtype=float32)>"
"tf.math.logical_and(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import logical_and
>>> a = tf.constant([True])
>>> b = tf.constant([False])
>>> tf.math.logical_and(a, b)
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>
>>> a & b
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>
"
"tf.math.logical_not(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import logical_not
tf.math.logical_not(tf.constant([True, False]))
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>"
"tf.math.logical_or(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import logical_or
>>> a = tf.constant([True])
>>> b = tf.constant([False])
>>> tf.math.logical_or(a, b)
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>
>>> a | b
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>
"
"tf.math.logical_xor(
    x, y, name='LogicalXor'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import logical_xor
a = tf.constant([True])
b = tf.constant([False])
tf.math.logical_xor(a, b)
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>"
"tf.math.maximum(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import maximum
x = tf.constant([0., 0., 0., 0.])
y = tf.constant([-2., 0., 2., 5.])
tf.math.maximum(x, y)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 2., 5.], dtype=float32)>
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 2., 5.], dtype=float32)>"
"tf.math.minimum(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import minimum
x = tf.constant([0., 0., 0., 0.])
y = tf.constant([-5., -2., 0., 3.])
tf.math.minimum(x, y)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([-5., -2., 0., 0.], dtype=float32)>
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([-5., -2., 0., 0.], dtype=float32)>"
"tf.math.multiply(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import multiply
x = tf.constant(([1, 2, 3, 4]))
tf.math.multiply(x, x)
<tf.Tensor: shape=(4,), dtype=..., numpy=array([ 1,  4,  9, 16], dtype=int32)>
<tf.Tensor: shape=(4,), dtype=..., numpy=array([ 1,  4,  9, 16], dtype=int32)>"
"tf.math.not_equal(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import not_equal
x = tf.constant([2, 4])
y = tf.constant(2)
tf.math.not_equal(x, y)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>"
"tf.math.pow(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import pow
x = tf.constant([[2, 2], [3, 3]])
y = tf.constant([[8, 16], [2, 3]])
"
"tf.math.real(
    input, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import real
x = tf.constant([-2.25 + 4.75j, 3.25 + 5.75j])
"
"tf.math.reciprocal_no_nan(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reciprocal_no_nan
x = tf.constant([2.0, 0.5, 0, 1], dtype=tf.float32)
"
"tf.math.reduce_all(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_all
>>> x = tf.constant([[True,  True], [False, False]])
>>> tf.math.reduce_all(x)
<tf.Tensor: shape=(), dtype=bool, numpy=False>
<tf.Tensor: shape=(), dtype=bool, numpy=False>
>>> tf.math.reduce_all(x, 0)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False, False])>
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False, False])>
>>> tf.math.reduce_all(x, 1)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>
"
"tf.math.reduce_any(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_any
>>> x = tf.constant([[True,  True], [False, False]])
>>> tf.reduce_any(x)
<tf.Tensor: shape=(), dtype=bool, numpy=True>
<tf.Tensor: shape=(), dtype=bool, numpy=True>
>>> tf.reduce_any(x, 0)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  True])>
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  True])>
>>> tf.reduce_any(x, 1)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>
"
"tf.math.reduce_euclidean_norm(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_euclidean_norm
y = tf.constant([[1, 2, 3], [1, 1, 1]], dtype = tf.float32)
"
"tf.math.reduce_logsumexp(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_logsumexp
x = tf.constant([[0., 0., 0.], [0., 0., 0.]])
"
"tf.math.reduce_max(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_max
>>> x = tf.constant([5, 1, 2, 4])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=int32, numpy=5>
<tf.Tensor: shape=(), dtype=int32, numpy=5>
>>> x = tf.constant([-5, -1, -2, -4])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=int32, numpy=-1>
<tf.Tensor: shape=(), dtype=int32, numpy=-1>
>>> x = tf.constant([4, float('nan')])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=float32, numpy=nan>
<tf.Tensor: shape=(), dtype=float32, numpy=nan>
>>> x = tf.constant([float('nan'), float('nan')])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=float32, numpy=nan>
<tf.Tensor: shape=(), dtype=float32, numpy=nan>
>>> x = tf.constant([float('-inf'), float('inf')])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=float32, numpy=inf>
<tf.Tensor: shape=(), dtype=float32, numpy=inf>
"
"tf.math.reduce_mean(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_mean
x = tf.constant([[1., 1.], [2., 2.]])
tf.reduce_mean(x)
<tf.Tensor: shape=(), dtype=float32, numpy=1.5>
<tf.Tensor: shape=(), dtype=float32, numpy=1.5>
tf.reduce_mean(x, 0)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.5, 1.5], dtype=float32)>
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.5, 1.5], dtype=float32)>
tf.reduce_mean(x, 1)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 2.], dtype=float32)>
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 2.], dtype=float32)>"
"tf.math.reduce_min(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_min
a = tf.constant([
  [[1, 2], [3, 4]],
  [[1, 2], [3, 4]]
])
tf.reduce_min(a)
<tf.Tensor: shape=(), dtype=int32, numpy=1>
<tf.Tensor: shape=(), dtype=int32, numpy=1>"
"tf.math.reduce_prod(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_prod
>>> x = tf.constant([[1., 2.], [3., 4.]])
>>> tf.math.reduce_prod(x)
<tf.Tensor: shape=(), dtype=float32, numpy=24.>
<tf.Tensor: shape=(), dtype=float32, numpy=24.>
>>> tf.math.reduce_prod(x, 0)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([3., 8.], dtype=float32)>
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([3., 8.], dtype=float32)>
>>> tf.math.reduce_prod(x, 1)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 12.],
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 12.],
dtype=float32)>
"
"tf.math.reduce_std(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_std
x = tf.constant([[1., 2.], [3., 4.]])
tf.math.reduce_std(x)
<tf.Tensor: shape=(), dtype=float32, numpy=1.118034>
<tf.Tensor: shape=(), dtype=float32, numpy=1.118034>
tf.math.reduce_std(x, 0)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)>
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)>
tf.math.reduce_std(x, 1)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.5, 0.5], dtype=float32)>
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.5, 0.5], dtype=float32)>"
"tf.math.reduce_sum(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_sum
>>> x = tf.constant([[1, 1, 1], [1, 1, 1]])
>>> x.numpy()
array([[1, 1, 1],
       [1, 1, 1]], dtype=int32)
>>> tf.reduce_sum(x).numpy()
6
>>> tf.reduce_sum(x, 0).numpy()
array([2, 2, 2], dtype=int32)
>>> tf.reduce_sum(x, 1).numpy()
array([3, 3], dtype=int32)
>>> tf.reduce_sum(x, 1, keepdims=True).numpy()
array([[3],
       [3]], dtype=int32)
>>> tf.reduce_sum(x, [0, 1]).numpy()
6
"
"tf.math.reduce_variance(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_variance
x = tf.constant([[1., 2.], [3., 4.]])
tf.math.reduce_variance(x)
<tf.Tensor: shape=(), dtype=float32, numpy=1.25>
<tf.Tensor: shape=(), dtype=float32, numpy=1.25>
tf.math.reduce_variance(x, 0)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], ...)>
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], ...)>
tf.math.reduce_variance(x, 1)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.25, 0.25], ...)>
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.25, 0.25], ...)>"
"tf.math.rint(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import rint
rint(-1.5) ==> -2.0
rint(0.5000001) ==> 1.0
rint([-1.7, -1.5, -0.2, 0.2, 1.5, 1.7, 2.0]) ==> [-2., -2., -0., 0., 2., 2., 2.]
"
"tf.math.round(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import round
x = tf.constant([0.9, 2.5, 2.3, 1.5, -4.5])
"
"tf.math.rsqrt(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import rsqrt
x = tf.constant([2., 0., -2.])
tf.math.rsqrt(x)
<tf.Tensor: shape=(3,), dtype=float32,
<tf.Tensor: shape=(3,), dtype=float32,
numpy=array([0.707, inf, nan], dtype=float32)>"
"tf.math.scalar_mul(
    scalar, x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import scalar_mul
x = tf.reshape(tf.range(30, dtype=tf.float32), [10, 3])
with tf.GradientTape() as g:
  g.watch(x)
  z = tf.math.scalar_mul(10.0, y)"
"tf.math.segment_max(
    data, segment_ids, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import segment_max
c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
tf.math.segment_max(c, tf.constant([0, 0, 1])).numpy()
array([[4, 3, 3, 4],
       [5, 6, 7, 8]], dtype=int32)"
"tf.math.segment_mean(
    data, segment_ids, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import segment_mean
c = tf.constant([[1.0,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
tf.math.segment_mean(c, tf.constant([0, 0, 1])).numpy()
array([[2.5, 2.5, 2.5, 2.5],
       [5., 6., 7., 8.]], dtype=float32)"
"tf.math.segment_min(
    data, segment_ids, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import segment_min
c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
tf.math.segment_min(c, tf.constant([0, 0, 1])).numpy()
array([[1, 2, 2, 1],
       [5, 6, 7, 8]], dtype=int32)"
"tf.math.segment_prod(
    data, segment_ids, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import segment_prod
c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
tf.math.segment_prod(c, tf.constant([0, 0, 1])).numpy()
array([[4, 6, 6, 4],
       [5, 6, 7, 8]], dtype=int32)"
"tf.math.segment_sum(
    data, segment_ids, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import segment_sum
c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
tf.math.segment_sum(c, tf.constant([0, 0, 1])).numpy()
array([[5, 5, 5, 5],
       [5, 6, 7, 8]], dtype=int32)"
"tf.math.sigmoid(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sigmoid
x = tf.constant([0.0, 1.0, 50.0, 100.0])
tf.math.sigmoid(x)
<tf.Tensor: shape=(4,), dtype=float32,
<tf.Tensor: shape=(4,), dtype=float32,
numpy=array([0.5, 0.7310586, 1.0, 1.0], dtype=float32)>"
"tf.math.sign(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sign
tf.math.sign([0., 2., -3.])
<tf.Tensor: shape=(3,), dtype=float32,
<tf.Tensor: shape=(3,), dtype=float32,
numpy=array([ 0.,  1., -1.], dtype=float32)>"
"tf.math.sin(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sin
  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 200, 10, float(""inf"")])
  tf.math.sin(x) ==> [nan -0.4121185 -0.47942555 0.84147096 0.9320391 -0.87329733 -0.54402107 nan]
"
"tf.math.sinh(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sinh
  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 2, 10, float(""inf"")])
  tf.math.sinh(x) ==> [-inf -4.0515420e+03 -5.2109528e-01 1.1752012e+00 1.5094614e+00 3.6268604e+00 1.1013232e+04 inf]
"
"tf.nn.softmax(
    logits, axis=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis, keepdims=True)
"
"tf.math.softplus(
    features, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import softplus
import tensorflow as tf
tf.math.softplus(tf.range(0, 2, dtype=tf.float32)).numpy()
array([0.6931472, 1.3132616], dtype=float32)"
"tf.math.bessel_i0(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bessel_i0
tf.math.special.bessel_i0([-1., -0.5, 0.5, 1.]).numpy()
array([1.26606588, 1.06348337, 1.06348337, 1.26606588], dtype=float32)"
"tf.math.bessel_i0e(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bessel_i0e
tf.math.special.bessel_i0e([-1., -0.5, 0.5, 1.]).numpy()
array([0.46575961, 0.64503527, 0.64503527, 0.46575961], dtype=float32)"
"tf.math.bessel_i1(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bessel_i1
tf.math.special.bessel_i1([-1., -0.5, 0.5, 1.]).numpy()
array([-0.5651591 , -0.25789431,  0.25789431,  0.5651591 ], dtype=float32)"
"tf.math.bessel_i1e(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bessel_i1e
tf.math.special.bessel_i1e([-1., -0.5, 0.5, 1.]).numpy()
array([-0.20791042, -0.15642083,  0.15642083,  0.20791042], dtype=float32)"
"tf.math.special.bessel_j0(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bessel_j0
tf.math.special.bessel_j0([0.5, 1., 2., 4.]).numpy()
array([ 0.93846981,  0.76519769,  0.22389078, -0.39714981], dtype=float32)"
"tf.math.special.bessel_j1(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bessel_j1
tf.math.special.bessel_j1([0.5, 1., 2., 4.]).numpy()
array([ 0.24226846,  0.44005059,  0.57672481, -0.06604333], dtype=float32)"
"tf.math.special.bessel_k0(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bessel_k0
tf.math.special.bessel_k0([0.5, 1., 2., 4.]).numpy()
array([0.92441907, 0.42102444, 0.11389387, 0.01115968], dtype=float32)"
"tf.math.special.bessel_k0e(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bessel_k0e
tf.math.special.bessel_k0e([0.5, 1., 2., 4.]).numpy()
array([1.52410939, 1.14446308, 0.84156822, 0.60929767], dtype=float32)"
"tf.math.special.bessel_k1(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bessel_k1
tf.math.special.bessel_k1([0.5, 1., 2., 4.]).numpy()
array([1.65644112, 0.60190723, 0.13986588, 0.0124835 ], dtype=float32)"
"tf.math.special.bessel_k1e(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bessel_k1e
tf.math.special.bessel_k1e([0.5, 1., 2., 4.]).numpy()
array([2.73100971, 1.63615349, 1.03347685, 0.68157595], dtype=float32)"
"tf.math.special.bessel_y0(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bessel_y0
tf.math.special.bessel_y0([0.5, 1., 2., 4.]).numpy()
array([-0.44451873,  0.08825696,  0.51037567, -0.01694074], dtype=float32)"
"tf.math.special.bessel_y1(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bessel_y1
tf.math.special.bessel_y1([0.5, 1., 2., 4.]).numpy()
array([-1.47147239, -0.78121282, -0.10703243,  0.39792571], dtype=float32)"
"tf.math.special.dawsn(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import dawsn
>>> tf.math.special.dawsn([-1., -0.5, 0.5, 1.]).numpy()
array([-0.5380795, -0.4244364, 0.4244364,  0.5380795], dtype=float32)
"
"tf.math.special.expint(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import expint
tf.math.special.expint([1., 1.1, 2.1, 4.1]).numpy()
array([ 1.8951179,  2.1673784,  5.3332353, 21.048464], dtype=float32)"
"tf.math.special.fresnel_cos(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import fresnel_cos
>>> tf.math.special.fresnel_cos([-1., -0.1, 0.1, 1.]).numpy()
array([-0.7798934 , -0.09999753,  0.09999753,  0.7798934 ], dtype=float32)
"
"tf.math.special.fresnel_sin(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import fresnel_sin
tf.math.special.fresnel_sin([-1., -0.1, 0.1, 1.]).numpy()
array([-0.43825912, -0.00052359,  0.00052359,  0.43825912], dtype=float32)"
"tf.math.special.spence(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import spence
tf.math.special.spence([0.5, 1., 2., 3.]).numpy()
array([ 0.58224034,  0.        , -0.82246685, -1.4367464], dtype=float32)"
"tf.math.sqrt(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sqrt
x = tf.constant([[4.0], [16.0]])
tf.sqrt(x)
<tf.Tensor: shape=(2, 1), dtype=float32, numpy=
<tf.Tensor: shape=(2, 1), dtype=float32, numpy=
  array([[2.],
         [4.]], dtype=float32)>
y = tf.constant([[-4.0], [16.0]])
tf.sqrt(y)
<tf.Tensor: shape=(2, 1), dtype=float32, numpy=
<tf.Tensor: shape=(2, 1), dtype=float32, numpy=
  array([[nan],
         [ 4.]], dtype=float32)>
z = tf.constant([[-1.0], [16.0]], dtype=tf.complex128)
tf.sqrt(z)
<tf.Tensor: shape=(2, 1), dtype=complex128, numpy=
<tf.Tensor: shape=(2, 1), dtype=complex128, numpy=
  array([[0.0+1.j],
         [4.0+0.j]])>"
"tf.math.square(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import square
tf.math.square([-2., 0., 3.])
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([4., 0., 9.], dtype=float32)>
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([4., 0., 9.], dtype=float32)>"
"tf.math.subtract(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import subtract
x = [1, 2, 3, 4, 5]
y = 1
tf.subtract(x, y)
<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 1, 2, 3, 4], dtype=int32)>
<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 1, 2, 3, 4], dtype=int32)>
tf.subtract(y, x)
<tf.Tensor: shape=(5,), dtype=int32,
<tf.Tensor: shape=(5,), dtype=int32,
numpy=array([ 0, -1, -2, -3, -4], dtype=int32)>"
"tf.math.tan(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import tan
  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
  tf.math.tan(x) ==> [nan 0.45231566 -0.5463025 1.5574077 2.572152 -1.7925274 0.32097113 nan]
"
"tf.math.top_k(
    input, k=1, sorted=True, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import top_k
result = tf.math.top_k([1, 2, 98, 1, 1, 99, 3, 1, 3, 96, 4, 1],
                        k=3)
result.values.numpy()
array([99, 98, 96], dtype=int32)
result.indices.numpy()
array([5, 2, 9], dtype=int32)"
"tf.math.unsorted_segment_max(
    data, segment_ids, num_segments, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import unsorted_segment_max
c = tf.constant([[1,2,3,4], [5,6,7,8], [4,3,2,1]])
tf.math.unsorted_segment_max(c, tf.constant([0, 1, 0]), num_segments=2).numpy()
array([[4, 3, 3, 4],
       [5,  6, 7, 8]], dtype=int32)"
"tf.math.unsorted_segment_min(
    data, segment_ids, num_segments, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import unsorted_segment_min
c = tf.constant([[1,2,3,4], [5,6,7,8], [4,3,2,1]])
tf.math.unsorted_segment_min(c, tf.constant([0, 1, 0]), num_segments=2).numpy()
array([[1, 2, 2, 1],
       [5, 6, 7, 8]], dtype=int32)"
"tf.math.unsorted_segment_prod(
    data, segment_ids, num_segments, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import unsorted_segment_prod
c = tf.constant([[1,2,3,4], [5,6,7,8], [4,3,2,1]])
tf.math.unsorted_segment_prod(c, tf.constant([0, 1, 0]), num_segments=2).numpy()
array([[4, 6, 6, 4],
       [5, 6, 7, 8]], dtype=int32)"
"tf.math.unsorted_segment_sum(
    data, segment_ids, num_segments, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import unsorted_segment_sum
c = [[1,2,3,4], [5,6,7,8], [4,3,2,1]]
tf.math.unsorted_segment_sum(c, [0, 1, 0], num_segments=2).numpy()
array([[5, 5, 5, 5],
       [5, 6, 7, 8]], dtype=int32)"
"tf.math.xlog1py(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import xlog1py
tf.math.xlog1py(0., 1.)
<tf.Tensor: shape=(), dtype=float32, numpy=0.>
<tf.Tensor: shape=(), dtype=float32, numpy=0.>
tf.math.xlog1py(1., 1.)
<tf.Tensor: shape=(), dtype=float32, numpy=0.6931472>
<tf.Tensor: shape=(), dtype=float32, numpy=0.6931472>
tf.math.xlog1py(2., 2.)
<tf.Tensor: shape=(), dtype=float32, numpy=2.1972246>
<tf.Tensor: shape=(), dtype=float32, numpy=2.1972246>
tf.math.xlog1py(0., -1.)
<tf.Tensor: shape=(), dtype=float32, numpy=0.>
<tf.Tensor: shape=(), dtype=float32, numpy=0.>"
"tf.math.zero_fraction(
    value, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import zero_fraction
    z = tf.nn.relu(...)
    summ = tf.compat.v1.summary.scalar('sparsity', tf.nn.zero_fraction(z))
"
"tf.linalg.matmul(
    a,
    b,
    transpose_a=False,
    transpose_b=False,
    adjoint_a=False,
    adjoint_b=False,
    a_is_sparse=False,
    b_is_sparse=False,
    output_type=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import matmul
a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
array([[1, 2, 3],
       [4, 5, 6]], dtype=int32)>
b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
array([[ 7,  8],
       [ 9, 10],
       [11, 12]], dtype=int32)>
c = tf.matmul(a, b)
<tf.Tensor: shape=(2, 2), dtype=int32, numpy=
<tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[ 58,  64],
       [139, 154]], dtype=int32)>"
"tf.math.maximum(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import maximum
x = tf.constant([0., 0., 0., 0.])
y = tf.constant([-2., 0., 2., 5.])
tf.math.maximum(x, y)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 2., 5.], dtype=float32)>
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 2., 5.], dtype=float32)>"
"tf.meshgrid(
    *args, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import meshgrid
x = [1, 2, 3]
y = [4, 5, 6]
X, Y = tf.meshgrid(x, y)
"
"tf.keras.metrics.AUC(
    num_thresholds=200,
    curve='ROC',
    summation_method='interpolation',
    name=None,
    dtype=None,
    thresholds=None,
    multi_label=False,
    num_labels=None,
    label_weights=None,
    from_logits=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import AUC
m = tf.keras.metrics.AUC(num_thresholds=3)
m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])
m.result().numpy()
0.75"
"tf.keras.metrics.Accuracy(
    name='accuracy', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Accuracy
m = tf.keras.metrics.Accuracy()
m.update_state([[1], [2], [3], [4]], [[0], [2], [3], [4]])
m.result().numpy()
0.75"
"tf.keras.metrics.BinaryAccuracy(
    name='binary_accuracy', dtype=None, threshold=0.5
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import BinaryAccuracy
m = tf.keras.metrics.BinaryAccuracy()
m.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]])
m.result().numpy()
0.75"
"tf.keras.metrics.BinaryCrossentropy(
    name='binary_crossentropy',
    dtype=None,
    from_logits=False,
    label_smoothing=0
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import BinaryCrossentropy
m = tf.keras.metrics.BinaryCrossentropy()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
0.81492424"
"tf.keras.metrics.CategoricalAccuracy(
    name='categorical_accuracy', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CategoricalAccuracy
m = tf.keras.metrics.CategoricalAccuracy()
m.update_state([[0, 0, 1], [0, 1, 0]], [[0.1, 0.9, 0.8],
                [0.05, 0.95, 0]])
m.result().numpy()
0.5"
"tf.keras.metrics.CategoricalCrossentropy(
    name='categorical_crossentropy',
    dtype=None,
    from_logits=False,
    label_smoothing=0,
    axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CategoricalCrossentropy
m = tf.keras.metrics.CategoricalCrossentropy()
m.update_state([[0, 1, 0], [0, 0, 1]],
               [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])
m.result().numpy()
1.1769392"
"tf.keras.metrics.CategoricalHinge(
    name='categorical_hinge', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CategoricalHinge
m = tf.keras.metrics.CategoricalHinge()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
1.4000001"
"tf.keras.metrics.CosineSimilarity(
    name='cosine_similarity', dtype=None, axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CosineSimilarity
m = tf.keras.metrics.CosineSimilarity(axis=1)
m.update_state([[0., 1.], [1., 1.]], [[1., 0.], [1., 1.]])
m.result().numpy()
0.49999997"
"tf.keras.metrics.FalseNegatives(
    thresholds=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import FalseNegatives
m = tf.keras.metrics.FalseNegatives()
m.update_state([0, 1, 1, 1], [0, 1, 0, 0])
m.result().numpy()
2.0"
"tf.keras.metrics.FalsePositives(
    thresholds=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import FalsePositives
m = tf.keras.metrics.FalsePositives()
m.update_state([0, 1, 0, 0], [0, 0, 1, 1])
m.result().numpy()
2.0"
"tf.keras.metrics.Hinge(
    name='hinge', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Hinge
m = tf.keras.metrics.Hinge()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
1.3"
"tf.keras.metrics.KLDivergence(
    name='kullback_leibler_divergence', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import KLDivergence
m = tf.keras.metrics.KLDivergence()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
0.45814306"
"tf.keras.metrics.LogCoshError(
    name='logcosh', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import LogCoshError
m = tf.keras.metrics.LogCoshError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.10844523"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_percentage_error
y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_logarithmic_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.Mean(
    name='mean', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Mean
m = tf.keras.metrics.Mean()
m.update_state([1, 3, 5, 7])
m.result().numpy()
4.0
m.reset_state()
m.update_state([1, 3, 5, 7], sample_weight=[1, 1, 0, 0])
m.result().numpy()
2.0"
"tf.keras.metrics.MeanAbsoluteError(
    name='mean_absolute_error', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanAbsoluteError
m = tf.keras.metrics.MeanAbsoluteError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.25"
"tf.keras.metrics.MeanAbsolutePercentageError(
    name='mean_absolute_percentage_error', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanAbsolutePercentageError
m = tf.keras.metrics.MeanAbsolutePercentageError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
250000000.0"
"tf.keras.metrics.MeanMetricWrapper(
    fn, name=None, dtype=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanMetricWrapper
def accuracy(y_true, y_pred):
  return tf.cast(tf.math.equal(y_true, y_pred), tf.float32)

accuracy_metric = tf.keras.metrics.MeanMetricWrapper(fn=accuracy)

keras_model.compile(..., metrics=accuracy_metric)
"
"tf.keras.metrics.MeanRelativeError(
    normalizer, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanRelativeError
m = tf.keras.metrics.MeanRelativeError(normalizer=[1, 3, 2, 3])
m.update_state([1, 3, 2, 3], [2, 4, 6, 8])"
"tf.keras.metrics.MeanSquaredError(
    name='mean_squared_error', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanSquaredError
m = tf.keras.metrics.MeanSquaredError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.25"
"tf.keras.metrics.MeanSquaredLogarithmicError(
    name='mean_squared_logarithmic_error', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanSquaredLogarithmicError
m = tf.keras.metrics.MeanSquaredLogarithmicError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.12011322"
"tf.keras.metrics.MeanTensor(
    name='mean_tensor', dtype=None, shape=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import MeanTensor
m = tf.keras.metrics.MeanTensor()
m.update_state([0, 1, 2, 3])
m.update_state([4, 5, 6, 7])
m.result().numpy()
array([2., 3., 4., 5.], dtype=float32)"
"tf.keras.metrics.Metric(
    name=None, dtype=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Metric
m = SomeMetric(...)
for input in ...:
  m.update_state(input)
print('Final result: ', m.result().numpy())
"
"tf.keras.metrics.Poisson(
    name='poisson', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Poisson
m = tf.keras.metrics.Poisson()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.49999997"
"tf.keras.metrics.Precision(
    thresholds=None, top_k=None, class_id=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Precision
m = tf.keras.metrics.Precision()
m.update_state([0, 1, 1, 1], [1, 0, 1, 1])
m.result().numpy()
0.6666667"
"tf.keras.metrics.PrecisionAtRecall(
    recall, num_thresholds=200, class_id=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import PrecisionAtRecall
m = tf.keras.metrics.PrecisionAtRecall(0.5)
m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])
m.result().numpy()
0.5"
"tf.keras.metrics.Recall(
    thresholds=None, top_k=None, class_id=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Recall
m = tf.keras.metrics.Recall()
m.update_state([0, 1, 1, 1], [1, 0, 1, 1])
m.result().numpy()
0.6666667"
"tf.keras.metrics.RecallAtPrecision(
    precision, num_thresholds=200, class_id=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RecallAtPrecision
m = tf.keras.metrics.RecallAtPrecision(0.8)
m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])
m.result().numpy()
0.5"
"tf.keras.metrics.RootMeanSquaredError(
    name='root_mean_squared_error', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RootMeanSquaredError
m = tf.keras.metrics.RootMeanSquaredError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.5"
"tf.keras.metrics.SensitivityAtSpecificity(
    specificity, num_thresholds=200, class_id=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SensitivityAtSpecificity
m = tf.keras.metrics.SensitivityAtSpecificity(0.5)
m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])
m.result().numpy()
0.5"
"tf.keras.metrics.SparseCategoricalCrossentropy(
    name: str = 'sparse_categorical_crossentropy',
    dtype: Optional[Union[str, tf.dtypes.DType]] = None,
    from_logits: bool = False,
    ignore_class: Optional[int] = None,
    axis: int = -1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SparseCategoricalCrossentropy
m = tf.keras.metrics.SparseCategoricalCrossentropy()
m.update_state([1, 2],
               [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])
m.result().numpy()
1.1769392"
"tf.keras.metrics.SparseTopKCategoricalAccuracy(
    k=5, name='sparse_top_k_categorical_accuracy', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SparseTopKCategoricalAccuracy
m = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)
m.update_state([2, 1], [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])
m.result().numpy()
0.5"
"tf.keras.metrics.SpecificityAtSensitivity(
    sensitivity, num_thresholds=200, class_id=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SpecificityAtSensitivity
m = tf.keras.metrics.SpecificityAtSensitivity(0.5)
m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])
m.result().numpy()
0.66666667"
"tf.keras.metrics.SquaredHinge(
    name='squared_hinge', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SquaredHinge
m = tf.keras.metrics.SquaredHinge()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
1.86"
"tf.keras.metrics.Sum(
    name='sum', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Sum
m = tf.keras.metrics.Sum()
m.update_state([1, 3, 5, 7])
m.result().numpy()
16.0"
"tf.keras.metrics.TopKCategoricalAccuracy(
    k=5, name='top_k_categorical_accuracy', dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TopKCategoricalAccuracy
m = tf.keras.metrics.TopKCategoricalAccuracy(k=1)
m.update_state([[0, 0, 1], [0, 1, 0]],
               [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])
m.result().numpy()
0.5"
"tf.keras.metrics.TrueNegatives(
    thresholds=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TrueNegatives
m = tf.keras.metrics.TrueNegatives()
m.update_state([0, 1, 0, 0], [1, 1, 0, 0])
m.result().numpy()
2.0"
"tf.keras.metrics.TruePositives(
    thresholds=None, name=None, dtype=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TruePositives
m = tf.keras.metrics.TruePositives()
m.update_state([0, 1, 1, 1], [1, 0, 1, 1])
m.result().numpy()
2.0"
"tf.keras.metrics.binary_accuracy(
    y_true, y_pred, threshold=0.5
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import binary_accuracy
y_true = [[1], [1], [0], [0]]
y_pred = [[1], [1], [0], [0]]
m = tf.keras.metrics.binary_accuracy(y_true, y_pred)
assert m.shape == (4,)
m.numpy()
array([1., 1., 1., 1.], dtype=float32)"
"tf.keras.metrics.binary_crossentropy(
    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import binary_crossentropy
y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.916 , 0.714], dtype=float32)"
"tf.keras.metrics.binary_focal_crossentropy(
    y_true,
    y_pred,
    apply_class_balancing=False,
    alpha=0.25,
    gamma=2.0,
    from_logits=False,
    label_smoothing=0.0,
    axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import binary_focal_crossentropy
y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
loss = tf.keras.losses.binary_focal_crossentropy(y_true, y_pred,
                                                 gamma=2)
assert loss.shape == (2,)
loss.numpy()
array([0.330, 0.206], dtype=float32)"
"tf.keras.metrics.categorical_accuracy(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import categorical_accuracy
y_true = [[0, 0, 1], [0, 1, 0]]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.categorical_accuracy(y_true, y_pred)
assert m.shape == (2,)
m.numpy()
array([0., 1.], dtype=float32)"
"tf.keras.metrics.categorical_crossentropy(
    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import categorical_crossentropy
y_true = [[0, 1, 0], [0, 0, 1]]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.0513, 2.303], dtype=float32)"
"tf.keras.metrics.get(
    identifier
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import get
metric = tf.keras.metrics.get(""categorical_crossentropy"")
type(metric)
<class 'function'>
<class 'function'>
metric = tf.keras.metrics.get(""CategoricalCrossentropy"")
type(metric)
<class '...metrics.CategoricalCrossentropy'>
<class '...metrics.CategoricalCrossentropy'>"
"tf.keras.metrics.hinge(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import hinge
y_true = np.random.choice([-1, 1], size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.hinge(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    np.mean(np.maximum(1. - y_true * y_pred, 0.), axis=-1))"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_percentage_error
y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_absolute_percentage_error
y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_logarithmic_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mean_squared_logarithmic_error
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.poisson(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import poisson
y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.poisson(y_true, y_pred)
assert loss.shape == (2,)
y_pred = y_pred + 1e-7
assert np.allclose(
    loss.numpy(), np.mean(y_pred - y_true * np.log(y_pred), axis=-1),
    atol=1e-5)"
"tf.keras.metrics.sparse_categorical_accuracy(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sparse_categorical_accuracy
y_true = [2, 1]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)
assert m.shape == (2,)
m.numpy()
array([0., 1.], dtype=float32)"
"tf.keras.metrics.sparse_categorical_crossentropy(
    y_true, y_pred, from_logits=False, axis=-1, ignore_class=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sparse_categorical_crossentropy
y_true = [1, 2]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.0513, 2.303], dtype=float32)"
"tf.keras.metrics.sparse_top_k_categorical_accuracy(
    y_true, y_pred, k=5
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sparse_top_k_categorical_accuracy
y_true = [2, 1]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.sparse_top_k_categorical_accuracy(
    y_true, y_pred, k=3)
assert m.shape == (2,)
m.numpy()
array([1., 1.], dtype=float32)"
"tf.keras.metrics.squared_hinge(
    y_true, y_pred
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import squared_hinge
y_true = np.random.choice([-1, 1], size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.squared_hinge(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    np.mean(np.square(np.maximum(1. - y_true * y_pred, 0.)), axis=-1))"
"tf.keras.metrics.top_k_categorical_accuracy(
    y_true, y_pred, k=5
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import top_k_categorical_accuracy
y_true = [[0, 0, 1], [0, 1, 0]]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.top_k_categorical_accuracy(y_true, y_pred, k=3)
assert m.shape == (2,)
m.numpy()
array([1., 1.], dtype=float32)"
"tf.math.minimum(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import minimum
x = tf.constant([0., 0., 0., 0.])
y = tf.constant([-5., -2., 0., 3.])
tf.math.minimum(x, y)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([-5., -2., 0., 0.], dtype=float32)>
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([-5., -2., 0., 0.], dtype=float32)>"
"tf.math.multiply(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import multiply
x = tf.constant(([1, 2, 3, 4]))
tf.math.multiply(x, x)
<tf.Tensor: shape=(4,), dtype=..., numpy=array([ 1,  4,  9, 16], dtype=int32)>
<tf.Tensor: shape=(4,), dtype=..., numpy=array([ 1,  4,  9, 16], dtype=int32)>"
"tf.name_scope(
    name
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import name_scope
def my_op(a, b, c, name=None):
  with tf.name_scope(""MyOp"") as scope:
    a = tf.convert_to_tensor(a, name=""a"")
    b = tf.convert_to_tensor(b, name=""b"")
    c = tf.convert_to_tensor(c, name=""c"")
    return foo_op(..., name=scope)
"
"tf.nest.map_structure(
    func, *structure, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import map_structure
a = {""hello"": 24, ""world"": 76}
tf.nest.map_structure(lambda p: p * 2, a)
{'hello': 48, 'world': 152}"
"tf.math.approx_max_k(
    operand,
    k,
    reduction_dimension=-1,
    recall_target=0.95,
    reduction_input_size_override=-1,
    aggregate_to_topk=True,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import approx_max_k
import tensorflow as tf
@tf.function(jit_compile=True)
def mips(qy, db, k=10, recall_target=0.95):
  dists = tf.einsum('ik,jk->ij', qy, db)
  return tf.nn.approx_max_k(dists, k=k, recall_target=recall_target)
qy = tf.random.uniform((256,128))
db = tf.random.uniform((2048,128))
dot_products, neighbors = mips(qy, db, k=20)"
"tf.math.approx_min_k(
    operand,
    k,
    reduction_dimension=-1,
    recall_target=0.95,
    reduction_input_size_override=-1,
    aggregate_to_topk=True,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import approx_min_k
import tensorflow as tf
@tf.function(jit_compile=True)
def l2_ann(qy, db, half_db_norms, k=10, recall_target=0.95):
  dists = half_db_norms - tf.einsum('ik,jk->ij', qy, db)
  return tf.nn.approx_min_k(dists, k=k, recall_target=recall_target)
qy = tf.random.uniform((256,128))
db = tf.random.uniform((2048,128))
half_db_norms = tf.norm(db, axis=1) / 2
dists, neighbors = l2_ann(qy, db, half_db_norms)"
"tf.nn.compute_average_loss(
    per_example_loss, sample_weight=None, global_batch_size=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import compute_average_loss
with strategy.scope():
  def compute_loss(labels, predictions, sample_weight=None):

    per_example_loss = tf.keras.losses.sparse_categorical_crossentropy(
        labels, predictions)

    return tf.nn.compute_average_loss(
        per_example_loss,
        sample_weight=sample_weight,
        global_batch_size=GLOBAL_BATCH_SIZE)
"
"tf.nn.ctc_greedy_decoder(
    inputs, sequence_length, merge_repeated=True, blank_index=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ctc_greedy_decoder
inf = float(""inf"")
logits = tf.constant([[[   0., -inf, -inf],
                       [ -2.3, -inf, -0.1]],
                      [[ -inf, -0.5, -inf],
                       [ -inf, -inf, -0.1]],
                      [[ -inf, -inf, -inf],
                       [ -0.1, -inf, -2.3]]])
seq_lens = tf.constant([2, 3])
outputs = tf.nn.ctc_greedy_decoder(
    logits,
    seq_lens,
    blank_index=1)"
"tf.nn.dropout(
    x, rate, noise_shape=None, seed=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import dropout
tf.random.set_seed(0)
x = tf.ones([3,5])
tf.nn.dropout(x, rate = 0.5, seed = 1).numpy()
array([[2., 0., 0., 2., 2.],
     [2., 2., 2., 2., 2.],
     [2., 0., 2., 0., 2.]], dtype=float32)"
"tf.nn.elu(
    features, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import elu
tf.nn.elu(1.0)
<tf.Tensor: shape=(), dtype=float32, numpy=1.0>
<tf.Tensor: shape=(), dtype=float32, numpy=1.0>
tf.nn.elu(0.0)
<tf.Tensor: shape=(), dtype=float32, numpy=0.0>
<tf.Tensor: shape=(), dtype=float32, numpy=0.0>
tf.nn.elu(-1000.0)
<tf.Tensor: shape=(), dtype=float32, numpy=-1.0>
<tf.Tensor: shape=(), dtype=float32, numpy=-1.0>"
"tf.nn.experimental.stateless_dropout(
    x, rate, seed, rng_alg=None, noise_shape=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_dropout
x = tf.ones([3,5])
tf.nn.experimental.stateless_dropout(x, rate=0.5, seed=[1, 0])
<tf.Tensor: shape=(3, 5), dtype=float32, numpy=
<tf.Tensor: shape=(3, 5), dtype=float32, numpy=
array([[2., 0., 2., 0., 0.],
       [0., 0., 2., 0., 2.],
       [0., 0., 0., 0., 2.]], dtype=float32)>"
"tf.nn.gelu(
    features, approximate=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import gelu
x = tf.constant([-3.0, -1.0, 0.0, 1.0, 3.0], dtype=tf.float32)
y = tf.nn.gelu(x)
y.numpy()
array([-0.00404951, -0.15865529,  0.        ,  0.8413447 ,  2.9959507 ],
    dtype=float32)
y = tf.nn.gelu(x, approximate=True)
y.numpy()
array([-0.00363752, -0.15880796,  0.        ,  0.841192  ,  2.9963627 ],
    dtype=float32)"
"tf.nn.isotonic_regression(
    inputs, decreasing=True, axis=-1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import isotonic_regression
>>> x = tf.constant([[3, 1, 2], [1, 3, 4]], dtype=tf.float32)
>>> y, segments = tf.nn.isotonic_regression(x, axis=1)
<tf.Tensor: shape=(2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 3), dtype=float32, numpy=
array([[3.       , 1.5      , 1.5      ],
       [2.6666667, 2.6666667, 2.6666667]], dtype=float32)>
"
"tf.nn.max_pool(
    input, ksize, strides, padding, data_format=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import max_pool
matrix = tf.constant([
    [0, 0, 1, 7],
    [0, 2, 0, 0],
    [5, 2, 0, 0],
    [0, 0, 9, 8],
])
reshaped = tf.reshape(matrix, (1, 4, 4, 1))
tf.nn.max_pool(reshaped, ksize=2, strides=2, padding=""SAME"")
<tf.Tensor: shape=(1, 2, 2, 1), dtype=int32, numpy=
<tf.Tensor: shape=(1, 2, 2, 1), dtype=int32, numpy=
array([[[[2],
         [7]],
        [[5],
         [9]]]], dtype=int32)>"
"tf.nn.max_pool2d(
    input, ksize, strides, padding, data_format='NHWC', name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import max_pool2d
x = tf.constant([[1., 2., 3., 4.],
                 [5., 6., 7., 8.],
                 [9., 10., 11., 12.]])
x = x[tf.newaxis, :, :, tf.newaxis]
result = tf.nn.max_pool2d(x, ksize=(2, 2), strides=(2, 2),
                          padding=""VALID"")
result[0, :, :, 0]
<tf.Tensor: shape=(1, 2), dtype=float32, numpy=
<tf.Tensor: shape=(1, 2), dtype=float32, numpy=
array([[6., 8.]], dtype=float32)>"
"tf.nn.nce_loss(
    weights,
    biases,
    labels,
    inputs,
    num_sampled,
    num_classes,
    num_true=1,
    sampled_values=None,
    remove_accidental_hits=False,
    name='nce_loss'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import nce_loss
if mode == ""train"":
  loss = tf.nn.nce_loss(
      weights=weights,
      biases=biases,
      labels=labels,
      inputs=inputs,
      ...)
elif mode == ""eval"":
  logits = tf.matmul(inputs, tf.transpose(weights))
  logits = tf.nn.bias_add(logits, biases)
  labels_one_hot = tf.one_hot(labels, n_classes)
  loss = tf.nn.sigmoid_cross_entropy_with_logits(
      labels=labels_one_hot,
      logits=logits)
  loss = tf.reduce_sum(loss, axis=1)
"
"tf.nn.relu(
    features, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import relu
>>> tf.nn.relu([-2., 0., 3.]).numpy()
array([0., 0., 3.], dtype=float32)
"
"tf.nn.relu6(
    features, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import relu6
x = tf.constant([-3.0, -1.0, 0.0, 6.0, 10.0], dtype=tf.float32)
y = tf.nn.relu6(x)
y.numpy()
array([0., 0., 0., 6., 6.], dtype=float32)"
"tf.nn.sampled_softmax_loss(
    weights,
    biases,
    labels,
    inputs,
    num_sampled,
    num_classes,
    num_true=1,
    sampled_values=None,
    remove_accidental_hits=True,
    seed=None,
    name='sampled_softmax_loss'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sampled_softmax_loss
if mode == ""train"":
  loss = tf.nn.sampled_softmax_loss(
      weights=weights,
      biases=biases,
      labels=labels,
      inputs=inputs,
      ...)
elif mode == ""eval"":
  logits = tf.matmul(inputs, tf.transpose(weights))
  logits = tf.nn.bias_add(logits, biases)
  labels_one_hot = tf.one_hot(labels, n_classes)
  loss = tf.nn.softmax_cross_entropy_with_logits(
      labels=labels_one_hot,
      logits=logits)
"
"tf.nn.scale_regularization_loss(
    regularization_loss
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import scale_regularization_loss
with strategy.scope():
  def compute_loss(self, label, predictions):
    per_example_loss = tf.keras.losses.sparse_categorical_crossentropy(
        labels, predictions)

    loss = tf.nn.compute_average_loss(
        per_example_loss,
        sample_weight=sample_weight,
        global_batch_size=GLOBAL_BATCH_SIZE)

    loss += tf.nn.scale_regularization_loss(tf.nn.l2_loss(weights))
    return loss
"
"tf.math.sigmoid(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sigmoid
x = tf.constant([0.0, 1.0, 50.0, 100.0])
tf.math.sigmoid(x)
<tf.Tensor: shape=(4,), dtype=float32,
<tf.Tensor: shape=(4,), dtype=float32,
numpy=array([0.5, 0.7310586, 1.0, 1.0], dtype=float32)>"
"tf.nn.softmax(
    logits, axis=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis, keepdims=True)
"
"tf.nn.softmax_cross_entropy_with_logits(
    labels, logits, axis=-1, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import softmax_cross_entropy_with_logits
logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]
labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]
tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)
<tf.Tensor: shape=(2,), dtype=float32,
<tf.Tensor: shape=(2,), dtype=float32,
numpy=array([0.16984604, 0.82474494], dtype=float32)>"
"tf.math.softplus(
    features, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import softplus
import tensorflow as tf
tf.math.softplus(tf.range(0, 2, dtype=tf.float32)).numpy()
array([0.6931472, 1.3132616], dtype=float32)"
"tf.nn.sparse_softmax_cross_entropy_with_logits(
    labels, logits, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sparse_softmax_cross_entropy_with_logits
logits = tf.constant([[2., -5., .5, -.1],
                      [0., 0., 1.9, 1.4],
                      [-100., 100., -100., -100.]])
labels = tf.constant([0, 3, 1])
tf.nn.sparse_softmax_cross_entropy_with_logits(
    labels=labels, logits=logits).numpy()
array([0.29750752, 1.1448325 , 0.        ], dtype=float32)"
"tf.math.top_k(
    input, k=1, sorted=True, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import top_k
result = tf.math.top_k([1, 2, 98, 1, 1, 99, 3, 1, 3, 96, 4, 1],
                        k=3)
result.values.numpy()
array([99, 98, 96], dtype=int32)
result.indices.numpy()
array([5, 2, 9], dtype=int32)"
"tf.math.zero_fraction(
    value, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import zero_fraction
    z = tf.nn.relu(...)
    summ = tf.compat.v1.summary.scalar('sparsity', tf.nn.zero_fraction(z))
"
"tf.no_gradient(
    op_type
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import no_gradient
tf.no_gradient(""Size"")
"
"tf.math.not_equal(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import not_equal
x = tf.constant([2, 4])
y = tf.constant(2)
tf.math.not_equal(x, y)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>"
"tf.numpy_function(
    func, inp, Tout, stateful=True, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import numpy_function
def my_numpy_func(x):
  return np.sinh(x)
@tf.function(input_signature=[tf.TensorSpec(None, tf.float32)])
def tf_function(input):
  y = tf.numpy_function(my_numpy_func, [input], tf.float32)
  return y * y
tf_function(tf.constant(1.))
<tf.Tensor: shape=(), dtype=float32, numpy=1.3810978>
<tf.Tensor: shape=(), dtype=float32, numpy=1.3810978>"
"tf.ones(
    shape,
    dtype=tf.dtypes.float32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ones
tf.ones([3, 4], tf.int32)
<tf.Tensor: shape=(3, 4), dtype=int32, numpy=
<tf.Tensor: shape=(3, 4), dtype=int32, numpy=
array([[1, 1, 1, 1],
       [1, 1, 1, 1],
       [1, 1, 1, 1]], dtype=int32)>"
"tf.ones_like(
    input, dtype=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ones_like
tensor = tf.constant([[1, 2, 3], [4, 5, 6]])
tf.ones_like(tensor)
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
  array([[1, 1, 1],
         [1, 1, 1]], dtype=int32)>"
"tf.keras.optimizers.experimental.RMSprop(
    learning_rate=0.001,
    rho=0.9,
    momentum=0.0,
    epsilon=1e-07,
    centered=False,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=100,
    jit_compile=True,
    name='RMSprop',
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RMSprop
opt = tf.keras.optimizers.experimental.RMSprop(learning_rate=0.1)
var1 = tf.Variable(10.0)
opt.minimize(loss, [var1])
var1.numpy()
9.683772"
"tf.keras.optimizers.experimental.RMSprop(
    learning_rate=0.001,
    rho=0.9,
    momentum=0.0,
    epsilon=1e-07,
    centered=False,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=100,
    jit_compile=True,
    name='RMSprop',
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RMSprop
opt = tf.keras.optimizers.experimental.RMSprop(learning_rate=0.1)
var1 = tf.Variable(10.0)
opt.minimize(loss, [var1])
var1.numpy()
9.683772"
"tf.keras.optimizers.legacy.Adam(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    amsgrad=False,
    name='Adam',
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Adam
opt = tf.keras.optimizers.legacy.Adam(learning_rate=0.1)
var1 = tf.Variable(10.0)
step_count = opt.minimize(loss, [var1]).numpy()
var1.numpy()
9.9"
"tf.keras.optimizers.legacy.Nadam(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    name='Nadam',
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Nadam
>>> opt = tf.keras.optimizers.legacy.Nadam(learning_rate=0.2)
>>> var1 = tf.Variable(10.0)
>>> loss = lambda: (var1 ** 2) / 2.0
>>> step_count = opt.minimize(loss, [var1]).numpy()
>>> ""{:.1f}"".format(var1.numpy())
9.8
"
"tf.keras.optimizers.legacy.RMSprop(
    learning_rate=0.001,
    rho=0.9,
    momentum=0.0,
    epsilon=1e-07,
    centered=False,
    name='RMSprop',
    **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import RMSprop
opt = tf.keras.optimizers.legacy.RMSprop(learning_rate=0.1)
var1 = tf.Variable(10.0)
step_count = opt.minimize(loss, [var1]).numpy()
var1.numpy()
9.683772"
"tf.keras.optimizers.schedules.CosineDecayRestarts(
    initial_learning_rate,
    first_decay_steps,
    t_mul=2.0,
    m_mul=1.0,
    alpha=0.0,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CosineDecayRestarts
first_decay_steps = 1000
lr_decayed_fn = (
  tf.keras.optimizers.schedules.CosineDecayRestarts(
      initial_learning_rate,
      first_decay_steps))
"
"tf.keras.optimizers.schedules.PiecewiseConstantDecay(
    boundaries, values, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import PiecewiseConstantDecay
step = tf.Variable(0, trainable=False)
boundaries = [100000, 110000]
values = [1.0, 0.5, 0.1]
learning_rate_fn = keras.optimizers.schedules.PiecewiseConstantDecay(
    boundaries, values)

learning_rate = learning_rate_fn(step)
"
"tf.keras.optimizers.schedules.deserialize(
    config, custom_objects=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import deserialize
config = {
  'class_name': 'PolynomialDecay',
  'config': {'cycle': False,
    'decay_steps': 10000,
    'end_learning_rate': 0.01,
    'initial_learning_rate': 0.1,
    'name': None,
    'power': 0.5} }
lr_schedule = tf.keras.optimizers.schedules.deserialize(config)
"
"tf.keras.optimizers.schedules.serialize(
    learning_rate_schedule
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import serialize
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
  0.1, decay_steps=100000, decay_rate=0.96, staircase=True)
tf.keras.optimizers.schedules.serialize(lr_schedule)
{'class_name': 'ExponentialDecay', 'config': {...} }"
"tf.keras.optimizers.serialize(
    optimizer
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import serialize
tf.keras.optimizers.serialize(tf.keras.optimizers.legacy.SGD())
{'class_name': 'SGD', 'config': {'name': 'SGD', 'learning_rate': 0.01,
                                 'decay': 0.0, 'momentum': 0.0,
                                 'nesterov': False} }"
"tf.pad(
    tensor, paddings, mode='CONSTANT', constant_values=0, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import pad
t = tf.constant([[1, 2, 3], [4, 5, 6]])
paddings = tf.constant([[1, 1,], [2, 2]])


"
"tf.parallel_stack(
    values, name='parallel_stack'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import parallel_stack
x = tf.constant([1, 4])
y = tf.constant([2, 5])
z = tf.constant([3, 6])
"
"tf.math.pow(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import pow
x = tf.constant([[2, 2], [3, 3]])
y = tf.constant([[8, 16], [2, 3]])
"
"tf.print(
    *inputs, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import print
tensor = tf.range(10)
tf.print(tensor, output_stream=sys.stderr)
"
"tf.profiler.experimental.Profile(
    logdir, options=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Profile
with tf.profiler.experimental.Profile(""/path/to/logdir""):
"
"tf.profiler.experimental.Trace(
    name, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Trace
tf.profiler.experimental.start('logdir')
for step in range(num_steps):
  with tf.profiler.experimental.Trace(""Train"", step_num=step, _r=1):
    train_fn()
tf.profiler.experimental.stop()
"
"tf.profiler.experimental.client.monitor(
    service_addr, duration_ms, level=1
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import monitor

  for query in range(0, 100):
    print(
      tf.profiler.experimental.client.monitor('grpc://10.0.0.2:8466', 1000))
"
"tf.profiler.experimental.client.trace(
    service_addr,
    logdir,
    duration_ms,
    worker_list='',
    num_tracing_attempts=3,
    options=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import trace
  tf.profiler.experimental.server.start(6009)
  tf.profiler.experimental.client.trace('grpc://localhost:6009',
                                        '/nfs/tb_log', 2000)
"
"tf.profiler.experimental.start(
    logdir, options=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import start
options = tf.profiler.experimental.ProfilerOptions(host_tracer_level = 3,
                                                   python_tracer_level = 1,
                                                   device_tracer_level = 1)
tf.profiler.experimental.start('logdir_path', options = options)
tf.profiler.experimental.stop()
"
"tf.py_function(
    func, inp, Tout, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import py_function
def log_huber(x, m):
  if tf.abs(x) <= m:
  if tf.abs(x) <= m:
    return x**2
  else:
    return m**2 * (1 - 2 * tf.math.log(m) + tf.math.log(x**2))

x = tf.constant(1.0)
m = tf.constant(2.0)

with tf.GradientTape() as t:
  t.watch([x, m])
  y = tf.py_function(func=log_huber, inp=[x, m], Tout=tf.float32)

dy_dx = t.gradient(y, x)
assert dy_dx.numpy() == 2.0
"
"tf.quantization.quantize_and_dequantize_v2(
    input,
    input_min,
    input_max,
    signed_input=True,
    num_bits=8,
    range_given=False,
    round_mode='HALF_TO_EVEN',
    name=None,
    narrow_range=False,
    axis=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import quantize_and_dequantize_v2
def getQuantizeOp(input):
    input_tensor = tf.placeholder(tf.float32, shape=[4, 4])
    net = tf.quantization.quantize_and_dequantize(input,
                                                  input_min=min_threshold,
                                                  input_max=max_threshold,
                                                  range_given=True)

To simulate v1 behavior:

def testDecomposeQuantizeDequantize(self):
    def f(input_tensor):
      return tf.quantization.quantize_and_dequantize_v2(input_tensor,
                                                        input_min = 5.0,
                                                        input_max= -10.0,
                                                        range_given=True)
    input_tensor = tf.placeholder(tf.float32, shape=[4, 4])
    net = tf.grad_pass_through(f)(input_tensor)
"
"tf.ragged.constant(
    pylist,
    dtype=None,
    ragged_rank=None,
    inner_shape=None,
    name=None,
    row_splits_dtype=tf.dtypes.int64
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import constant
tf.ragged.constant([[1, 2], [3], [4, 5, 6]])
<tf.RaggedTensor [[1, 2], [3], [4, 5, 6]]>
<tf.RaggedTensor [[1, 2], [3], [4, 5, 6]]>"
"tf.ragged.cross(
    inputs, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import cross
tf.ragged.cross([tf.ragged.constant([['a'], ['b', 'c']]),
                 tf.ragged.constant([['d'], ['e']]),
                 tf.ragged.constant([['f'], ['g']])])
<tf.RaggedTensor [[b'a_X_d_X_f'], [b'b_X_e_X_g', b'c_X_e_X_g']]>
<tf.RaggedTensor [[b'a_X_d_X_f'], [b'b_X_e_X_g', b'c_X_e_X_g']]>"
"tf.ragged.cross_hashed(
    inputs, num_buckets=0, hash_key=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import cross_hashed
tf.ragged.cross_hashed([tf.ragged.constant([['a'], ['b', 'c']]),
                        tf.ragged.constant([['d'], ['e']]),
                        tf.ragged.constant([['f'], ['g']])],
                       num_buckets=100)
<tf.RaggedTensor [[78], [66, 74]]>
<tf.RaggedTensor [[78], [66, 74]]>"
"tf.ragged.map_flat_values(
    op, *args, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import map_flat_values
rt = tf.ragged.constant([[1, 2, 3], [], [4, 5], [6]])
tf.ragged.map_flat_values(tf.ones_like, rt)
<tf.RaggedTensor [[1, 1, 1], [], [1, 1], [1]]>
<tf.RaggedTensor [[1, 1, 1], [], [1, 1], [1]]>
tf.ragged.map_flat_values(tf.multiply, rt, rt)
<tf.RaggedTensor [[1, 4, 9], [], [16, 25], [36]]>
<tf.RaggedTensor [[1, 4, 9], [], [16, 25], [36]]>
tf.ragged.map_flat_values(tf.add, rt, 5)
<tf.RaggedTensor [[6, 7, 8], [], [9, 10], [11]]>
<tf.RaggedTensor [[6, 7, 8], [], [9, 10], [11]]>"
"tf.ragged.range(
    starts,
    limits=None,
    deltas=1,
    dtype=None,
    name=None,
    row_splits_dtype=tf.dtypes.int64
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import range
ragged.range(starts, limits, deltas)[i] ==
    tf.range(starts[i], limits[i], deltas[i])
"
"tf.ragged.row_splits_to_segment_ids(
    splits, name=None, out_type=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import row_splits_to_segment_ids
print(tf.ragged.row_splits_to_segment_ids([0, 3, 3, 5, 6, 9]))
 tf.Tensor([0 0 0 2 2 3 4 4 4], shape=(9,), dtype=int64)"
"tf.ragged.segment_ids_to_row_splits(
    segment_ids, num_segments=None, out_type=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import segment_ids_to_row_splits
print(tf.ragged.segment_ids_to_row_splits([0, 0, 0, 2, 2, 3, 4, 4, 4]))
tf.Tensor([0 3 3 5 6 9], shape=(6,), dtype=int64)"
"tf.ragged.stack(
    values: typing.List[ragged_tensor.RaggedOrDense], axis=0, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stack
t1 = tf.ragged.constant([[1, 2], [3, 4, 5]])
t2 = tf.ragged.constant([[6], [7, 8, 9]])
tf.ragged.stack([t1, t2], axis=0)
<tf.RaggedTensor [[[1, 2], [3, 4, 5]], [[6], [7, 8, 9]]]>
<tf.RaggedTensor [[[1, 2], [3, 4, 5]], [[6], [7, 8, 9]]]>
tf.ragged.stack([t1, t2], axis=1)
<tf.RaggedTensor [[[1, 2], [6]], [[3, 4, 5], [7, 8, 9]]]>
<tf.RaggedTensor [[[1, 2], [6]], [[3, 4, 5], [7, 8, 9]]]>"
"tf.ragged.stack_dynamic_partitions(
    data, partitions, num_partitions, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stack_dynamic_partitions
data           = ['a', 'b', 'c', 'd', 'e']
partitions     = [  3,   0,   2,   2,   3]
num_partitions = 5
tf.ragged.stack_dynamic_partitions(data, partitions, num_partitions)
<tf.RaggedTensor [[b'b'], [], [b'c', b'd'], [b'a', b'e'], []]>
<tf.RaggedTensor [[b'b'], [], [b'c', b'd'], [b'a', b'e'], []]>"
"tf.random.Generator(
    copy_from=None, state=None, alg=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
g = tf.random.Generator.from_seed(1234)
g.normal(shape=(2, 3))
<tf.Tensor: shape=(2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 3), dtype=float32, numpy=
array([[ 0.9356609 ,  1.0854305 , -0.93788373],
       [-0.5061547 ,  1.3169702 ,  0.7137579 ]], dtype=float32)>"
"tf.random.categorical(
    logits, num_samples, dtype=None, seed=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import categorical
samples = tf.random.categorical(tf.math.log([[0.5, 0.5]]), 5)
"
"tf.random.create_rng_state(
    seed, alg
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import create_rng_state
tf.random.create_rng_state(
    1234, ""philox"")
<tf.Tensor: shape=(3,), dtype=int64, numpy=array([1234,    0,    0])>
<tf.Tensor: shape=(3,), dtype=int64, numpy=array([1234,    0,    0])>
tf.random.create_rng_state(
    [12, 34], ""threefry"")
<tf.Tensor: shape=(2,), dtype=int64, numpy=array([12, 34])>
<tf.Tensor: shape=(2,), dtype=int64, numpy=array([12, 34])>"
"tf.random.Generator(
    copy_from=None, state=None, alg=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
g = tf.random.Generator.from_seed(1234)
g.normal(shape=(2, 3))
<tf.Tensor: shape=(2, 3), dtype=float32, numpy=
<tf.Tensor: shape=(2, 3), dtype=float32, numpy=
array([[ 0.9356609 ,  1.0854305 , -0.93788373],
       [-0.5061547 ,  1.3169702 ,  0.7137579 ]], dtype=float32)>"
"tf.random.create_rng_state(
    seed, alg
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import create_rng_state
tf.random.create_rng_state(
    1234, ""philox"")
<tf.Tensor: shape=(3,), dtype=int64, numpy=array([1234,    0,    0])>
<tf.Tensor: shape=(3,), dtype=int64, numpy=array([1234,    0,    0])>
tf.random.create_rng_state(
    [12, 34], ""threefry"")
<tf.Tensor: shape=(2,), dtype=int64, numpy=array([12, 34])>
<tf.Tensor: shape=(2,), dtype=int64, numpy=array([12, 34])>"
"tf.random.experimental.index_shuffle(
    index, seed, max_index
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import index_shuffle
vector = tf.constant(['e0', 'e1', 'e2', 'e3'])
indices = tf.random.experimental.index_shuffle(tf.range(4), [5, 9], 3)
shuffled_vector = tf.gather(vector, indices)
print(shuffled_vector)
tf.Tensor([b'e2' b'e0' b'e1' b'e3'], shape=(4,), dtype=string)"
"tf.random.experimental.stateless_fold_in(
    seed, data, alg='auto_select'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_fold_in
master_seed = [1, 2]
replica_id = 3
replica_seed = tf.random.experimental.stateless_fold_in(
  master_seed, replica_id)
print(replica_seed)
tf.Tensor([1105988140          3], shape=(2,), dtype=int32)
tf.random.stateless_normal(shape=[3], seed=replica_seed)
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.03197195, 0.8979765 ,
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.03197195, 0.8979765 ,
0.13253039], dtype=float32)>"
"tf.random.experimental.stateless_split(
    seed, num=2, alg='auto_select'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_split
seed = [1, 2]
new_seeds = tf.random.experimental.stateless_split(seed, num=3)
print(new_seeds)
tf.Tensor(
[[1105988140 1738052849]
 [-335576002  370444179]
 [  10670227 -246211131]], shape=(3, 2), dtype=int32)
tf.random.stateless_normal(shape=[3], seed=new_seeds[0, :])
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([-0.59835213, -0.9578608 ,
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([-0.59835213, -0.9578608 ,
0.9002807 ], dtype=float32)>"
"tf.random.gamma(
    shape,
    alpha,
    beta=None,
    dtype=tf.dtypes.float32,
    seed=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import gamma
samples = tf.random.gamma([10], [0.5, 1.5])

samples = tf.random.gamma([7, 5], [0.5, 1.5])

alpha = tf.constant([[1.],[3.],[5.]])
beta = tf.constant([[3., 4.]])
samples = tf.random.gamma([30], alpha=alpha, beta=beta)

loss = tf.reduce_mean(tf.square(samples))
dloss_dalpha, dloss_dbeta = tf.gradients(loss, [alpha, beta])
"
"tf.random.normal(
    shape,
    mean=0.0,
    stddev=1.0,
    dtype=tf.dtypes.float32,
    seed=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import normal
tf.random.set_seed(5);
tf.random.normal([4], 0, 1, tf.float32)
<tf.Tensor: shape=(4,), dtype=float32, numpy=..., dtype=float32)>
<tf.Tensor: shape=(4,), dtype=float32, numpy=..., dtype=float32)>"
"tf.random.poisson(
    shape,
    lam,
    dtype=tf.dtypes.float32,
    seed=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import poisson
samples = tf.random.poisson([10], [0.5, 1.5])

samples = tf.random.poisson([7, 5], [12.2, 3.3])
"
"tf.random.stateless_binomial(
    shape,
    seed,
    counts,
    probs,
    output_dtype=tf.dtypes.int32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_binomial
counts = [10., 20.]
probs = [0.8]

binomial_samples = tf.random.stateless_binomial(
    shape=[2], seed=[123, 456], counts=counts, probs=probs)

shape = [3, 4, 3, 4, 2]
binomial_samples = tf.random.stateless_binomial(
    shape=shape, seed=[123, 456], counts=counts, probs=probs)
"
"tf.random.stateless_categorical(
    logits,
    num_samples,
    seed,
    dtype=tf.dtypes.int64,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_categorical
samples = tf.random.stateless_categorical(
    tf.math.log([[0.5, 0.5]]), 5, seed=[7, 17])
"
"tf.random.stateless_gamma(
    shape,
    seed,
    alpha,
    beta=None,
    dtype=tf.dtypes.float32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_gamma
samples = tf.random.stateless_gamma([10, 2], seed=[12, 34], alpha=[0.5, 1.5])

samples = tf.random.stateless_gamma([7, 5, 2], seed=[12, 34], alpha=[.5, 1.5])

alpha = tf.constant([[1.], [3.], [5.]])
beta = tf.constant([[3., 4.]])
samples = tf.random.stateless_gamma(
    [30, 3, 2], seed=[12, 34], alpha=alpha, beta=beta)

with tf.GradientTape() as tape:
  tape.watch([alpha, beta])
  loss = tf.reduce_mean(tf.square(tf.random.stateless_gamma(
      [30, 3, 2], seed=[12, 34], alpha=alpha, beta=beta)))
dloss_dalpha, dloss_dbeta = tape.gradient(loss, [alpha, beta])
"
"tf.random.stateless_parameterized_truncated_normal(
    shape, seed, means=0.0, stddevs=1.0, minvals=-2.0, maxvals=2.0, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_parameterized_truncated_normal
means = 0.
stddevs = tf.math.exp(tf.random.uniform(shape=[2, 3]))
minvals = [-1., -2., -1000.]
maxvals = [[10000.], [1.]]
y = tf.random.stateless_parameterized_truncated_normal(
  shape=[10, 2, 3], seed=[7, 17],
  means=means, stddevs=stddevs, minvals=minvals, maxvals=maxvals)
y.shape
TensorShape([10, 2, 3])"
"tf.random.stateless_poisson(
    shape,
    seed,
    lam,
    dtype=tf.dtypes.int32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_poisson
samples = tf.random.stateless_poisson([10, 2], seed=[12, 34], lam=[5, 15])

samples = tf.random.stateless_poisson([7, 5, 2], seed=[12, 34], lam=[5, 15])

rate = tf.constant([[1.], [3.], [5.]])
samples = tf.random.stateless_poisson([30, 3, 1], seed=[12, 34], lam=rate)
"
"tf.random.stateless_uniform(
    shape,
    seed,
    minval=0,
    maxval=None,
    dtype=tf.dtypes.float32,
    name=None,
    alg='auto_select'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stateless_uniform
ints = tf.random.stateless_uniform(
    [10], seed=(2, 3), minval=None, maxval=None, dtype=tf.int32)
"
"tf.random.truncated_normal(
    shape,
    mean=0.0,
    stddev=1.0,
    dtype=tf.dtypes.float32,
    seed=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import truncated_normal
tf.random.truncated_normal(shape=[2])
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([..., ...], dtype=float32)>
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([..., ...], dtype=float32)>"
"tf.random.uniform(
    shape,
    minval=0,
    maxval=None,
    dtype=tf.dtypes.float32,
    seed=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import uniform
tf.random.uniform(shape=[2])
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([..., ...], dtype=float32)>
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([..., ...], dtype=float32)>
tf.random.uniform(shape=[], minval=-1., maxval=0.)
<tf.Tensor: shape=(), dtype=float32, numpy=-...>
<tf.Tensor: shape=(), dtype=float32, numpy=-...>
tf.random.uniform(shape=[], minval=5, maxval=10, dtype=tf.int64)
<tf.Tensor: shape=(), dtype=int64, numpy=...>
<tf.Tensor: shape=(), dtype=int64, numpy=...>"
"tf.random_normal_initializer(
    mean=0.0, stddev=0.05, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import random_normal_initializer
def make_variables(k, initializer):
  return (tf.Variable(initializer(shape=[k], dtype=tf.float32)),
          tf.Variable(initializer(shape=[k, k], dtype=tf.float32)))
v1, v2 = make_variables(3,
                        tf.random_normal_initializer(mean=1., stddev=2.))
v1
<tf.Variable ... shape=(3,) ... numpy=array([...], dtype=float32)>
<tf.Variable ... shape=(3,) ... numpy=array([...], dtype=float32)>
v2
<tf.Variable ... shape=(3, 3) ... numpy=
<tf.Variable ... shape=(3, 3) ... numpy=
make_variables(4, tf.random_uniform_initializer(minval=-1., maxval=1.))
(<tf.Variable...shape=(4,) dtype=float32...>, <tf.Variable...shape=(4, 4) ...
(<tf.Variable...shape=(4,) dtype=float32...>, <tf.Variable...shape=(4, 4) ..."
"tf.random_uniform_initializer(
    minval=-0.05, maxval=0.05, seed=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import random_uniform_initializer
def make_variables(k, initializer):
  return (tf.Variable(initializer(shape=[k], dtype=tf.float32)),
          tf.Variable(initializer(shape=[k, k], dtype=tf.float32)))
v1, v2 = make_variables(3, tf.ones_initializer())
v1
<tf.Variable ... shape=(3,) ... numpy=array([1., 1., 1.], dtype=float32)>
<tf.Variable ... shape=(3,) ... numpy=array([1., 1., 1.], dtype=float32)>
v2
<tf.Variable ... shape=(3, 3) ... numpy=
<tf.Variable ... shape=(3, 3) ... numpy=
array([[1., 1., 1.],
       [1., 1., 1.],
       [1., 1., 1.]], dtype=float32)>
make_variables(4, tf.random_uniform_initializer(minval=-1., maxval=1.))
(<tf.Variable...shape=(4,) dtype=float32...>, <tf.Variable...shape=(4, 4) ...
(<tf.Variable...shape=(4,) dtype=float32...>, <tf.Variable...shape=(4, 4) ..."
"tf.rank(
    input, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import rank
t = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])
"
"tf.raw_ops.Bitcast(
    input, type, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
a = [1., 2., 3.]
equality_bitcast = tf.bitcast(a, tf.complex128)
Traceback (most recent call last):
InvalidArgumentError: Cannot bitcast from 1 to 18 [Op:Bitcast]
equality_cast = tf.cast(a, tf.complex128)
print(equality_cast)
tf.Tensor([1.+0.j 2.+0.j 3.+0.j], shape=(3,), dtype=complex128)"
"tf.raw_ops.ComplexAbs(
    x,
    Tout=tf.dtypes.float32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ComplexAbs
x = tf.complex(3.0, 4.0)
print((tf.raw_ops.ComplexAbs(x=x, Tout=tf.dtypes.float32, name=None)).numpy())
5.0"
"tf.raw_ops.Fingerprint(
    data, method, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Fingerprint
Fingerprint(data) == Fingerprint(Reshape(data, ...))
Fingerprint(data) == Fingerprint(Bitcast(data, ...))
"
"tf.raw_ops.IdentityN(
    input, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
with tf.get_default_graph().gradient_override_map(
    {'IdentityN': 'OverrideGradientWithG'}):
  y, _ = identity_n([f(x), x])

@tf.RegisterGradient('OverrideGradientWithG')
def ApplyG(op, dy, _):
"
"tf.raw_ops.Pack(
    values, axis=0, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
pack([x, y, z], axis=1) => [[1, 2, 3], [4, 5, 6]]
"
"tf.raw_ops.TPUReplicatedInput(
    inputs, is_mirrored_variable=False, index=-1, is_packed=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
%a = ""tf.opA""()
%b = ""tf.opB""()
%replicated_input = ""tf.TPUReplicatedInput""(%a, %b)
%computation = ""tf.Computation""(%replicated_input)
"
"tf.raw_ops.TPUReplicatedOutput(
    input, num_replicas, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
%computation = ""tf.Computation""()
%replicated_output:2 = ""tf.TPUReplicatedOutput""(%computation)
"
"tf.raw_ops.UniqueWithCountsV2(
    x,
    axis,
    out_idx=tf.dtypes.int32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import UniqueWithCountsV2
x = tf.constant([1, 1, 2, 4, 4, 4, 7, 8, 8])
y, idx, count = UniqueWithCountsV2(x, axis = [0])
y ==> [1, 2, 4, 7, 8]
idx ==> [0, 0, 1, 2, 2, 2, 3, 4, 4]
count ==> [2, 1, 3, 1, 2]
"
"tf.math.reduce_all(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_all
>>> x = tf.constant([[True,  True], [False, False]])
>>> tf.math.reduce_all(x)
<tf.Tensor: shape=(), dtype=bool, numpy=False>
<tf.Tensor: shape=(), dtype=bool, numpy=False>
>>> tf.math.reduce_all(x, 0)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False, False])>
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False, False])>
>>> tf.math.reduce_all(x, 1)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>
"
"tf.math.reduce_any(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_any
>>> x = tf.constant([[True,  True], [False, False]])
>>> tf.reduce_any(x)
<tf.Tensor: shape=(), dtype=bool, numpy=True>
<tf.Tensor: shape=(), dtype=bool, numpy=True>
>>> tf.reduce_any(x, 0)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  True])>
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  True])>
>>> tf.reduce_any(x, 1)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>
"
"tf.math.reduce_logsumexp(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_logsumexp
x = tf.constant([[0., 0., 0.], [0., 0., 0.]])
"
"tf.math.reduce_max(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_max
>>> x = tf.constant([5, 1, 2, 4])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=int32, numpy=5>
<tf.Tensor: shape=(), dtype=int32, numpy=5>
>>> x = tf.constant([-5, -1, -2, -4])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=int32, numpy=-1>
<tf.Tensor: shape=(), dtype=int32, numpy=-1>
>>> x = tf.constant([4, float('nan')])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=float32, numpy=nan>
<tf.Tensor: shape=(), dtype=float32, numpy=nan>
>>> x = tf.constant([float('nan'), float('nan')])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=float32, numpy=nan>
<tf.Tensor: shape=(), dtype=float32, numpy=nan>
>>> x = tf.constant([float('-inf'), float('inf')])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=float32, numpy=inf>
<tf.Tensor: shape=(), dtype=float32, numpy=inf>
"
"tf.math.reduce_mean(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_mean
x = tf.constant([[1., 1.], [2., 2.]])
tf.reduce_mean(x)
<tf.Tensor: shape=(), dtype=float32, numpy=1.5>
<tf.Tensor: shape=(), dtype=float32, numpy=1.5>
tf.reduce_mean(x, 0)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.5, 1.5], dtype=float32)>
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.5, 1.5], dtype=float32)>
tf.reduce_mean(x, 1)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 2.], dtype=float32)>
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 2.], dtype=float32)>"
"tf.math.reduce_min(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_min
a = tf.constant([
  [[1, 2], [3, 4]],
  [[1, 2], [3, 4]]
])
tf.reduce_min(a)
<tf.Tensor: shape=(), dtype=int32, numpy=1>
<tf.Tensor: shape=(), dtype=int32, numpy=1>"
"tf.math.reduce_prod(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_prod
>>> x = tf.constant([[1., 2.], [3., 4.]])
>>> tf.math.reduce_prod(x)
<tf.Tensor: shape=(), dtype=float32, numpy=24.>
<tf.Tensor: shape=(), dtype=float32, numpy=24.>
>>> tf.math.reduce_prod(x, 0)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([3., 8.], dtype=float32)>
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([3., 8.], dtype=float32)>
>>> tf.math.reduce_prod(x, 1)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 12.],
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 12.],
dtype=float32)>
"
"tf.math.reduce_sum(
    input_tensor, axis=None, keepdims=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_sum
>>> x = tf.constant([[1, 1, 1], [1, 1, 1]])
>>> x.numpy()
array([[1, 1, 1],
       [1, 1, 1]], dtype=int32)
>>> tf.reduce_sum(x).numpy()
6
>>> tf.reduce_sum(x, 0).numpy()
array([2, 2, 2], dtype=int32)
>>> tf.reduce_sum(x, 1).numpy()
array([3, 3], dtype=int32)
>>> tf.reduce_sum(x, 1, keepdims=True).numpy()
array([[3],
       [3]], dtype=int32)
>>> tf.reduce_sum(x, [0, 1]).numpy()
6
"
"tf.repeat(
    input, repeats, axis=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import repeat
repeat(['a', 'b', 'c'], repeats=[3, 0, 2], axis=0)
<tf.Tensor: shape=(5,), dtype=string,
<tf.Tensor: shape=(5,), dtype=string,
numpy=array([b'a', b'a', b'a', b'c', b'c'], dtype=object)>"
"tf.reshape(
    tensor, shape, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reshape
t1 = [[1, 2, 3],
      [4, 5, 6]]
print(tf.shape(t1).numpy())
[2 3]
t2 = tf.reshape(t1, [6])
t2
<tf.Tensor: shape=(6,), dtype=int32,
<tf.Tensor: shape=(6,), dtype=int32,
  numpy=array([1, 2, 3, 4, 5, 6], dtype=int32)>
tf.reshape(t2, [3, 2])
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
  array([[1, 2],
         [3, 4],
         [5, 6]], dtype=int32)>"
"tf.reverse(
    tensor, axis, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reverse

reverse(t, dims) ==> [[[[ 3,  2,  1,  0],
                        [ 7,  6,  5,  4],
                        [ 11, 10, 9, 8]],
                       [[15, 14, 13, 12],
                        [19, 18, 17, 16],
                        [23, 22, 21, 20]]]]

reverse(t, dims) ==> [[[[12, 13, 14, 15],
                        [16, 17, 18, 19],
                        [20, 21, 22, 23]
                       [[ 0,  1,  2,  3],
                        [ 4,  5,  6,  7],
                        [ 8,  9, 10, 11]]]]

reverse(t, dims) ==> [[[[8, 9, 10, 11],
                        [4, 5, 6, 7],
                        [0, 1, 2, 3]]
                       [[20, 21, 22, 23],
                        [16, 17, 18, 19],
                        [12, 13, 14, 15]]]]
"
"tf.reverse_sequence(
    input, seq_lengths, seq_axis=None, batch_axis=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reverse_sequence
seq_lengths = [7, 2, 3, 5]
input = [[1, 2, 3, 4, 5, 0, 0, 0], [1, 2, 0, 0, 0, 0, 0, 0],
         [1, 2, 3, 4, 0, 0, 0, 0], [1, 2, 3, 4, 5, 6, 7, 8]]
output = tf.reverse_sequence(input, seq_lengths, seq_axis=1, batch_axis=0)
output
<tf.Tensor: shape=(4, 8), dtype=int32, numpy=
<tf.Tensor: shape=(4, 8), dtype=int32, numpy=
array([[0, 0, 5, 4, 3, 2, 1, 0],
       [2, 1, 0, 0, 0, 0, 0, 0],
       [3, 2, 1, 4, 0, 0, 0, 0],
       [5, 4, 3, 2, 1, 6, 7, 8]], dtype=int32)>"
"tf.roll(
    input, shift, axis, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import roll
roll(t, shift=2, axis=0) ==> [3, 4, 0, 1, 2]

roll(t, shift=[1, -2], axis=[0, 1]) ==> [[7, 8, 9, 5, 6], [2, 3, 4, 0, 1]]

roll(t, shift=[2, -3], axis=[1, 1]) ==> [[1, 2, 3, 4, 0], [6, 7, 8, 9, 5]]
"
"tf.math.round(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import round
x = tf.constant([0.9, 2.5, 2.3, 1.5, -4.5])
"
"tf.saved_model.Asset(
    path
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Asset
filename = tf.saved_model.Asset(""file.txt"")

@tf.function(input_signature=[])
def func():
  return tf.io.read_file(filename)

trackable_obj = tf.train.Checkpoint()
trackable_obj.func = func
trackable_obj.filename = filename
tf.saved_model.save(trackable_obj, ""/tmp/saved_model"")

tf.io.gfile.remove(""file.txt"")
tf.io.gfile.rename(""/tmp/saved_model"", ""/tmp/new_location"")

reloaded_obj = tf.saved_model.load(""/tmp/new_location"")
print(reloaded_obj.func())
"
"tf.saved_model.experimental.TrackableResource(
    device=''
)
","import pandas as pd
import numpy as np
import tensorflow as tf
class DemoResource(tf.saved_model.experimental.TrackableResource):
  def __init__(self):
    super().__init__()
    self._initialize()
  def _create_resource(self):
    return tf.raw_ops.VarHandleOp(dtype=tf.float32, shape=[2])
  def _initialize(self):
    tf.raw_ops.AssignVariableOp(
        resource=self.resource_handle, value=tf.ones([2]))
  def _destroy_resource(self):
    tf.raw_ops.DestroyResourceOp(resource=self.resource_handle)
class DemoModule(tf.Module):
  def __init__(self):
    self.resource = DemoResource()
  def increment(self, tensor):
    return tensor + tf.raw_ops.ReadVariableOp(
        resource=self.resource.resource_handle, dtype=tf.float32)
demo = DemoModule()
demo.increment([5, 1])
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([6., 2.], dtype=float32)>
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([6., 2.], dtype=float32)>"
"tf.saved_model.load(
    export_dir, tags=None, options=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import load
imported = tf.saved_model.load(path)
f = imported.signatures[""serving_default""]
print(f(x=tf.constant([[1.]])))
"
"tf.math.scalar_mul(
    scalar, x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import scalar_mul
x = tf.reshape(tf.range(30, dtype=tf.float32), [10, 3])
with tf.GradientTape() as g:
  g.watch(x)
  z = tf.math.scalar_mul(10.0, y)"
"tf.scan(
    fn,
    elems,
    initializer=None,
    parallel_iterations=10,
    back_prop=True,
    swap_memory=False,
    infer_shape=True,
    reverse=False,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import scan
elems = np.array([1, 2, 3, 4, 5, 6])
sum = scan(lambda a, x: a + x, elems)
sum = scan(lambda a, x: a + x, elems, reverse=True)
"
"tf.searchsorted(
    sorted_sequence,
    values,
    side='left',
    out_type=tf.dtypes.int32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import searchsorted
edges = [-1, 3.3, 9.1, 10.0]
values = [0.0, 4.1, 12.0]
tf.searchsorted(edges, values).numpy()
array([1, 2, 4], dtype=int32)"
"tf.sets.difference(
    a, b, aminusb=True, validate_indices=True
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import difference
  import tensorflow as tf
  import collections

  a = collections.OrderedDict([
      ((0, 0, 0), 1),
      ((0, 0, 1), 2),
      ((0, 1, 0), 3),
      ((1, 0, 0), 4),
      ((1, 1, 0), 5),
      ((1, 1, 1), 6),
  ])
  a = tf.sparse.SparseTensor(list(a.keys()), list(a.values()),
                             dense_shape=[2, 2, 2])

  b = collections.OrderedDict([
      ((0, 0, 0), 1),
      ((0, 0, 1), 3),
      ((0, 1, 0), 2),
      ((1, 0, 0), 4),
      ((1, 0, 1), 5),
      ((1, 1, 0), 5),
      ((1, 1, 1), 6),
      ((1, 1, 2), 7),
      ((1, 1, 3), 8),
  ])
  b = tf.sparse.SparseTensor(list(b.keys()), list(b.values()),
                             dense_shape=[2, 2, 4])

  tf.sets.difference(a, b)

  #
  #
"
"tf.sets.intersection(
    a, b, validate_indices=True
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import intersection
  import tensorflow as tf
  import collections

  a = collections.OrderedDict([
      ((0, 0, 0), 1),
      ((0, 0, 1), 2),
      ((0, 1, 0), 3),
      ((1, 0, 0), 4),
      ((1, 1, 0), 5),
      ((1, 1, 1), 6),
  ])
  a = tf.sparse.SparseTensor(list(a.keys()), list(a.values()),
                             dense_shape=[2,2,2])

  b = collections.OrderedDict([
      ((0, 0, 0), 1),
      ((1, 0, 0), 4),
      ((1, 1, 0), 5),
      ((1, 1, 1), 6),
      ((1, 1, 2), 7),
      ((1, 1, 3), 8),
  ])
  b = tf.sparse.SparseTensor(list(b.keys()), list(b.values()),
                             dense_shape=[2, 2, 4])

  tf.sets.intersection(a, b)

  #
  #
"
"tf.sets.union(
    a, b, validate_indices=True
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import union
  import tensorflow as tf
  import collections

  a = collections.OrderedDict([
      ((0, 0, 0), 1),
      ((0, 0, 1), 2),
      ((0, 1, 0), 3),
      ((1, 0, 0), 4),
      ((1, 1, 0), 5),
      ((1, 1, 1), 6),
  ])
  a = tf.sparse.SparseTensor(list(a.keys()), list(a.values()),
                             dense_shape=[2, 2, 2])

  b = collections.OrderedDict([
      ((0, 0, 0), 1),
      ((0, 0, 1), 3),
      ((0, 1, 0), 2),
      ((1, 0, 0), 4),
      ((1, 0, 1), 5),
      ((1, 1, 0), 5),
      ((1, 1, 1), 6),
      ((1, 1, 2), 7),
      ((1, 1, 3), 8),
  ])
  b = tf.sparse.SparseTensor(list(b.keys()), list(b.values()),
                             dense_shape=[2, 2, 4])

  tf.sets.union(a, b)

  #
  #
"
"tf.shape(
    input,
    out_type=tf.dtypes.int32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import shape
tf.shape(1.)
<tf.Tensor: shape=(0,), dtype=int32, numpy=array([], dtype=int32)>
<tf.Tensor: shape=(0,), dtype=int32, numpy=array([], dtype=int32)>"
"tf.math.sigmoid(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sigmoid
x = tf.constant([0.0, 1.0, 50.0, 100.0])
tf.math.sigmoid(x)
<tf.Tensor: shape=(4,), dtype=float32,
<tf.Tensor: shape=(4,), dtype=float32,
numpy=array([0.5, 0.7310586, 1.0, 1.0], dtype=float32)>"
"tf.math.sign(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sign
tf.math.sign([0., 2., -3.])
<tf.Tensor: shape=(3,), dtype=float32,
<tf.Tensor: shape=(3,), dtype=float32,
numpy=array([ 0.,  1., -1.], dtype=float32)>"
"tf.signal.fftshift(
    x, axes=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import fftshift
x = tf.signal.fftshift([ 0.,  1.,  2.,  3.,  4., -5., -4., -3., -2., -1.])
"
"tf.signal.frame(
    signal,
    frame_length,
    frame_step,
    pad_end=False,
    pad_value=0,
    axis=-1,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import frame
audio = tf.random.normal([3, 9152])
frames = tf.signal.frame(audio, 512, 180)
frames.shape.assert_is_compatible_with([3, 49, 512])
frames = tf.signal.frame(audio, 512, 180, pad_end=True)
frames.shape.assert_is_compatible_with([3, 51, 512])"
"tf.signal.ifftshift(
    x, axes=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ifftshift
x = tf.signal.ifftshift([[ 0.,  1.,  2.],[ 3.,  4., -4.],[-3., -2., -1.]])
"
"tf.signal.inverse_mdct(
    mdcts,
    window_fn=tf.signal.vorbis_window,
    norm=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import inverse_mdct
@tf.function
def compare_round_trip():
  samples = 1000
  frame_length = 400
  halflen = frame_length // 2
  waveform = tf.random.normal(dtype=tf.float32, shape=[samples])
  waveform_pad = tf.pad(waveform, [[halflen, 0],])
  mdct = tf.signal.mdct(waveform_pad, frame_length, pad_end=True,
                        window_fn=tf.signal.vorbis_window)
  inverse_mdct = tf.signal.inverse_mdct(mdct,
                                        window_fn=tf.signal.vorbis_window)
  inverse_mdct = inverse_mdct[halflen: halflen + samples]
  return waveform, inverse_mdct
waveform, inverse_mdct = compare_round_trip()
np.allclose(waveform.numpy(), inverse_mdct.numpy(), rtol=1e-3, atol=1e-4)
True"
"tf.signal.inverse_stft(
    stfts,
    frame_length,
    frame_step,
    fft_length=None,
    window_fn=tf.signal.hann_window,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import inverse_stft
frame_length = 400
frame_step = 160
waveform = tf.random.normal(dtype=tf.float32, shape=[1000])
stft = tf.signal.stft(waveform, frame_length, frame_step)
inverse_stft = tf.signal.inverse_stft(
    stft, frame_length, frame_step,
    window_fn=tf.signal.inverse_stft_window_fn(frame_step))
"
"tf.signal.mfccs_from_log_mel_spectrograms(
    log_mel_spectrograms, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mfccs_from_log_mel_spectrograms
batch_size, num_samples, sample_rate = 32, 32000, 16000.0
pcm = tf.random.normal([batch_size, num_samples], dtype=tf.float32)

stfts = tf.signal.stft(pcm, frame_length=1024, frame_step=256,
                       fft_length=1024)
spectrograms = tf.abs(stfts)

num_spectrogram_bins = stfts.shape[-1].value
lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 80
linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(
  num_mel_bins, num_spectrogram_bins, sample_rate, lower_edge_hertz,
  upper_edge_hertz)
mel_spectrograms = tf.tensordot(
  spectrograms, linear_to_mel_weight_matrix, 1)
mel_spectrograms.set_shape(spectrograms.shape[:-1].concatenate(
  linear_to_mel_weight_matrix.shape[-1:]))

log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)

mfccs = tf.signal.mfccs_from_log_mel_spectrograms(
  log_mel_spectrograms)[..., :13]
"
"tf.math.sin(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sin
  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 200, 10, float(""inf"")])
  tf.math.sin(x) ==> [nan -0.4121185 -0.47942555 0.84147096 0.9320391 -0.87329733 -0.54402107 nan]
"
"tf.math.sinh(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sinh
  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 2, 10, float(""inf"")])
  tf.math.sinh(x) ==> [-inf -4.0515420e+03 -5.2109528e-01 1.1752012e+00 1.5094614e+00 3.6268604e+00 1.1013232e+04 inf]
"
"tf.size(
    input,
    out_type=tf.dtypes.int32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import size
t = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])
tf.size(t)
<tf.Tensor: shape=(), dtype=int32, numpy=12>
<tf.Tensor: shape=(), dtype=int32, numpy=12>"
"tf.slice(
    input_, begin, size, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import slice
t = tf.constant([[[1, 1, 1], [2, 2, 2]],
                 [[3, 3, 3], [4, 4, 4]],
                 [[5, 5, 5], [6, 6, 6]]])
"
"tf.sort(
    values, axis=-1, direction='ASCENDING', name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sort
a = [1, 10, 26.9, 2.8, 166.32, 62.3]
tf.sort(a).numpy()
array([  1.  ,   2.8 ,  10.  ,  26.9 ,  62.3 , 166.32], dtype=float32)"
"tf.sparse.bincount(
    values,
    weights=None,
    axis=0,
    minlength=None,
    maxlength=None,
    binary_output=False,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bincount
data = np.array([[10, 20, 30, 20], [11, 101, 11, 10001]], dtype=np.int64)
output = tf.sparse.bincount(data, axis=-1)
print(output)
SparseTensor(indices=tf.Tensor(
[[    0    10]
 [    0    20]
 [    0    30]
 [    1    11]
 [    1   101]
 [    1 10001]], shape=(6, 2), dtype=int64),
 values=tf.Tensor([1 2 1 2 1 1], shape=(6,), dtype=int64),
 dense_shape=tf.Tensor([    2 10002], shape=(2,), dtype=int64))"
"tf.sparse.from_dense(
    tensor, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import from_dense
sp = tf.sparse.from_dense([0, 0, 3, 0, 1])
sp.shape.as_list()
[5]
sp.values.numpy()
array([3, 1], dtype=int32)
sp.indices.numpy()
array([[2],
       [4]])"
"tf.sparse.map_values(
    op, *args, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import map_values
s = tf.sparse.from_dense([[1, 2, 0],
                          [0, 4, 0],
                          [1, 0, 0]])
tf.sparse.to_dense(tf.sparse.map_values(tf.ones_like, s)).numpy()
array([[1, 1, 0],
       [0, 1, 0],
       [1, 0, 0]], dtype=int32)"
"tf.sparse.mask(
    a, mask_indices, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import mask

b = tf.sparse.mask(a, [12, 45])

"
"tf.sparse.maximum(
    sp_a, sp_b, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import maximum
>>> sp_zero = tf.sparse.SparseTensor([[0]], [0], [7])
>>> sp_one = tf.sparse.SparseTensor([[1]], [1], [7])
>>> res = tf.sparse.maximum(sp_zero, sp_one)
>>> res.indices
<tf.Tensor: shape=(2, 1), dtype=int64, numpy=
<tf.Tensor: shape=(2, 1), dtype=int64, numpy=
array([[0],
       [1]])>
>>> res.values
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 1], dtype=int32)>
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 1], dtype=int32)>
>>> res.dense_shape
<tf.Tensor: shape=(1,), dtype=int64, numpy=array([7])>
<tf.Tensor: shape=(1,), dtype=int64, numpy=array([7])>
"
"tf.sparse.minimum(
    sp_a, sp_b, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import minimum
>>> sp_zero = tf.sparse.SparseTensor([[0]], [0], [7])
>>> sp_one = tf.sparse.SparseTensor([[1]], [1], [7])
>>> res = tf.sparse.minimum(sp_zero, sp_one)
>>> res.indices
<tf.Tensor: shape=(2, 1), dtype=int64, numpy=
<tf.Tensor: shape=(2, 1), dtype=int64, numpy=
array([[0],
       [1]])>
>>> res.values
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 0], dtype=int32)>
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 0], dtype=int32)>
>>> res.dense_shape
<tf.Tensor: shape=(1,), dtype=int64, numpy=array([7])>
<tf.Tensor: shape=(1,), dtype=int64, numpy=array([7])>
"
"tf.sparse.reduce_max(
    sp_input, axis=None, keepdims=None, output_is_sparse=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_max
x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 2, 3], [2, 3])
tf.sparse.reduce_max(x)
<tf.Tensor: shape=(), dtype=int32, numpy=3>
<tf.Tensor: shape=(), dtype=int32, numpy=3>
tf.sparse.reduce_max(x, 0)
<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 3, 2], dtype=int32)>
<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 3, 2], dtype=int32)>
tf.sparse.reduce_max(x, 1)
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 3], dtype=int32)>
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 3], dtype=int32)>
tf.sparse.reduce_max(x, 1, keepdims=True)
<tf.Tensor: shape=(2, 1), dtype=int32, numpy=
<tf.Tensor: shape=(2, 1), dtype=int32, numpy=
array([[2],
       [3]], dtype=int32)>
tf.sparse.reduce_max(x, [0, 1])
<tf.Tensor: shape=(), dtype=int32, numpy=3>
<tf.Tensor: shape=(), dtype=int32, numpy=3>"
"tf.sparse.reduce_sum(
    sp_input, axis=None, keepdims=None, output_is_sparse=False, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_sum
x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 1, 1], [2, 3])
tf.sparse.reduce_sum(x)
<tf.Tensor: shape=(), dtype=int32, numpy=3>
<tf.Tensor: shape=(), dtype=int32, numpy=3>
tf.sparse.reduce_sum(x, 0)
<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 1, 1], dtype=int32)>
<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 1, 1], dtype=int32)>
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 1], dtype=int32)>
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 1], dtype=int32)>
tf.sparse.reduce_sum(x, 1, keepdims=True)
<tf.Tensor: shape=(2, 1), dtype=int32, numpy=
<tf.Tensor: shape=(2, 1), dtype=int32, numpy=
array([[2],
       [1]], dtype=int32)>
tf.sparse.reduce_sum(x, [0, 1])
<tf.Tensor: shape=(), dtype=int32, numpy=3>
<tf.Tensor: shape=(), dtype=int32, numpy=3>"
"tf.sparse.segment_sum(
    data, indices, segment_ids, num_segments=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import segment_sum
c = tf.constant([[1,2,3,4], [-1,-2,-3,-4], [5,6,7,8]])

tf.sparse.segment_sum(c, tf.constant([0, 1]), tf.constant([0, 0]))

tf.sparse.segment_sum(c, tf.constant([0, 1]), tf.constant([0, 1]))

tf.sparse.segment_sum(c, tf.constant([0, 1]), tf.constant([0, 2]),
                      num_segments=4)

tf.sparse.segment_sum(c, tf.constant([0, 1, 2]), tf.constant([0, 0, 1]))

tf.math.segment_sum(c, tf.constant([0, 0, 1]))
"
"tf.sparse.softmax(
    sp_input, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import softmax
values = np.asarray([[[0., np.e], [1., 0.]], [[np.e, 0.], [np.e, np.e]]])
indices = np.vstack(np.where(values)).astype(np.int64).T

result = tf.sparse.softmax(tf.sparse.SparseTensor(indices, values, shape))
"
"tf.split(
    value, num_or_size_splits, axis=0, num=None, name='split'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import split
x = tf.Variable(tf.random.uniform([5, 30], -1, 1))
s0, s1, s2 = tf.split(x, num_or_size_splits=3, axis=1)
tf.shape(s0).numpy()
array([ 5, 10], dtype=int32)
split0, split1, split2 = tf.split(x, [4, 15, 11], 1)
tf.shape(split0).numpy()
array([5, 4], dtype=int32)
tf.shape(split1).numpy()
array([ 5, 15], dtype=int32)
tf.shape(split2).numpy()
array([ 5, 11], dtype=int32)"
"tf.math.sqrt(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import sqrt
x = tf.constant([[4.0], [16.0]])
tf.sqrt(x)
<tf.Tensor: shape=(2, 1), dtype=float32, numpy=
<tf.Tensor: shape=(2, 1), dtype=float32, numpy=
  array([[2.],
         [4.]], dtype=float32)>
y = tf.constant([[-4.0], [16.0]])
tf.sqrt(y)
<tf.Tensor: shape=(2, 1), dtype=float32, numpy=
<tf.Tensor: shape=(2, 1), dtype=float32, numpy=
  array([[nan],
         [ 4.]], dtype=float32)>
z = tf.constant([[-1.0], [16.0]], dtype=tf.complex128)
tf.sqrt(z)
<tf.Tensor: shape=(2, 1), dtype=complex128, numpy=
<tf.Tensor: shape=(2, 1), dtype=complex128, numpy=
  array([[0.0+1.j],
         [4.0+0.j]])>"
"tf.math.square(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import square
tf.math.square([-2., 0., 3.])
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([4., 0., 9.], dtype=float32)>
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([4., 0., 9.], dtype=float32)>"
"tf.squeeze(
    input, axis=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import squeeze
"
"tf.stack(
    values, axis=0, name='stack'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import stack
x = tf.constant([1, 4])
y = tf.constant([2, 5])
z = tf.constant([3, 6])
tf.stack([x, y, z])
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
array([[1, 4],
       [2, 5],
       [3, 6]], dtype=int32)>
tf.stack([x, y, z], axis=1)
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
array([[1, 2, 3],
       [4, 5, 6]], dtype=int32)>"
"tf.strided_slice(
    input_,
    begin,
    end,
    strides=None,
    begin_mask=0,
    end_mask=0,
    ellipsis_mask=0,
    new_axis_mask=0,
    shrink_axis_mask=0,
    var=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import strided_slice
t = tf.constant([[[1, 1, 1], [2, 2, 2]],
                 [[3, 3, 3], [4, 4, 4]],
                 [[5, 5, 5], [6, 6, 6]]])
"
"tf.strings.as_string(
    input,
    precision=-1,
    scientific=False,
    shortest=False,
    width=-1,
    fill='',
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import as_string
tf.strings.as_string([3, 2])
<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'3', b'2'], dtype=object)>
<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'3', b'2'], dtype=object)>
tf.strings.as_string([3.1415926, 2.71828], precision=2).numpy()
array([b'3.14', b'2.72'], dtype=object)"
"tf.strings.bytes_split(
    input, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import bytes_split
tf.strings.bytes_split('hello').numpy()
array([b'h', b'e', b'l', b'l', b'o'], dtype=object)
tf.strings.bytes_split(['hello', '123'])
<tf.RaggedTensor [[b'h', b'e', b'l', b'l', b'o'], [b'1', b'2', b'3']]>
<tf.RaggedTensor [[b'h', b'e', b'l', b'l', b'o'], [b'1', b'2', b'3']]>"
"tf.strings.format(
    template, inputs, placeholder='{}', summarize=3, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import format
tensor = tf.range(5)
tf.strings.format(""tensor: {}, suffix"", tensor)
<tf.Tensor: shape=(), dtype=string, numpy=b'tensor: [0 1 2 3 4], suffix'>
<tf.Tensor: shape=(), dtype=string, numpy=b'tensor: [0 1 2 3 4], suffix'>"
"tf.strings.join(
    inputs, separator='', name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import join
tf.strings.join(['abc','def']).numpy()
b'abcdef'
tf.strings.join([['abc','123'],
                 ['def','456'],
                 ['ghi','789']]).numpy()
array([b'abcdefghi', b'123456789'], dtype=object)
tf.strings.join([['abc','123'],
                 ['def','456']],
                 separator="" "").numpy()
array([b'abc def', b'123 456'], dtype=object)"
"tf.strings.length(
    input, unit='BYTE', name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import length
strings = tf.constant(['Hello','TensorFlow', '\U0001F642'])
array([ 5, 10, 4], dtype=int32)
tf.strings.length(strings, unit=""UTF8_CHAR"").numpy()
array([ 5, 10, 1], dtype=int32)"
"tf.strings.lower(
    input, encoding='', name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import lower
tf.strings.lower(""CamelCase string and ALL CAPS"")
<tf.Tensor: shape=(), dtype=string, numpy=b'camelcase string and all caps'>
<tf.Tensor: shape=(), dtype=string, numpy=b'camelcase string and all caps'>"
"tf.strings.ngrams(
    data,
    ngram_width,
    separator=' ',
    pad_values=None,
    padding_width=None,
    preserve_short_sequences=False,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ngrams
tf.strings.ngrams([""A"", ""B"", ""C"", ""D""], 2).numpy()
array([b'A B', b'B C', b'C D'], dtype=object)
tf.strings.ngrams([""TF"", ""and"", ""keras""], 1).numpy()
array([b'TF', b'and', b'keras'], dtype=object)"
"tf.strings.reduce_join(
    inputs, axis=None, keepdims=False, separator='', name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import reduce_join
tf.strings.reduce_join([['abc','123'],
                        ['def','456']]).numpy()
b'abc123def456'
tf.strings.reduce_join([['abc','123'],
                        ['def','456']], axis=-1).numpy()
array([b'abc123', b'def456'], dtype=object)
tf.strings.reduce_join([['abc','123'],
                        ['def','456']],
                       axis=-1,
                       separator="" "").numpy()
array([b'abc 123', b'def 456'], dtype=object)"
"tf.strings.regex_full_match(
    input, pattern, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import regex_full_match
tf.strings.regex_full_match([""TF lib"", ""lib TF""], "".*lib$"")
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>
tf.strings.regex_full_match([""TF lib"", ""lib TF""], "".*TF$"")
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>"
"tf.strings.regex_replace(
    input, pattern, rewrite, replace_global=True, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import regex_replace
tf.strings.regex_replace(""Text with tags.<br /><b>contains html</b>"",
tf.strings.regex_replace(""Text with tags.<br /><b>contains html</b>"",
                         ""<[^>]+>"", "" "")
                         ""<[^>]+>"", "" "")
<tf.Tensor: shape=(), dtype=string, numpy=b'Text with tags.  contains html '>
<tf.Tensor: shape=(), dtype=string, numpy=b'Text with tags.  contains html '>"
"tf.strings.split(
    input, sep=None, maxsplit=-1, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import split
tf.strings.split('hello world').numpy()
 array([b'hello', b'world'], dtype=object)
tf.strings.split(['hello world', 'a b c'])
<tf.RaggedTensor [[b'hello', b'world'], [b'a', b'b', b'c']]>
<tf.RaggedTensor [[b'hello', b'world'], [b'a', b'b', b'c']]>"
"tf.strings.strip(
    input, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import strip
tf.strings.strip([""\nTensorFlow"", ""     The python library    ""]).numpy()
array([b'TensorFlow', b'The python library'], dtype=object)"
"tf.strings.to_hash_bucket(
    input, num_buckets, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import to_hash_bucket
tf.strings.to_hash_bucket([""Hello"", ""TensorFlow"", ""2.x""], 3)
<tf.Tensor: shape=(3,), dtype=int64, numpy=array([2, 0, 1])>
<tf.Tensor: shape=(3,), dtype=int64, numpy=array([2, 0, 1])>"
"tf.strings.to_hash_bucket_fast(
    input, num_buckets, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import to_hash_bucket_fast
tf.strings.to_hash_bucket_fast([""Hello"", ""TensorFlow"", ""2.x""], 3).numpy()
array([0, 2, 2])"
"tf.strings.to_hash_bucket_strong(
    input, num_buckets, key, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import to_hash_bucket_strong
tf.strings.to_hash_bucket_strong([""Hello"", ""TF""], 3, [1, 2]).numpy()
array([2, 0])"
"tf.strings.to_number(
    input,
    out_type=tf.dtypes.float32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import to_number
tf.strings.to_number(""1.55"")
<tf.Tensor: shape=(), dtype=float32, numpy=1.55>
<tf.Tensor: shape=(), dtype=float32, numpy=1.55>
tf.strings.to_number(""3"", tf.int32)
<tf.Tensor: shape=(), dtype=int32, numpy=3>
<tf.Tensor: shape=(), dtype=int32, numpy=3>"
"tf.strings.unicode_decode(
    input,
    input_encoding,
    errors='replace',
    replacement_char=65533,
    replace_control_characters=False,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import unicode_decode
input = [s.encode('utf8') for s in (u'G\xf6\xf6dnight', u'\U0001f60a')]
tf.strings.unicode_decode(input, 'UTF-8').to_list()
[[71, 246, 246, 100, 110, 105, 103, 104, 116], [128522]]"
"tf.strings.unicode_decode_with_offsets(
    input,
    input_encoding,
    errors='replace',
    replacement_char=65533,
    replace_control_characters=False,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import unicode_decode_with_offsets
input = [s.encode('utf8') for s in (u'G\xf6\xf6dnight', u'\U0001f60a')]
result = tf.strings.unicode_decode_with_offsets(input, 'UTF-8')
[[71, 246, 246, 100, 110, 105, 103, 104, 116], [128522]]
[[0, 1, 3, 5, 6, 7, 8, 9, 10], [0]]"
"tf.strings.unicode_script(
    input, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import unicode_script
tf.strings.unicode_script([1, 31, 38])
<tf.Tensor: shape=(3,), dtype=int32, numpy=array([0, 0, 0], dtype=int32)>
<tf.Tensor: shape=(3,), dtype=int32, numpy=array([0, 0, 0], dtype=int32)>"
"tf.strings.unicode_split(
    input,
    input_encoding,
    errors='replace',
    replacement_char=65533,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import unicode_split
input = [s.encode('utf8') for s in (u'G\xf6\xf6dnight', u'\U0001f60a')]
tf.strings.unicode_split(input, 'UTF-8').to_list()
[[b'G', b'\xc3\xb6', b'\xc3\xb6', b'd', b'n', b'i', b'g', b'h', b't'],
 [b'\xf0\x9f\x98\x8a']]"
"tf.strings.unicode_split_with_offsets(
    input,
    input_encoding,
    errors='replace',
    replacement_char=65533,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import unicode_split_with_offsets
input = [s.encode('utf8') for s in (u'G\xf6\xf6dnight', u'\U0001f60a')]
result = tf.strings.unicode_split_with_offsets(input, 'UTF-8')
[[b'G', b'\xc3\xb6', b'\xc3\xb6', b'd', b'n', b'i', b'g', b'h', b't'],
 [b'\xf0\x9f\x98\x8a']]
[[0, 1, 3, 5, 6, 7, 8, 9, 10], [0]]"
"tf.strings.unicode_transcode(
    input,
    input_encoding,
    output_encoding,
    errors='replace',
    replacement_char=65533,
    replace_control_characters=False,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import unicode_transcode
tf.strings.unicode_transcode([""Hello"", ""TensorFlow"", ""2.x""], ""UTF-8"", ""UTF-16-BE"")
<tf.Tensor: shape=(3,), dtype=string, numpy=
<tf.Tensor: shape=(3,), dtype=string, numpy=
array([b'\x00H\x00e\x00l\x00l\x00o',
       b'\x00T\x00e\x00n\x00s\x00o\x00r\x00F\x00l\x00o\x00w',
       b'\x002\x00.\x00x'], dtype=object)>
tf.strings.unicode_transcode([""A"", ""B"", ""C""], ""US ASCII"", ""UTF-8"").numpy()
array([b'A', b'B', b'C'], dtype=object)"
"tf.strings.upper(
    input, encoding='', name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import upper
tf.strings.upper(""CamelCase string and ALL CAPS"")
<tf.Tensor: shape=(), dtype=string, numpy=b'CAMELCASE STRING AND ALL CAPS'>
<tf.Tensor: shape=(), dtype=string, numpy=b'CAMELCASE STRING AND ALL CAPS'>"
"tf.math.subtract(
    x, y, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import subtract
x = [1, 2, 3, 4, 5]
y = 1
tf.subtract(x, y)
<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 1, 2, 3, 4], dtype=int32)>
<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 1, 2, 3, 4], dtype=int32)>
tf.subtract(y, x)
<tf.Tensor: shape=(5,), dtype=int32,
<tf.Tensor: shape=(5,), dtype=int32,
numpy=array([ 0, -1, -2, -3, -4], dtype=int32)>"
"tf.summary.graph(
    graph_data
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import graph
writer = tf.summary.create_file_writer(""/tmp/mylogs"")

@tf.function
def f():
  x = constant_op.constant(2)
  y = constant_op.constant(3)
  return x**y

with writer.as_default():
  tf.summary.graph(f.get_concrete_function().graph)

graph = tf.Graph()
with graph.as_default():
  c = tf.constant(30.0)
with writer.as_default():
  tf.summary.graph(graph)
"
"tf.summary.histogram(
    name, data, step=None, buckets=None, description=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import histogram
w = tf.summary.create_file_writer('test/logs')
with w.as_default():
    tf.summary.histogram(""activations"", tf.random.uniform([100, 50]), step=0)
    tf.summary.histogram(""initial_weights"", tf.random.normal([1000]), step=0)
"
"tf.summary.image(
    name, data, step=None, max_outputs=3, description=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import image
w = tf.summary.create_file_writer('test/logs')
with w.as_default():
  image1 = tf.random.uniform(shape=[8, 8, 1])
  image2 = tf.random.uniform(shape=[8, 8, 1])
  tf.summary.image(""grayscale_noise"", [image1, image2], step=0)
"
"tf.summary.scalar(
    name, data, step=None, description=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import scalar
test_summary_writer = tf.summary.create_file_writer('test/logdir')
with test_summary_writer.as_default():
    tf.summary.scalar('loss', 0.345, step=1)
    tf.summary.scalar('loss', 0.234, step=2)
    tf.summary.scalar('loss', 0.123, step=3)
"
"tf.summary.text(
    name, data, step=None, description=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import text
test_summary_writer = tf.summary.create_file_writer('test/logdir')
with test_summary_writer.as_default():
    tf.summary.text('first_text', 'hello world!', step=0)
    tf.summary.text('first_text', 'nice to meet you!', step=1)
"
"tf.math.tan(
    x, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import tan
  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
  tf.math.tan(x) ==> [nan 0.45231566 -0.5463025 1.5574077 2.572152 -1.7925274 0.32097113 nan]
"
"tf.tensor_scatter_nd_max(
    tensor, indices, updates, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import tensor_scatter_nd_max
tensor = [0, 0, 0, 0, 0, 0, 0, 0]
indices = [[1], [4], [5]]
updates = [1, -1, 1]
tf.tensor_scatter_nd_max(tensor, indices, updates).numpy()
array([0, 1, 0, 0, 0, 1, 0, 0], dtype=int32)"
"tf.test.compute_gradient(
    f, x, delta=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import compute_gradient
@tf.function
def test_func(x):
  return x*x
class MyTest(tf.test.TestCase):
  def test_gradient_of_test_func(self):
    theoretical, numerical = tf.test.compute_gradient(test_func, [1.0])
    self.assertAllClose(theoretical, numerical)"
"tf.test.create_local_cluster(
    num_workers,
    num_ps,
    protocol='grpc',
    worker_config=None,
    ps_config=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import create_local_cluster
workers, _ = tf.test.create_local_cluster(num_workers=2, num_ps=2)

worker_sessions = [tf.compat.v1.Session(w.target) for w in workers]

with tf.device(""/job:ps/task:0""):
  ...
with tf.device(""/job:ps/task:1""):
  ...
with tf.device(""/job:worker/task:0""):
  ...
with tf.device(""/job:worker/task:1""):
  ...

worker_sessions[0].run(...)
"
"tf.test.is_gpu_available(
    cuda_only=False, min_cuda_compute_capability=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import is_gpu_available
>>> gpu_available = tf.test.is_gpu_available()
>>> is_cuda_gpu_available = tf.test.is_gpu_available(cuda_only=True)
>>> is_cuda_gpu_min_3 = tf.test.is_gpu_available(True, (3,0))
"
"tf.tile(
    input, multiples, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import tile
a = tf.constant([[1,2,3],[4,5,6]], tf.int32)
b = tf.constant([1,2], tf.int32)
tf.tile(a, b)
<tf.Tensor: shape=(2, 6), dtype=int32, numpy=
<tf.Tensor: shape=(2, 6), dtype=int32, numpy=
array([[1, 2, 3, 1, 2, 3],
       [4, 5, 6, 4, 5, 6]], dtype=int32)>
c = tf.constant([2,1], tf.int32)
tf.tile(a, c)
<tf.Tensor: shape=(4, 3), dtype=int32, numpy=
<tf.Tensor: shape=(4, 3), dtype=int32, numpy=
array([[1, 2, 3],
       [4, 5, 6],
       [1, 2, 3],
       [4, 5, 6]], dtype=int32)>
d = tf.constant([2,2], tf.int32)
tf.tile(a, d)
<tf.Tensor: shape=(4, 6), dtype=int32, numpy=
<tf.Tensor: shape=(4, 6), dtype=int32, numpy=
array([[1, 2, 3, 1, 2, 3],
       [4, 5, 6, 4, 5, 6],
       [1, 2, 3, 1, 2, 3],
       [4, 5, 6, 4, 5, 6]], dtype=int32)>"
"tf.tpu.experimental.embedding.Adagrad(
    learning_rate: Union[float, Callable[[], float]] = 0.001,
    initial_accumulator_value: float = 0.1,
    use_gradient_accumulation: bool = True,
    clip_weight_min: Optional[float] = None,
    clip_weight_max: Optional[float] = None,
    weight_decay_factor: Optional[float] = None,
    multiply_weight_decay_factor_by_learning_rate: bool = None,
    slot_variable_creation_fn: Optional[SlotVarCreationFnType] = None,
    clipvalue: Optional[ClipValueType] = None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Adagrad
embedding = tf.tpu.experimental.embedding.TPUEmbedding(
    ...
    optimizer=tf.tpu.experimental.embedding.Adagrad(0.1))
"
"tf.tpu.experimental.embedding.AdagradMomentum(
    learning_rate: Union[float, Callable[[], float]] = 0.001,
    momentum: float = 0.0,
    use_nesterov: bool = False,
    exponent: float = 2,
    beta2: float = 1,
    epsilon: float = 1e-10,
    use_gradient_accumulation: bool = True,
    clip_weight_min: Optional[float] = None,
    clip_weight_max: Optional[float] = None,
    weight_decay_factor: Optional[float] = None,
    multiply_weight_decay_factor_by_learning_rate: bool = None,
    slot_variable_creation_fn: Optional[SlotVarCreationFnType] = None,
    clipvalue: Optional[ClipValueType] = None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import AdagradMomentum
embedding = tf.tpu.experimental.embedding.TPUEmbedding(
    ...
    optimizer=tf.tpu.experimental.embedding.AdagradMomentum(0.1))
"
"tf.tpu.experimental.embedding.Adam(
    learning_rate: Union[float, Callable[[], float]] = 0.001,
    beta_1: float = 0.9,
    beta_2: float = 0.999,
    epsilon: float = 1e-07,
    lazy_adam: bool = True,
    sum_inside_sqrt: bool = True,
    use_gradient_accumulation: bool = True,
    clip_weight_min: Optional[float] = None,
    clip_weight_max: Optional[float] = None,
    weight_decay_factor: Optional[float] = None,
    multiply_weight_decay_factor_by_learning_rate: bool = None,
    slot_variable_creation_fn: Optional[SlotVarCreationFnType] = None,
    clipvalue: Optional[ClipValueType] = None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Adam
embedding = tf.tpu.experimental.embedding.TPUEmbedding(
    ...
    optimizer=tf.tpu.experimental.embedding.Adam(0.1))
"
"tf.tpu.experimental.embedding.FTRL(
    learning_rate: Union[float, Callable[[], float]] = 0.001,
    learning_rate_power: float = -0.5,
    l1_regularization_strength: float = 0.0,
    l2_regularization_strength: float = 0.0,
    beta: float = 0.0,
    initial_accumulator_value: float = 0.1,
    use_gradient_accumulation: bool = True,
    clip_weight_min: Optional[float] = None,
    clip_weight_max: Optional[float] = None,
    weight_decay_factor: Optional[float] = None,
    multiply_weight_decay_factor_by_learning_rate: bool = None,
    slot_variable_creation_fn: Optional[SlotVarCreationFnType] = None,
    clipvalue: Optional[ClipValueType] = None,
    multiply_linear_by_learning_rate: bool = False,
    allow_zero_accumulator: bool = False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import FTRL
embedding = tf.tpu.experimental.embedding.TPUEmbedding(
    ...
    optimizer=tf.tpu.experimental.embedding.FTRL(0.1))
"
"tf.tpu.experimental.embedding.FeatureConfig(
    table: tf.tpu.experimental.embedding.TableConfig,
    max_sequence_length: int = 0,
    validate_weights_and_indices: bool = True,
    output_shape: Optional[Union[List[int], tf.TensorShape]] = None,
    name: Optional[Text] = None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import FeatureConfig
table_config_one = tf.tpu.experimental.embedding.TableConfig(
    vocabulary_size=...,
    dim=...)
table_config_two = tf.tpu.experimental.embedding.TableConfig(
    vocabulary_size=...,
    dim=...)
feature_config = {
    'feature_one': tf.tpu.experimental.embedding.FeatureConfig(
        table=table_config_one),
    'feature_two': tf.tpu.experimental.embedding.FeatureConfig(
        table=table_config_one),
    'feature_three': tf.tpu.experimental.embedding.FeatureConfig(
        table=table_config_two)}
embedding = tf.tpu.experimental.embedding.TPUEmbedding(
    feature_config=feature_config,
    batch_size=...
    optimizer=tf.tpu.experimental.embedding.Adam(0.1))
"
"tf.tpu.experimental.embedding.SGD(
    learning_rate: Union[float, Callable[[], float]] = 0.01,
    use_gradient_accumulation: bool = True,
    clip_weight_min: Optional[float] = None,
    clip_weight_max: Optional[float] = None,
    weight_decay_factor: Optional[float] = None,
    multiply_weight_decay_factor_by_learning_rate: bool = None,
    clipvalue: Optional[ClipValueType] = None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import SGD
embedding = tf.tpu.experimental.embedding.TPUEmbedding(
    ...
    optimizer=tf.tpu.experimental.embedding.SGD(0.1))
"
"tf.tpu.experimental.embedding.TPUEmbeddingV0(
    feature_config: Union[tf.tpu.experimental.embedding.FeatureConfig, Iterable],
    optimizer: Optional[tpu_embedding_v2_utils._Optimizer]
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TPUEmbeddingV0
strategy = tf.distribute.TPUStrategy(...)
with strategy.scope():
  embedding = tf.tpu.experimental.embedding.TPUEmbeddingV0(
      feature_config=feature_config,
      optimizer=tf.tpu.experimental.embedding.SGD(0.1))
"
"tf.tpu.experimental.embedding.TableConfig(
    vocabulary_size: int,
    dim: int,
    initializer: Optional[Callable[[Any], None]] = None,
    optimizer: Optional[_Optimizer] = None,
    combiner: Text = 'mean',
    name: Optional[Text] = None,
    quantization_config: tf.tpu.experimental.embedding.QuantizationConfig = None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import TableConfig
table_config_one = tf.tpu.experimental.embedding.TableConfig(
    vocabulary_size=...,
    dim=...)
table_config_two = tf.tpu.experimental.embedding.TableConfig(
    vocabulary_size=...,
    dim=...)
feature_config = {
    'feature_one': tf.tpu.experimental.embedding.FeatureConfig(
        table=table_config_one),
    'feature_two': tf.tpu.experimental.embedding.FeatureConfig(
        table=table_config_one),
    'feature_three': tf.tpu.experimental.embedding.FeatureConfig(
        table=table_config_two)}
embedding = tf.tpu.experimental.embedding.TPUEmbedding(
    feature_config=feature_config,
    batch_size=...
    optimizer=tf.tpu.experimental.embedding.Adam(0.1))
"
"tf.tpu.experimental.embedding.serving_embedding_lookup(
    inputs: Any,
    weights: Optional[Any],
    tables: Dict[tf.tpu.experimental.embedding.TableConfig, tf.Variable],
    feature_config: Union[tf.tpu.experimental.embedding.FeatureConfig, Iterable]
) -> Any
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import serving_embedding_lookup
model = model_fn(...)
embedding = tf.tpu.experimental.embedding.TPUEmbedding(
    feature_config=feature_config,
    batch_size=1024,
    optimizer=tf.tpu.experimental.embedding.SGD(0.1))
checkpoint = tf.train.Checkpoint(model=model, embedding=embedding)
checkpoint.restore(...)

@tf.function(input_signature=[{'feature_one': tf.TensorSpec(...),
                               'feature_two': tf.TensorSpec(...),
                               'feature_three': tf.TensorSpec(...)}])
def serve_tensors(embedding_features):
  embedded_features = tf.tpu.experimental.embedding.serving_embedding_lookup(
      embedding_features, None, embedding.embedding_tables,
      feature_config)
  return model(embedded_features)

model.embedding_api = embedding
tf.saved_model.save(model,
                    export_dir=...,
                    signatures={'serving_default': serve_tensors})

"
"tf.train.Checkpoint(
    root=None, **kwargs
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Checkpoint
model = tf.keras.Model(...)
checkpoint = tf.train.Checkpoint(model)

save_path = checkpoint.save('/tmp/training_checkpoints')

checkpoint.restore(save_path)
"
"tf.train.CheckpointManager(
    checkpoint,
    directory,
    max_to_keep,
    keep_checkpoint_every_n_hours=None,
    checkpoint_name='ckpt',
    step_counter=None,
    checkpoint_interval=None,
    init_fn=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CheckpointManager
import tensorflow as tf
checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)
manager = tf.train.CheckpointManager(
    checkpoint, directory=""/tmp/model"", max_to_keep=5)
status = checkpoint.restore(manager.latest_checkpoint)
while True:
  manager.save()
"
"tf.train.CheckpointOptions(
    experimental_io_device=None, experimental_enable_async_checkpoint=False
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import CheckpointOptions
step = tf.Variable(0, name=""step"")
checkpoint = tf.train.Checkpoint(step=step)
options = tf.train.CheckpointOptions(experimental_io_device=""/job:localhost"")
checkpoint.save(""/tmp/ckpt"", options=options)
"
"tf.train.ClusterSpec(
    cluster
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ClusterSpec
cluster = tf.train.ClusterSpec({""worker"": [""worker0.example.com:2222"",
                                           ""worker1.example.com:2222"",
                                           ""worker2.example.com:2222""],
                                ""ps"": [""ps0.example.com:2222"",
                                       ""ps1.example.com:2222""]})
"
"tf.train.Coordinator(
    clean_stop_exception_types=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import Coordinator
coord = Coordinator()
...start thread 1...(coord, ...)
...start thread N...(coord, ...)
coord.join(threads)
"
"tf.train.ExponentialMovingAverage(
    decay,
    num_updates=None,
    zero_debias=False,
    name='ExponentialMovingAverage'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import ExponentialMovingAverage
var0 = tf.Variable(...)
var1 = tf.Variable(...)

ema = tf.train.ExponentialMovingAverage(decay=0.9999)

ema.apply([var0, var1])

averages = [ema.average(var0), ema.average(var1)]

...
def train_step(...):
...
  opt.minimize(my_loss, [var0, var1])

  ema.apply([var0, var1])

...train the model by running train_step multiple times...
"
"tf.train.list_variables(
    ckpt_dir_or_file
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import list_variables
</td>
</td>
</tr>
</tr>

</table>
</table>


import tensorflow as tf
import os
ckpt_directory = ""/tmp/training_checkpoints/ckpt""
ckpt = tf.train.Checkpoint(optimizer=optimizer, model=model)
manager = tf.train.CheckpointManager(ckpt, ckpt_directory, max_to_keep=3)
train_and_checkpoint(model, manager)
tf.train.list_variables(manager.latest_checkpoint)
"
"tf.transpose(
    a, perm=None, conjugate=False, name='transpose'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import transpose
x = tf.constant([[1, 2, 3], [4, 5, 6]])
tf.transpose(x)
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
array([[1, 4],
       [2, 5],
       [3, 6]], dtype=int32)>"
"tf.type_spec_from_value(
    value
) -> tf.TypeSpec
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import type_spec_from_value
>>> tf.type_spec_from_value(tf.constant([1, 2, 3]))
TensorSpec(shape=(3,), dtype=tf.int32, name=None)
>>> tf.type_spec_from_value(np.array([4.0, 5.0], np.float64))
TensorSpec(shape=(2,), dtype=tf.float64, name=None)
>>> tf.type_spec_from_value(tf.ragged.constant([[1, 2], [3, 4, 5]]))
RaggedTensorSpec(TensorShape([2, None]), tf.int32, 1, tf.int64)
"
"tf.unique(
    x,
    out_idx=tf.dtypes.int32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import unique
y, idx = unique(x)
y ==> [1, 2, 4, 7, 8]
idx ==> [0, 0, 1, 2, 2, 2, 3, 4, 4]
"
"tf.unique_with_counts(
    x,
    out_idx=tf.dtypes.int32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import unique_with_counts
y, idx, count = unique_with_counts(x)
y ==> [1, 2, 4, 7, 8]
idx ==> [0, 0, 1, 2, 2, 2, 3, 4, 4]
count ==> [2, 1, 3, 1, 2]
"
"tf.unravel_index(
    indices, dims, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import unravel_index
y = tf.unravel_index(indices=[2, 5, 7], dims=[3, 3])
y ==> [[0, 1, 2], [2, 2, 1]]
"
"tf.unstack(
    value, num=None, axis=0, name='unstack'
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import unstack
x = tf.reshape(tf.range(12), (3,4))
p, q, r = tf.unstack(x)
p.shape.as_list()
[4]"
"tf.vectorized_map(
    fn, elems, fallback_to_while_loop=True, warn=True
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import vectorized_map
def outer_product(a):
  return tf.tensordot(a, a, 0)

batch_size = 100
a = tf.ones((batch_size, 32, 32))
c = tf.vectorized_map(outer_product, a)
assert c.shape == (batch_size, 32, 32, 32, 32)
"
"tf.where(
    condition, x=None, y=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import where
tf.where([True, False, False, True]).numpy()
array([[0],
       [3]])"
"tf.while_loop(
    cond,
    body,
    loop_vars,
    shape_invariants=None,
    parallel_iterations=10,
    back_prop=True,
    swap_memory=False,
    maximum_iterations=None,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import while_loop
i = tf.constant(0)
c = lambda i: tf.less(i, 10)
b = lambda i: (tf.add(i, 1), )
r = tf.while_loop(c, b, [i])
"
"tf.zeros(
    shape,
    dtype=tf.dtypes.float32,
    name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import zeros
tf.zeros([3, 4], tf.int32)
<tf.Tensor: shape=(3, 4), dtype=int32, numpy=
<tf.Tensor: shape=(3, 4), dtype=int32, numpy=
array([[0, 0, 0, 0],
       [0, 0, 0, 0],
       [0, 0, 0, 0]], dtype=int32)>"
"tf.zeros_like(
    input, dtype=None, name=None
)
","import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import zeros_like
>>> tensor = tf.constant([[1, 2, 3], [4, 5, 6]])
>>> tf.zeros_like(tensor)
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
array([[0, 0, 0],
       [0, 0, 0]], dtype=int32)>
"
