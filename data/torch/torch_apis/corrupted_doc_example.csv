"
 torch. sort ( input ,  dim ,  descending ,  stable ,  * ,  out ) ¶","  File ""example.py"", line 6
    [ 0,  3,  1,  2]])sorted,indices=torch.sort(x,0)
    ^
IndentationError: unexpected indent
"
"
 torch. log10 ( input ,  * ,  out )  →  Tensor ¶","  File ""example.py"", line 4
    tensor([ 0.5224,  0.9354,  0.7257,  0.1301,  0.2251])torch.log10(a)
                                                         ^
SyntaxError: invalid syntax
"
"
 torch. tensor_split ( input ,  indices_or_sections ,  dim )  →  List   of   Tensors ¶","  File ""example.py"", line 4
    (tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7]))x=torch.arange(7)
                                                          ^
SyntaxError: invalid syntax
"
"
 torch. lu_solve ( b ,  LU_data ,  LU_pivots ,  * ,  out )  →  Tensor ¶","/home/nimashiri/.local/lib/python3.8/site-packages/torch/functional.py:1690: UserWarning: torch.lu is deprecated in favor of torch.linalg.lu_factor / torch.linalg.lu_factor_ex and will be removed in a future PyTorch release.
LU, pivots = torch.lu(A, compute_pivots)
should be replaced with
LU, pivots = torch.linalg.lu_factor(A, compute_pivots)
and
LU, pivots, info = torch.lu(A, compute_pivots, get_infos=True)
should be replaced with
LU, pivots, info = torch.linalg.lu_factor_ex(A, compute_pivots) (Triggered internally at /media/nimashiri/SSD1/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2018.)
  return torch._lu_with_info(A, pivot=pivot, check_errors=(not get_infos))
/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py:99: UserWarning: torch.lu_solve is deprecated in favor of torch.linalg.lu_solveand will be removed in a future PyTorch release.
Note that torch.linalg.lu_solve has its arguments reversed.
X = torch.lu_solve(B, LU, pivots)
should be replaced with
X = torch.linalg.lu_solve(LU, pivots, B) (Triggered internally at /media/nimashiri/SSD1/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2175.)
  outputs = func(*args, **kwargs)
"
"
 Tensor. scatter_reduce_ ( dim ,  index ,  src ,  reduce ,  * ,  include_self )  →  Tensor ¶","/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py:99: UserWarning: scatter_reduce() is in beta and the API may change at any time. (Triggered internally at /media/nimashiri/SSD1/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1685.)
  outputs = func(*args, **kwargs)
"
"
 torch. block_diag ( * ) [source] ¶","Traceback (most recent call last):
  File ""example.py"", line 2, in <module>
    importtorch
NameError: name 'importtorch' is not defined
"
"
 torch.linalg. matrix_exp ( A )  →  Tensor ¶","  File ""example.py"", line 7
    [0.0000, 7.3891]]])importmath
    ^
IndentationError: unexpected indent
"
"
 torch. norm ( input ,  p ,  dim ,  keepdim ,  out ,  dtype ) [source] ¶","Traceback (most recent call last):
  File ""example.py"", line 2, in <module>
    importtorch
NameError: name 'importtorch' is not defined
"
"
 torch. tril ( input ,  diagonal ,  * ,  out )  →  Tensor ¶","  File ""example.py"", line 5
    [-0.3409, -0.9828,  0.0289]])b=torch.randn(4,6)
    ^
IndentationError: unexpected indent
"
"
 torch. flatten ( input ,  start_dim ,  end_dim )  →  Tensor ¶","  File ""example.py"", line 4
    torch.flatten(t,start_dim=1)
    ^
SyntaxError: invalid syntax
"
"
 torch. tensor ( data ,  * ,  dtype ,  device ,  requires_grad ,  pin_memory )  →  Tensor ¶","  File ""example.py"", line 3
    [ 4.9000,  5.2000]])torch.tensor([0,1])# Type inference on data
    ^
IndentationError: unexpected indent
"
"
 torch. set_default_tensor_type ( t ) [source] ¶","Traceback (most recent call last):
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 13, in json_serialize
    json.dumps(v)
  File ""/usr/lib/python3.8/json/__init__.py"", line 231, in dumps
    return _default_encoder.encode(obj)
  File ""/usr/lib/python3.8/json/encoder.py"", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/usr/lib/python3.8/json/encoder.py"", line 257, in iterencode
    return _iterencode(o, 0)
  File ""/usr/lib/python3.8/json/encoder.py"", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type tensortype is not JSON serializable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 56, in get_var_shape
    return list(s)  # convert torch.Size to list
TypeError: 'getset_descriptor' object is not iterable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""example.py"", line 3, in <module>
    torch.set_default_tensor_type(torch.DoubleTensor)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 101, in wrapper
    param_dict = build_param_dict(*args, **kwargs)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 42, in build_param_dict
    param_dict['parameter:%d' % ind] = json_serialize(arg)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 17, in json_serialize
    return get_var_signature(
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 88, in get_var_signature
    s['shape'] = get_var_shape(var)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 58, in get_var_shape
    print(e.message)
AttributeError: 'TypeError' object has no attribute 'message'
"
"
 torch. triu ( input ,  diagonal ,  * ,  out )  →  Tensor ¶","  File ""example.py"", line 7
    [ 0.0000, -0.5211, -0.4573]])b=torch.randn(4,6)
    ^
IndentationError: unexpected indent
"
"
 torch. zeros ( *size ,  * ,  out=None ,  dtype=None ,  layout=torch.strided ,  device=None ,  requires_grad=False )  →  Tensor ¶","  File ""example.py"", line 3
    [ 0.,  0.,  0.]])torch.zeros(5)
    ^
IndentationError: unexpected indent
"
"
 torch. clamp ( input ,  min ,  max ,  * ,  out )  →  Tensor ¶","  File ""example.py"", line 5
    tensor([-0.5000,  0.1734, -0.0478, -0.0922])min=torch.linspace(-1,1,steps=4)
                                                ^
SyntaxError: invalid syntax
"
"
 torch. pow ( input ,  exponent ,  * ,  out )  →  Tensor ¶","  File ""example.py"", line 5
    exp=torch.arange(1.,5.)a=torch.arange(1.,5.)
                           ^
SyntaxError: invalid syntax
"
"
 torch. asarray ( obj ,  * ,  dtype ,  device ,  copy ,  requires_grad )  →  Tensor ¶","  File ""example.py"", line 18
    tensor([1., 2., 3.], grad_fn=<AddBackward0>)array=numpy.array([1,2,3])
                                 ^
SyntaxError: invalid syntax
"
"
 class torch. enable_grad [source] ¶","  File ""example.py"", line 3
    withtorch.no_grad():
                       ^
SyntaxError: invalid syntax
"
"
 torch. diag_embed ( input ,  offset ,  dim1 ,  dim2 )  →  Tensor ¶","  File ""example.py"", line 4
    [ 0.0000,  0.0000, -1.3986]]])torch.diag_embed(a,offset=1,dim1=0,dim2=2)
    ^
IndentationError: unexpected indent
"
"
 torch. round ( input ,  * ,  decimals ,  out )  →  Tensor ¶","Traceback (most recent call last):
  File ""example.py"", line 3, in <module>
    tensor([ 5.,  -2.,  9., -8.])# Values equidistant from two integers are rounded towards the
NameError: name 'tensor' is not defined
"
"
 torch. chain_matmul ( * ,  out ) [source] ¶","/home/nimashiri/.local/lib/python3.8/site-packages/torch/functional.py:1588: UserWarning: torch.chain_matmul is deprecated and will be removed in a future PyTorch release. Use torch.linalg.multi_dot instead, which accepts a list of two or more tensors rather than multiple parameters. (Triggered internally at /media/nimashiri/SSD1/pytorch/aten/src/ATen/native/LinearAlgebra.cpp:1072.)
  return _VF.chain_matmul(matrices)  # type: ignore[attr-defined]
"
"
 torch. fake_quantize_per_channel_affine ( input ,  scale ,  zero_point ,  quant_min ,  quant_max )  →  Tensor ¶","Traceback (most recent call last):
  File ""example.py"", line 8, in <module>
    torch.fake_quantize_per_channel_affine(x,scales,zero_points,1,0,255)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 99, in wrapper
    outputs = func(*args, **kwargs)
RuntimeError: Zero-point must be Int32, Float or Half, found Long
"
"
 torch. log2 ( input ,  * ,  out )  →  Tensor ¶","  File ""example.py"", line 4
    tensor([ 0.8419,  0.8003,  0.9971,  0.5287,  0.0490])torch.log2(a)
                                                         ^
SyntaxError: invalid syntax
"
"
 torch. ones ( *size ,  * ,  out=None ,  dtype=None ,  layout=torch.strided ,  device=None ,  requires_grad=False )  →  Tensor ¶","  File ""example.py"", line 3
    [ 1.,  1.,  1.]])torch.ones(5)
    ^
IndentationError: unexpected indent
"
"
 torch. get_default_dtype ( )  →  torch.dtype ¶","Traceback (most recent call last):
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 13, in json_serialize
    json.dumps(v)
  File ""/usr/lib/python3.8/json/__init__.py"", line 231, in dumps
    return _default_encoder.encode(obj)
  File ""/usr/lib/python3.8/json/encoder.py"", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/usr/lib/python3.8/json/encoder.py"", line 257, in iterencode
    return _iterencode(o, 0)
  File ""/usr/lib/python3.8/json/encoder.py"", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type tensortype is not JSON serializable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 56, in get_var_shape
    return list(s)  # convert torch.Size to list
TypeError: 'getset_descriptor' object is not iterable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""example.py"", line 5, in <module>
    torch.set_default_tensor_type(torch.FloatTensor)# setting tensor type also affects this
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 101, in wrapper
    param_dict = build_param_dict(*args, **kwargs)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 42, in build_param_dict
    param_dict['parameter:%d' % ind] = json_serialize(arg)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 17, in json_serialize
    return get_var_signature(
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 88, in get_var_signature
    s['shape'] = get_var_shape(var)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 58, in get_var_shape
    print(e.message)
AttributeError: 'TypeError' object has no attribute 'message'
"
"
 class torch. Generator ( device )  →  Generator ¶","Traceback (most recent call last):
  File ""example.py"", line 3, in <module>
    g_cuda=torch.Generator(device='cuda')
RuntimeError: Device type CUDA is not supported for torch.Generator() api.
"
"
 torch. meshgrid ( * ,  indexing ) [source] ¶","  File ""example.py"", line 3
    y=torch.tensor([4,5,6])Observe the element-wise pairings across the grid, (1, 4),
                           ^
SyntaxError: invalid syntax
"
"
 Tensor. index_reduce_ ( dim ,  index ,  source ,  reduce ,  * ,  include_self )  →  Tensor ¶","/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py:99: UserWarning: index_reduce() is in beta and the API may change at any time. (Triggered internally at /media/nimashiri/SSD1/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1070.)
  outputs = func(*args, **kwargs)
"
"
 torch. bernoulli ( input ,  * ,  generator ,  out )  →  Tensor ¶","  File ""example.py"", line 5
    [ 1.,  1.,  1.]])a=torch.ones(3,3)# probability of drawing ""1"" is 1
    ^
IndentationError: unexpected indent
"
"
 torch. argsort ( input ,  dim ,  descending )  →  LongTensor ¶","  File ""example.py"", line 4
    [ 0.0669, -0.2318, -0.8229, -0.9280]])torch.argsort(a,dim=1)
    ^
IndentationError: unexpected indent
"
"
 torch. searchsorted ( sorted_sequence ,  values ,  * ,  out_int32 ,  right ,  side ,  out ,  sorter )  →  Tensor ¶","  File ""example.py"", line 8
    [1, 3, 4]])sorted_sequence_1d=torch.tensor([1,3,5,7,9])
    ^
IndentationError: unexpected indent
"
"
 torch. nonzero ( input ,  * ,  out ,  as_tuple )  →  LongTensor   or   tuple   of   LongTensors ¶","  File ""example.py"", line 5
    torch.nonzero(torch.tensor([[0.6,0.0,0.0,0.0],
    ^
SyntaxError: invalid syntax
"
"
 torch. randint ( low=0 ,  high ,  size ,  \* ,  generator=None ,  out=None ,  dtype=None ,  layout=torch.strided ,  device=None ,  requires_grad=False )  →  Tensor ¶","  File ""example.py"", line 3
    tensor([4, 3, 4])torch.randint(10,(2,2))
                     ^
SyntaxError: invalid syntax
"
"
 torch. cartesian_prod ( * ) [source] ¶","Traceback (most recent call last):
  File ""example.py"", line 4, in <module>
    list(itertools.product(a,b))
NameError: name 'itertools' is not defined
"
"
 torch. tril_indices ( row ,  col ,  offset ,  * ,  dtype ,  device ,  layout )  →  Tensor ¶","  File ""example.py"", line 4
    [0, 0, 1, 0, 1, 2]])a=torch.tril_indices(4,3,-1)
    ^
IndentationError: unexpected indent
"
"
 torch. float_power ( input ,  exponent ,  * ,  out )  →  Tensor ¶","  File ""example.py"", line 5
    tensor([36., 16., 49.,  1.], dtype=torch.float64)a=torch.arange(1,5)
                                                     ^
SyntaxError: invalid syntax
"
"
 torch. cholesky ( input ,  upper ,  * ,  out )  →  Tensor ¶","/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py:99: UserWarning: torch.cholesky is deprecated in favor of torch.linalg.cholesky and will be removed in a future PyTorch release.
L = torch.cholesky(A)
should be replaced with
L = torch.linalg.cholesky(A)
and
U = torch.cholesky(A, upper=True)
should be replaced with
U = torch.linalg.cholesky(A).mH().
This transform will produce equivalent results for all valid (symmetric positive definite) inputs. (Triggered internally at /media/nimashiri/SSD1/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:1718.)
  outputs = func(*args, **kwargs)
"
"
 torch. combinations ( input ,  r ,  with_replacement )  →  seq ¶","Traceback (most recent call last):
  File ""example.py"", line 3, in <module>
    list(itertools.combinations(a,r=2))
NameError: name 'itertools' is not defined
"
"
 torch. aminmax ( input ,  * ,  dim=None ,  keepdim=False ,  out=None) ,  Tensor ) ¶","  File ""example.py"", line 3
    max=tensor(5))# aminmax propagates NaNs
                 ^
SyntaxError: unmatched ')'
"
"
 torch. slice_scatter ( input ,  src ,  dim ,  start ,  end ,  step )  →  Tensor ¶","  File ""example.py"", line 5
    [1., 1., 1., 1., 1., 1., 1., 1.]])b=torch.ones(2)
    ^
IndentationError: unexpected indent
"
"
 torch. exp ( input ,  * ,  out )  →  Tensor ¶","Traceback (most recent call last):
  File ""example.py"", line 2, in <module>
    torch.exp(torch.tensor([0,math.log(2.)]))
NameError: name 'math' is not defined
"
"
 torch. qr ( input ,  some ,  * ,  out ) ¶","/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py:99: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.
The boolean parameter 'some' has been replaced with a string parameter 'mode'.
Q, R = torch.qr(A, some)
should be replaced with
Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at /media/nimashiri/SSD1/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2452.)
  outputs = func(*args, **kwargs)
Traceback (most recent call last):
  File ""example.py"", line 11, in <module>
    torch.allclose(torch.matmul(q.mT,q),torch.eye(5))
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 99, in wrapper
    outputs = func(*args, **kwargs)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 99, in wrapper
    outputs = func(*args, **kwargs)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 99, in wrapper
    outputs = func(*args, **kwargs)
RuntimeError: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 2
"
"
 torch. ravel ( input )  →  Tensor ¶","  File ""example.py"", line 4
    
                  ^
SyntaxError: unexpected EOF while parsing
"
"
 torch. cumprod ( input ,  dim ,  * ,  dtype ,  out )  →  Tensor ¶","  File ""example.py"", line 5
    0.0014, -0.0006, -0.0001])a[5]=0.0
    ^
IndentationError: unexpected indent
"
"
 torch. lstsq ( input ,  A ,  * ,  out ) ¶","  File ""example.py"", line 3
    B=torch.tensor([[-10.,-3],
     ^
SyntaxError: invalid syntax
"
"
 torch. from_numpy ( ndarray )  →  Tensor ¶","Traceback (most recent call last):
  File ""example.py"", line 2, in <module>
    a=numpy.array([1,2,3])
NameError: name 'numpy' is not defined
"
"
 torch. rot90 ( input ,  k ,  dims )  →  Tensor ¶","  File ""example.py"", line 5
    [0, 2]])x=torch.arange(8).view(2,2,2)
    ^
IndentationError: unexpected indent
"
"
 torch. as_tensor ( data ,  dtype ,  device )  →  Tensor ¶","  File ""example.py"", line 7
    array([-1,  2,  3])a=numpy.array([1,2,3])
                       ^
SyntaxError: invalid syntax
"
"
 torch. eig ( input ,  eigenvectors ,  * ,  out ) ¶","  File ""example.py"", line 2
    Trivialexamplewithadiagonalmatrix.Bydefault,onlyeigenvaluesarecomputed:>>>a=torch.diag(torch.tensor([1,2,3],dtype=torch.double))
                                                                           ^
SyntaxError: invalid syntax
"
"
 torch. inner ( input ,  other ,  * ,  out )  →  Tensor ¶","  File ""example.py"", line 2
    >>>torch.inner(torch.tensor([1,2,3]),torch.tensor([0,2,1]))
    ^
SyntaxError: invalid syntax
"
"
 torch. range ( start=0 ,  end ,  step=1 ,  * ,  out=None ,  dtype=None ,  layout=torch.strided ,  device=None ,  requires_grad=False )  →  Tensor ¶","/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py:99: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).
  outputs = func(*args, **kwargs)
"
"
 torch. lu ( * ,  ** ) ¶","  File ""example.py"", line 7
    ifinfo.nonzero().size(0)==0:
                               ^
SyntaxError: invalid syntax
"
"
 torch. polar ( abs ,  angle ,  * ,  out )  →  Tensor ¶","Traceback (most recent call last):
  File ""example.py"", line 2, in <module>
    importnumpyasnp
NameError: name 'importnumpyasnp' is not defined
"
"
 class torch. no_grad [source] ¶","  File ""example.py"", line 3
    withtorch.no_grad():
                       ^
SyntaxError: invalid syntax
"
"
 Tensor. scatter_ ( dim ,  index ,  src ,  reduce )  →  Tensor ¶","  File ""example.py"", line 8
    [0, 0, 0, 0, 0]])torch.full((2,4),2.).scatter_(1,torch.tensor([[2],[3]]),
    ^
IndentationError: unexpected indent
"
"
 torch. frombuffer ( buffer ,  * ,  dtype ,  count ,  offset ,  requires_grad )  →  Tensor ¶","Traceback (most recent call last):
  File ""example.py"", line 2, in <module>
    importarray
NameError: name 'importarray' is not defined
"
"
 torch. triu_indices ( row ,  col ,  offset ,  * ,  dtype ,  device ,  layout )  →  Tensor ¶","  File ""example.py"", line 4
    [0, 1, 2, 1, 2, 2]])a=torch.triu_indices(4,3,-1)
    ^
IndentationError: unexpected indent
"
"
 torch. unique_consecutive ( * ,  ** ) ¶","  File ""example.py"", line 5
    tensor([1, 2, 3, 1, 2])output,inverse_indices=torch.unique_consecutive(x,return_inverse=True)
                           ^
SyntaxError: invalid syntax
"
"
 torch. bincount ( input ,  weights ,  minlength )  →  Tensor ¶","  File ""example.py"", line 5
    tensor([ 0.0000,  0.2500,  0.5000,  0.7500,  1.0000])torch.bincount(input)
    ^
IndentationError: unexpected indent
"
"
 torch. matrix_rank ( input ,  tol ,  symmetric ,  * ,  out )  →  Tensor ¶","Traceback (most recent call last):
  File ""example.py"", line 3, in <module>
    torch.matrix_rank(a)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 99, in wrapper
    outputs = func(*args, **kwargs)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/_linalg_utils.py"", line 102, in matrix_rank
    raise RuntimeError(
RuntimeError: ('This function was deprecated since version 1.9 and is now removed.', 'Please use the `torch.linalg.matrix_rank` function instead.')
"
"
 torch. take ( input ,  index )  →  Tensor ¶","  File ""example.py"", line 4
    
                                         ^
SyntaxError: unexpected EOF while parsing
"
"
 class torch. set_grad_enabled ( mode ) [source] ¶","  File ""example.py"", line 4
    withtorch.set_grad_enabled(is_train):
                                        ^
SyntaxError: invalid syntax
"
"
 torch. sparse_coo_tensor ( indices ,  values ,  size ,  * ,  dtype ,  device ,  requires_grad )  →  Tensor ¶","  File ""example.py"", line 3
    v=torch.tensor([3,4,5],dtype=torch.float32)
     ^
SyntaxError: invalid syntax
"
"
 torch. kthvalue ( input ,  k ,  dim ,  keepdim ,  * ,  out ) ¶","  File ""example.py"", line 5
    torch.return_types.kthvalue(values=tensor(4.), indices=tensor(3))x=torch.arange(1.,7.).resize_(2,3)
                                                                     ^
SyntaxError: invalid syntax
"
"
 torch. unique ( input ,  sorted ,  return_inverse ,  return_counts ,  dim )  →  Tuple [ Tensor , Tensor , Tensor ] ¶","  File ""example.py"", line 4
    tensor([ 2,  3,  1])output,inverse_indices=torch.unique(
                        ^
SyntaxError: invalid syntax
"
"
 class torch.nn. DataParallel ( module ,  device_ids ,  output_device ,  dim ) [source] ¶","Traceback (most recent call last):
  File ""example.py"", line 2, in <module>
    net=torch.nn.DataParallel(model,device_ids=[0,1,2])
NameError: name 'model' is not defined
"
"
 torch.nn.utils.parametrizations. orthogonal ( module ,  name ,  orthogonal_map ,  * ,  use_trivialization ) [source] ¶","Traceback (most recent call last):
  File ""example.py"", line 2, in <module>
    orth_linear=orthogonal(nn.Linear(20,40))
NameError: name 'orthogonal' is not defined
"
"
 class torch.nn. MaxUnpool2d ( kernel_size ,  stride ,  padding ) [source] ¶","  File ""example.py"", line 5
    output,indices=pool(input)
                  ^
SyntaxError: invalid syntax
"
"
 torch.nn.utils. weight_norm ( module ,  name ,  dim ) [source] ¶","Traceback (most recent call last):
  File ""example.py"", line 2, in <module>
    m=weight_norm(nn.Linear(20,40),name='weight')
NameError: name 'weight_norm' is not defined
"
"
 torch.nn.utils. spectral_norm ( module ,  name ,  n_power_iterations ,  eps ,  dim ) [source] ¶","Traceback (most recent call last):
  File ""example.py"", line 2, in <module>
    m=spectral_norm(nn.Linear(20,40))
NameError: name 'spectral_norm' is not defined
"
"
 class torch.nn. MaxUnpool3d ( kernel_size ,  stride ,  padding ) [source] ¶","Traceback (most recent call last):
  File ""example.py"", line 3, in <module>
    pool=nn.MaxPool3d(3,stride=2,return_indices=True)
NameError: name 'nn' is not defined
"
"
 torch.nn.utils.parametrizations. spectral_norm ( module ,  name ,  n_power_iterations ,  eps ,  dim ) [source] ¶","Traceback (most recent call last):
  File ""example.py"", line 2, in <module>
    snm=spectral_norm(nn.Linear(20,40))
NameError: name 'spectral_norm' is not defined
"
"
 torch.nn.utils. skip_init ( module_cls ,  * ,  ** ) [source] ¶","Traceback (most recent call last):
  File ""example.py"", line 2, in <module>
    importtorch
NameError: name 'importtorch' is not defined
"
"
 class torch.nn. MaxUnpool1d ( kernel_size ,  stride ,  padding ) [source] ¶","  File ""example.py"", line 11
    tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.,  0.]]])unpool(output,indices)
                                                            ^
SyntaxError: invalid syntax
"
"
 class torch.nn.parallel. DistributedDataParallel ( module ,  device_ids ,  output_device ,  dim ,  broadcast_buffers ,  process_group ,  bucket_cap_mb ,  find_unused_parameters ,  check_reduction ,  gradient_as_bucket_view ,  static_graph ) [source] ¶","Traceback (most recent call last):
  File ""example.py"", line 2, in <module>
    torch.distributed.init_process_group(backend='nccl',world_size=4,init_method='...')
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py"", line 836, in init_process_group
    rendezvous_iterator = rendezvous(
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/distributed/rendezvous.py"", line 103, in rendezvous
    return _rendezvous_helper(url, rank, world_size, **kwargs)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/distributed/rendezvous.py"", line 89, in _rendezvous_helper
    raise RuntimeError(""No rendezvous handler for {}://"".format(result.scheme))
RuntimeError: No rendezvous handler for ://
"
"
 torch.nn.functional. nll_loss ( input ,  target ,  weight ,  size_average ,  ignore_index ,  reduce ,  reduction ) [source] ¶","Traceback (most recent call last):
  File ""example.py"", line 6, in <module>
    output=F.nll_loss(F.log_softmax(input),target)
NameError: name 'F' is not defined
"
"
 torch.nn.functional. ctc_loss ( log_probs ,  targets ,  input_lengths ,  target_lengths ,  blank ,  reduction ,  zero_infinity ) [source] ¶","Traceback (most recent call last):
  File ""example.py"", line 6, in <module>
    loss=F.ctc_loss(log_probs,targets,input_lengths,target_lengths)
NameError: name 'F' is not defined
"
"
 torch.nn.functional. cosine_similarity ( x1 ,  x2 ,  dim ,  eps )  →  Tensor ¶","Traceback (most recent call last):
  File ""example.py"", line 4, in <module>
    output=F.cosine_similarity(input1,input2)
NameError: name 'F' is not defined
"
"
 torch. sort ( input ,  dim ,  descending ,  stable ,  * ,  out ) ¶","  File ""example.py"", line 6
    [ 0,  3,  1,  2]])sorted,indices=torch.sort(x,0)
    ^
IndentationError: unexpected indent
"
"
 Tensor. put_ ( index ,  source ,  accumulate )  →  Tensor ¶","  File ""example.py"", line 4
    
                                                      ^
SyntaxError: unexpected EOF while parsing
"
"
 torch. log10 ( input ,  * ,  out )  →  Tensor ¶","  File ""example.py"", line 4
    tensor([ 0.5224,  0.9354,  0.7257,  0.1301,  0.2251])torch.log10(a)
                                                         ^
SyntaxError: invalid syntax
"
"
 torch. tensor_split ( input ,  indices_or_sections ,  dim )  →  List   of   Tensors ¶","  File ""example.py"", line 4
    (tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7]))x=torch.arange(7)
                                                          ^
SyntaxError: invalid syntax
"
"
 torch. lu_solve ( b ,  LU_data ,  LU_pivots ,  * ,  out )  →  Tensor ¶","/home/nimashiri/.local/lib/python3.8/site-packages/torch/functional.py:1690: UserWarning: torch.lu is deprecated in favor of torch.linalg.lu_factor / torch.linalg.lu_factor_ex and will be removed in a future PyTorch release.
LU, pivots = torch.lu(A, compute_pivots)
should be replaced with
LU, pivots = torch.linalg.lu_factor(A, compute_pivots)
and
LU, pivots, info = torch.lu(A, compute_pivots, get_infos=True)
should be replaced with
LU, pivots, info = torch.linalg.lu_factor_ex(A, compute_pivots) (Triggered internally at /media/nimashiri/SSD1/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2018.)
  return torch._lu_with_info(A, pivot=pivot, check_errors=(not get_infos))
/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py:99: UserWarning: torch.lu_solve is deprecated in favor of torch.linalg.lu_solveand will be removed in a future PyTorch release.
Note that torch.linalg.lu_solve has its arguments reversed.
X = torch.lu_solve(B, LU, pivots)
should be replaced with
X = torch.linalg.lu_solve(LU, pivots, B) (Triggered internally at /media/nimashiri/SSD1/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2175.)
  outputs = func(*args, **kwargs)
"
"
 Tensor. requires_grad_ ( requires_grad )  →  Tensor ¶","Traceback (most recent call last):
  File ""example.py"", line 6, in <module>
    weights=preprocess(loaded_weights)# some function
NameError: name 'preprocess' is not defined
"
"
 Tensor. scatter_reduce_ ( dim ,  index ,  src ,  reduce ,  * ,  include_self )  →  Tensor ¶","/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py:99: UserWarning: scatter_reduce() is in beta and the API may change at any time. (Triggered internally at /media/nimashiri/SSD1/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1685.)
  outputs = func(*args, **kwargs)
"
"
 torch. norm ( input ,  p ,  dim ,  keepdim ,  out ,  dtype ) [source] ¶","Traceback (most recent call last):
  File ""example.py"", line 2, in <module>
    importtorch
NameError: name 'importtorch' is not defined
"
"
 torch. tril ( input ,  diagonal ,  * ,  out )  →  Tensor ¶","  File ""example.py"", line 5
    [-0.3409, -0.9828,  0.0289]])b=torch.randn(4,6)
    ^
IndentationError: unexpected indent
"
"
 torch. flatten ( input ,  start_dim ,  end_dim )  →  Tensor ¶","  File ""example.py"", line 4
    torch.flatten(t,start_dim=1)
    ^
SyntaxError: invalid syntax
"
"
 torch. tensor ( data ,  * ,  dtype ,  device ,  requires_grad ,  pin_memory )  →  Tensor ¶","  File ""example.py"", line 3
    [ 4.9000,  5.2000]])torch.tensor([0,1])# Type inference on data
    ^
IndentationError: unexpected indent
"
"
 torch. triu ( input ,  diagonal ,  * ,  out )  →  Tensor ¶","  File ""example.py"", line 7
    [ 0.0000, -0.5211, -0.4573]])b=torch.randn(4,6)
    ^
IndentationError: unexpected indent
"
"
 Tensor. view ( * )  →  Tensor ¶","  File ""example.py"", line 8
    torch.Size([2, 8])a=torch.randn(1,2,3,4)
                      ^
SyntaxError: invalid syntax
"
"
 Tensor. register_hook ( hook ) [source] ¶","  File ""example.py"", line 3
    h=v.register_hook(lambdagrad:grad*2)# double the gradient
                                ^
SyntaxError: invalid syntax
"
"
 Tensor. sparse_mask ( mask )  →  Tensor ¶","  File ""example.py"", line 5
    V=torch.randn(nse,dims[2],dims[3])
     ^
SyntaxError: invalid syntax
"
"
 torch. clamp ( input ,  min ,  max ,  * ,  out )  →  Tensor ¶","  File ""example.py"", line 5
    tensor([-0.5000,  0.1734, -0.0478, -0.0922])min=torch.linspace(-1,1,steps=4)
                                                ^
SyntaxError: invalid syntax
"
"
 torch. pow ( input ,  exponent ,  * ,  out )  →  Tensor ¶","  File ""example.py"", line 5
    exp=torch.arange(1.,5.)a=torch.arange(1.,5.)
                           ^
SyntaxError: invalid syntax
"
"
 torch. diag_embed ( input ,  offset ,  dim1 ,  dim2 )  →  Tensor ¶","  File ""example.py"", line 4
    [ 0.0000,  0.0000, -1.3986]]])torch.diag_embed(a,offset=1,dim1=0,dim2=2)
    ^
IndentationError: unexpected indent
"
"
 torch. round ( input ,  * ,  decimals ,  out )  →  Tensor ¶","Traceback (most recent call last):
  File ""example.py"", line 3, in <module>
    tensor([ 5.,  -2.,  9., -8.])# Values equidistant from two integers are rounded towards the
NameError: name 'tensor' is not defined
"
"
 torch. log2 ( input ,  * ,  out )  →  Tensor ¶","  File ""example.py"", line 4
    tensor([ 0.8419,  0.8003,  0.9971,  0.5287,  0.0490])torch.log2(a)
                                                         ^
SyntaxError: invalid syntax
"
"
 Tensor. to ( * ,  ** )  →  Tensor ¶","  File ""example.py"", line 4
    [ 0.3310, -0.0584]], dtype=torch.float64)cuda0=torch.device('cuda:0')
    ^
IndentationError: unexpected indent
"
"
 torch. argsort ( input ,  dim ,  descending )  →  LongTensor ¶","  File ""example.py"", line 4
    [ 0.0669, -0.2318, -0.8229, -0.9280]])torch.argsort(a,dim=1)
    ^
IndentationError: unexpected indent
"
"
 Tensor. index_reduce_ ( dim ,  index ,  source ,  reduce ,  * ,  include_self )  →  Tensor ¶","/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py:99: UserWarning: index_reduce() is in beta and the API may change at any time. (Triggered internally at /media/nimashiri/SSD1/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1070.)
  outputs = func(*args, **kwargs)
"
"
 torch. nonzero ( input ,  * ,  out ,  as_tuple )  →  LongTensor   or   tuple   of   LongTensors ¶","  File ""example.py"", line 5
    torch.nonzero(torch.tensor([[0.6,0.0,0.0,0.0],
    ^
SyntaxError: invalid syntax
"
"
 torch. float_power ( input ,  exponent ,  * ,  out )  →  Tensor ¶","  File ""example.py"", line 5
    tensor([36., 16., 49.,  1.], dtype=torch.float64)a=torch.arange(1,5)
                                                     ^
SyntaxError: invalid syntax
"
"
 torch. cholesky ( input ,  upper ,  * ,  out )  →  Tensor ¶","/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py:99: UserWarning: torch.cholesky is deprecated in favor of torch.linalg.cholesky and will be removed in a future PyTorch release.
L = torch.cholesky(A)
should be replaced with
L = torch.linalg.cholesky(A)
and
U = torch.cholesky(A, upper=True)
should be replaced with
U = torch.linalg.cholesky(A).mH().
This transform will produce equivalent results for all valid (symmetric positive definite) inputs. (Triggered internally at /media/nimashiri/SSD1/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:1718.)
  outputs = func(*args, **kwargs)
"
"
 torch. aminmax ( input ,  * ,  dim=None ,  keepdim=False ,  out=None) ,  Tensor ) ¶","  File ""example.py"", line 3
    max=tensor(5))# aminmax propagates NaNs
                 ^
SyntaxError: unmatched ')'
"
"
 torch. exp ( input ,  * ,  out )  →  Tensor ¶","Traceback (most recent call last):
  File ""example.py"", line 2, in <module>
    torch.exp(torch.tensor([0,math.log(2.)]))
NameError: name 'math' is not defined
"
"
 torch. slice_scatter ( input ,  src ,  dim ,  start ,  end ,  step )  →  Tensor ¶","  File ""example.py"", line 5
    [1., 1., 1., 1., 1., 1., 1., 1.]])b=torch.ones(2)
    ^
IndentationError: unexpected indent
"
"
 torch. qr ( input ,  some ,  * ,  out ) ¶","/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py:99: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.
The boolean parameter 'some' has been replaced with a string parameter 'mode'.
Q, R = torch.qr(A, some)
should be replaced with
Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at /media/nimashiri/SSD1/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2452.)
  outputs = func(*args, **kwargs)
Traceback (most recent call last):
  File ""example.py"", line 11, in <module>
    torch.allclose(torch.matmul(q.mT,q),torch.eye(5))
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 99, in wrapper
    outputs = func(*args, **kwargs)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 99, in wrapper
    outputs = func(*args, **kwargs)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 99, in wrapper
    outputs = func(*args, **kwargs)
RuntimeError: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 2
"
"
 torch. ravel ( input )  →  Tensor ¶","  File ""example.py"", line 4
    
                  ^
SyntaxError: unexpected EOF while parsing
"
"
 torch. cumprod ( input ,  dim ,  * ,  dtype ,  out )  →  Tensor ¶","  File ""example.py"", line 5
    0.0014, -0.0006, -0.0001])a[5]=0.0
    ^
IndentationError: unexpected indent
"
"
 torch. lstsq ( input ,  A ,  * ,  out ) ¶","  File ""example.py"", line 3
    B=torch.tensor([[-10.,-3],
     ^
SyntaxError: invalid syntax
"
"
 torch. rot90 ( input ,  k ,  dims )  →  Tensor ¶","  File ""example.py"", line 5
    [0, 2]])x=torch.arange(8).view(2,2,2)
    ^
IndentationError: unexpected indent
"
"
 torch. as_tensor ( data ,  dtype ,  device )  →  Tensor ¶","  File ""example.py"", line 7
    array([-1,  2,  3])a=numpy.array([1,2,3])
                       ^
SyntaxError: invalid syntax
"
"
 torch. eig ( input ,  eigenvectors ,  * ,  out ) ¶","  File ""example.py"", line 2
    Trivialexamplewithadiagonalmatrix.Bydefault,onlyeigenvaluesarecomputed:>>>a=torch.diag(torch.tensor([1,2,3],dtype=torch.double))
                                                                           ^
SyntaxError: invalid syntax
"
"
 torch. inner ( input ,  other ,  * ,  out )  →  Tensor ¶","  File ""example.py"", line 2
    >>>torch.inner(torch.tensor([1,2,3]),torch.tensor([0,2,1]))
    ^
SyntaxError: invalid syntax
"
"
 torch. lu ( * ,  ** ) ¶","  File ""example.py"", line 7
    ifinfo.nonzero().size(0)==0:
                               ^
SyntaxError: invalid syntax
"
"
 Tensor. scatter_ ( dim ,  index ,  src ,  reduce )  →  Tensor ¶","  File ""example.py"", line 8
    [0, 0, 0, 0, 0]])torch.full((2,4),2.).scatter_(1,torch.tensor([[2],[3]]),
    ^
IndentationError: unexpected indent
"
"
 torch. bincount ( input ,  weights ,  minlength )  →  Tensor ¶","  File ""example.py"", line 5
    tensor([ 0.0000,  0.2500,  0.5000,  0.7500,  1.0000])torch.bincount(input)
    ^
IndentationError: unexpected indent
"
"
 torch. take ( input ,  index )  →  Tensor ¶","  File ""example.py"", line 4
    
                                         ^
SyntaxError: unexpected EOF while parsing
"
"
 torch. kthvalue ( input ,  k ,  dim ,  keepdim ,  * ,  out ) ¶","  File ""example.py"", line 5
    torch.return_types.kthvalue(values=tensor(4.), indices=tensor(3))x=torch.arange(1.,7.).resize_(2,3)
                                                                     ^
SyntaxError: invalid syntax
"
"
 Tensor. view ( * )  →  Tensor ¶","  File ""example.py"", line 8
    torch.Size([2, 8])a=torch.randn(1,2,3,4)
                      ^
SyntaxError: invalid syntax
"
"
 class torch.nn. DataParallel ( module ,  device_ids ,  output_device ,  dim ) [source] ¶","Traceback (most recent call last):
  File ""example.py"", line 2, in <module>
    net=torch.nn.DataParallel(model,device_ids=[0,1,2])
NameError: name 'model' is not defined
"
"
 class torch.nn.parallel. DistributedDataParallel ( module ,  device_ids ,  output_device ,  dim ,  broadcast_buffers ,  process_group ,  bucket_cap_mb ,  find_unused_parameters ,  check_reduction ,  gradient_as_bucket_view ,  static_graph ) [source] ¶","Traceback (most recent call last):
  File ""example.py"", line 2, in <module>
    torch.distributed.init_process_group(backend='nccl',world_size=4,init_method='...')
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py"", line 836, in init_process_group
    rendezvous_iterator = rendezvous(
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/distributed/rendezvous.py"", line 103, in rendezvous
    return _rendezvous_helper(url, rank, world_size, **kwargs)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/distributed/rendezvous.py"", line 89, in _rendezvous_helper
    raise RuntimeError(""No rendezvous handler for {}://"".format(result.scheme))
RuntimeError: No rendezvous handler for ://
"
"
 class torch.autograd.forward_ad. dual_level [source] ¶","  File ""example.py"", line 4
    withdual_level():
                    ^
SyntaxError: invalid syntax
"
"
 torch. zeros ( *size ,  * ,  out=None ,  dtype=None ,  layout=torch.strided ,  device=None ,  requires_grad=False )  →  Tensor ¶","  File ""example.py"", line 3
    [ 0.,  0.,  0.]])torch.zeros(5)
    ^
IndentationError: unexpected indent
"
"
 class torch.autograd. enable_grad [source] ¶","  File ""example.py"", line 3
    withtorch.no_grad():
                       ^
SyntaxError: invalid syntax
"
"
 torch.autograd.forward_ad. unpack_dual ( tensor ,  * ,  level ) [source] ¶","  File ""example.py"", line 2
    withdual_level():
                    ^
SyntaxError: invalid syntax
"
"
 class torch.autograd. set_grad_enabled ( mode ) [source] ¶","  File ""example.py"", line 4
    withtorch.set_grad_enabled(is_train):
                                        ^
SyntaxError: invalid syntax
"
"
 class torch.autograd. no_grad [source] ¶","  File ""example.py"", line 3
    withtorch.no_grad():
                       ^
SyntaxError: invalid syntax
"
"
 torch. ones ( *size ,  * ,  out=None ,  dtype=None ,  layout=torch.strided ,  device=None ,  requires_grad=False )  →  Tensor ¶","  File ""example.py"", line 3
    [ 1.,  1.,  1.]])torch.ones(5)
    ^
IndentationError: unexpected indent
"
"
 torch.autograd.forward_ad. make_dual ( tensor ,  tangent ,  * ,  level ) [source] ¶","  File ""example.py"", line 2
    withdual_level():
                    ^
SyntaxError: invalid syntax
"
"
 torch. lu ( * ,  ** ) ¶","  File ""example.py"", line 7
    ifinfo.nonzero().size(0)==0:
                               ^
SyntaxError: invalid syntax
"
"
 class torch.nn.parallel. DistributedDataParallel ( module ,  device_ids ,  output_device ,  dim ,  broadcast_buffers ,  process_group ,  bucket_cap_mb ,  find_unused_parameters ,  check_reduction ,  gradient_as_bucket_view ,  static_graph ) [source] ¶","Traceback (most recent call last):
  File ""example.py"", line 2, in <module>
    torch.distributed.init_process_group(backend='nccl',world_size=4,init_method='...')
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py"", line 836, in init_process_group
    rendezvous_iterator = rendezvous(
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/distributed/rendezvous.py"", line 103, in rendezvous
    return _rendezvous_helper(url, rank, world_size, **kwargs)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/distributed/rendezvous.py"", line 89, in _rendezvous_helper
    raise RuntimeError(""No rendezvous handler for {}://"".format(result.scheme))
RuntimeError: No rendezvous handler for ://
"
"
 class torch.nn. DataParallel ( module ,  device_ids ,  output_device ,  dim ) [source] ¶","Traceback (most recent call last):
  File ""example.py"", line 2, in <module>
    net=torch.nn.DataParallel(model,device_ids=[0,1,2])
NameError: name 'model' is not defined
"
"
 class torch.nn.parallel. DistributedDataParallel ( module ,  device_ids ,  output_device ,  dim ,  broadcast_buffers ,  process_group ,  bucket_cap_mb ,  find_unused_parameters ,  check_reduction ,  gradient_as_bucket_view ,  static_graph ) [source] ¶","Traceback (most recent call last):
  File ""example.py"", line 2, in <module>
    torch.distributed.init_process_group(backend='nccl',world_size=4,init_method='...')
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py"", line 836, in init_process_group
    rendezvous_iterator = rendezvous(
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/distributed/rendezvous.py"", line 103, in rendezvous
    return _rendezvous_helper(url, rank, world_size, **kwargs)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/distributed/rendezvous.py"", line 89, in _rendezvous_helper
    raise RuntimeError(""No rendezvous handler for {}://"".format(result.scheme))
RuntimeError: No rendezvous handler for ://
"
"
 torch.linalg. matrix_exp ( A )  →  Tensor ¶","  File ""example.py"", line 7
    [0.0000, 7.3891]]])importmath
    ^
IndentationError: unexpected indent
"
"
 torch.linalg. vander ( x ,  N )  →  Tensor ¶","Traceback (most recent call last):
  File ""example.py"", line 3, in <module>
    linalg.vander(x)
NameError: name 'linalg' is not defined
"
"
 torch. round ( input ,  * ,  decimals ,  out )  →  Tensor ¶","Traceback (most recent call last):
  File ""example.py"", line 3, in <module>
    tensor([ 5.,  -2.,  9., -8.])# Values equidistant from two integers are rounded towards the
NameError: name 'tensor' is not defined
"
"
 class torch.nn.parallel. DistributedDataParallel ( module ,  device_ids ,  output_device ,  dim ,  broadcast_buffers ,  process_group ,  bucket_cap_mb ,  find_unused_parameters ,  check_reduction ,  gradient_as_bucket_view ,  static_graph ) [source] ¶","Traceback (most recent call last):
  File ""example.py"", line 2, in <module>
    torch.distributed.init_process_group(backend='nccl',world_size=4,init_method='...')
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py"", line 836, in init_process_group
    rendezvous_iterator = rendezvous(
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/distributed/rendezvous.py"", line 103, in rendezvous
    return _rendezvous_helper(url, rank, world_size, **kwargs)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/distributed/rendezvous.py"", line 89, in _rendezvous_helper
    raise RuntimeError(""No rendezvous handler for {}://"".format(result.scheme))
RuntimeError: No rendezvous handler for ://
"
"
 class torch.nn.parallel. DistributedDataParallel ( module ,  device_ids ,  output_device ,  dim ,  broadcast_buffers ,  process_group ,  bucket_cap_mb ,  find_unused_parameters ,  check_reduction ,  gradient_as_bucket_view ,  static_graph ) [source] ¶","Traceback (most recent call last):
  File ""example.py"", line 2, in <module>
    torch.distributed.init_process_group(backend='nccl',world_size=4,init_method='...')
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py"", line 836, in init_process_group
    rendezvous_iterator = rendezvous(
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/distributed/rendezvous.py"", line 103, in rendezvous
    return _rendezvous_helper(url, rank, world_size, **kwargs)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/distributed/rendezvous.py"", line 89, in _rendezvous_helper
    raise RuntimeError(""No rendezvous handler for {}://"".format(result.scheme))
RuntimeError: No rendezvous handler for ://
"
"
 torch. zeros ( *size ,  * ,  out=None ,  dtype=None ,  layout=torch.strided ,  device=None ,  requires_grad=False )  →  Tensor ¶","  File ""example.py"", line 3
    [ 0.,  0.,  0.]])torch.zeros(5)
    ^
IndentationError: unexpected indent
"
"
 torch.sparse. sum ( input ,  dim ,  dtype ) [source] ¶","  File ""example.py"", line 5
    V=torch.randn(nnz,dims[2],dims[3])
     ^
SyntaxError: invalid syntax
"
"
 Tensor. sparse_mask ( mask )  →  Tensor ¶","  File ""example.py"", line 5
    V=torch.randn(nse,dims[2],dims[3])
     ^
SyntaxError: invalid syntax
"
"
 Tensor. to_dense ( )  →  Tensor ¶","  File ""example.py"", line 4
    
                ^
SyntaxError: unexpected EOF while parsing
"
"
 torch. sparse_coo_tensor ( indices ,  values ,  size ,  * ,  dtype ,  device ,  requires_grad )  →  Tensor ¶","  File ""example.py"", line 3
    v=torch.tensor([3,4,5],dtype=torch.float32)
     ^
SyntaxError: invalid syntax
"
"
 torch. get_default_dtype ( )  →  torch.dtype ¶","Traceback (most recent call last):
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 13, in json_serialize
    json.dumps(v)
  File ""/usr/lib/python3.8/json/__init__.py"", line 231, in dumps
    return _default_encoder.encode(obj)
  File ""/usr/lib/python3.8/json/encoder.py"", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/usr/lib/python3.8/json/encoder.py"", line 257, in iterencode
    return _iterencode(o, 0)
  File ""/usr/lib/python3.8/json/encoder.py"", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type tensortype is not JSON serializable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 56, in get_var_shape
    return list(s)  # convert torch.Size to list
TypeError: 'getset_descriptor' object is not iterable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""example.py"", line 5, in <module>
    torch.set_default_tensor_type(torch.FloatTensor)# setting tensor type also affects this
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 101, in wrapper
    param_dict = build_param_dict(*args, **kwargs)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 42, in build_param_dict
    param_dict['parameter:%d' % ind] = json_serialize(arg)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 17, in json_serialize
    return get_var_signature(
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 88, in get_var_signature
    s['shape'] = get_var_shape(var)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 58, in get_var_shape
    print(e.message)
AttributeError: 'TypeError' object has no attribute 'message'
"
"
 torch. as_tensor ( data ,  dtype ,  device )  →  Tensor ¶","  File ""example.py"", line 7
    array([-1,  2,  3])a=numpy.array([1,2,3])
                       ^
SyntaxError: invalid syntax
"
"
 class torch. no_grad [source] ¶","  File ""example.py"", line 3
    withtorch.no_grad():
                       ^
SyntaxError: invalid syntax
"
"
 class torch.nn.parallel. DistributedDataParallel ( module ,  device_ids ,  output_device ,  dim ,  broadcast_buffers ,  process_group ,  bucket_cap_mb ,  find_unused_parameters ,  check_reduction ,  gradient_as_bucket_view ,  static_graph ) [source] ¶","Traceback (most recent call last):
  File ""example.py"", line 2, in <module>
    torch.distributed.init_process_group(backend='nccl',world_size=4,init_method='...')
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py"", line 836, in init_process_group
    rendezvous_iterator = rendezvous(
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/distributed/rendezvous.py"", line 103, in rendezvous
    return _rendezvous_helper(url, rank, world_size, **kwargs)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/distributed/rendezvous.py"", line 89, in _rendezvous_helper
    raise RuntimeError(""No rendezvous handler for {}://"".format(result.scheme))
RuntimeError: No rendezvous handler for ://
"
"
 class torch. Generator ( device )  →  Generator ¶","Traceback (most recent call last):
  File ""example.py"", line 3, in <module>
    g_cuda=torch.Generator(device='cuda')
RuntimeError: Device type CUDA is not supported for torch.Generator() api.
"
"
 torch. get_default_dtype ( )  →  torch.dtype ¶","Traceback (most recent call last):
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 13, in json_serialize
    json.dumps(v)
  File ""/usr/lib/python3.8/json/__init__.py"", line 231, in dumps
    return _default_encoder.encode(obj)
  File ""/usr/lib/python3.8/json/encoder.py"", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/usr/lib/python3.8/json/encoder.py"", line 257, in iterencode
    return _iterencode(o, 0)
  File ""/usr/lib/python3.8/json/encoder.py"", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type tensortype is not JSON serializable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 56, in get_var_shape
    return list(s)  # convert torch.Size to list
TypeError: 'getset_descriptor' object is not iterable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""example.py"", line 5, in <module>
    torch.set_default_tensor_type(torch.FloatTensor)# setting tensor type also affects this
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 101, in wrapper
    param_dict = build_param_dict(*args, **kwargs)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 42, in build_param_dict
    param_dict['parameter:%d' % ind] = json_serialize(arg)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 17, in json_serialize
    return get_var_signature(
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 88, in get_var_signature
    s['shape'] = get_var_shape(var)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/torch/decorate_func.py"", line 58, in get_var_shape
    print(e.message)
AttributeError: 'TypeError' object has no attribute 'message'
"
"
 torch. zeros ( *size ,  * ,  out=None ,  dtype=None ,  layout=torch.strided ,  device=None ,  requires_grad=False )  →  Tensor ¶","  File ""example.py"", line 3
    [ 0.,  0.,  0.]])torch.zeros(5)
    ^
IndentationError: unexpected indent
"
"
 torch. tensor ( data ,  * ,  dtype ,  device ,  requires_grad ,  pin_memory )  →  Tensor ¶","  File ""example.py"", line 3
    [ 4.9000,  5.2000]])torch.tensor([0,1])# Type inference on data
    ^
IndentationError: unexpected indent
"
"
 torch. ones ( *size ,  * ,  out=None ,  dtype=None ,  layout=torch.strided ,  device=None ,  requires_grad=False )  →  Tensor ¶","  File ""example.py"", line 3
    [ 1.,  1.,  1.]])torch.ones(5)
    ^
IndentationError: unexpected indent
"
"
 Tensor. view ( * )  →  Tensor ¶","  File ""example.py"", line 8
    torch.Size([2, 8])a=torch.randn(1,2,3,4)
                      ^
SyntaxError: invalid syntax
"
"
 torch. exp ( input ,  * ,  out )  →  Tensor ¶","Traceback (most recent call last):
  File ""example.py"", line 2, in <module>
    torch.exp(torch.tensor([0,math.log(2.)]))
NameError: name 'math' is not defined
"
"
 torch. cumprod ( input ,  dim ,  * ,  dtype ,  out )  →  Tensor ¶","  File ""example.py"", line 5
    0.0014, -0.0006, -0.0001])a[5]=0.0
    ^
IndentationError: unexpected indent
"
"
 torch. zeros ( *size ,  * ,  out=None ,  dtype=None ,  layout=torch.strided ,  device=None ,  requires_grad=False )  →  Tensor ¶","  File ""example.py"", line 3
    [ 0.,  0.,  0.]])torch.zeros(5)
    ^
IndentationError: unexpected indent
"
"
 Tensor. register_hook ( hook ) [source] ¶","  File ""example.py"", line 3
    h=v.register_hook(lambdagrad:grad*2)# double the gradient
                                ^
SyntaxError: invalid syntax
"
"
 torch. clamp ( input ,  min ,  max ,  * ,  out )  →  Tensor ¶","  File ""example.py"", line 5
    tensor([-0.5000,  0.1734, -0.0478, -0.0922])min=torch.linspace(-1,1,steps=4)
                                                ^
SyntaxError: invalid syntax
"
"
 torch. log10 ( input ,  * ,  out )  →  Tensor ¶","  File ""example.py"", line 4
    tensor([ 0.5224,  0.9354,  0.7257,  0.1301,  0.2251])torch.log10(a)
                                                         ^
SyntaxError: invalid syntax
"
"
 torch. pow ( input ,  exponent ,  * ,  out )  →  Tensor ¶","  File ""example.py"", line 5
    exp=torch.arange(1.,5.)a=torch.arange(1.,5.)
                           ^
SyntaxError: invalid syntax
"
"
 torch. round ( input ,  * ,  decimals ,  out )  →  Tensor ¶","Traceback (most recent call last):
  File ""example.py"", line 3, in <module>
    tensor([ 5.,  -2.,  9., -8.])# Values equidistant from two integers are rounded towards the
NameError: name 'tensor' is not defined
"
"
 Tensor. requires_grad_ ( requires_grad )  →  Tensor ¶","Traceback (most recent call last):
  File ""example.py"", line 6, in <module>
    weights=preprocess(loaded_weights)# some function
NameError: name 'preprocess' is not defined
"
"
 torch. log2 ( input ,  * ,  out )  →  Tensor ¶","  File ""example.py"", line 4
    tensor([ 0.8419,  0.8003,  0.9971,  0.5287,  0.0490])torch.log2(a)
                                                         ^
SyntaxError: invalid syntax
"
"
 torch. ones ( *size ,  * ,  out=None ,  dtype=None ,  layout=torch.strided ,  device=None ,  requires_grad=False )  →  Tensor ¶","  File ""example.py"", line 3
    [ 1.,  1.,  1.]])torch.ones(5)
    ^
IndentationError: unexpected indent
"
"
 Tensor. to ( * ,  ** )  →  Tensor ¶","  File ""example.py"", line 4
    [ 0.3310, -0.0584]], dtype=torch.float64)cuda0=torch.device('cuda:0')
    ^
IndentationError: unexpected indent
"
"
 torch. bernoulli ( input ,  * ,  generator ,  out )  →  Tensor ¶","  File ""example.py"", line 5
    [ 1.,  1.,  1.]])a=torch.ones(3,3)# probability of drawing ""1"" is 1
    ^
IndentationError: unexpected indent
"
"
 torch. flatten ( input ,  start_dim ,  end_dim )  →  Tensor ¶","  File ""example.py"", line 4
    torch.flatten(t,start_dim=1)
    ^
SyntaxError: invalid syntax
"
"
 torch. tensor ( data ,  * ,  dtype ,  device ,  requires_grad ,  pin_memory )  →  Tensor ¶","  File ""example.py"", line 3
    [ 4.9000,  5.2000]])torch.tensor([0,1])# Type inference on data
    ^
IndentationError: unexpected indent
"
"
 torch. kthvalue ( input ,  k ,  dim ,  keepdim ,  * ,  out ) ¶","  File ""example.py"", line 5
    torch.return_types.kthvalue(values=tensor(4.), indices=tensor(3))x=torch.arange(1.,7.).resize_(2,3)
                                                                     ^
SyntaxError: invalid syntax
"
"tf.DeviceSpec(
    job=None, replica=None, task=None, device_type=None, device_index=None
)
","2022-12-02 10:33:20.629192: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-02 10:33:22.877586: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-12-02 10:33:22.878078: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2022-12-02 10:33:22.912986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-02 10:33:22.913131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5
coreClock: 1.77GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s
2022-12-02 10:33:22.913148: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-02 10:33:22.915176: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-12-02 10:33:22.915218: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-12-02 10:33:22.916043: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-12-02 10:33:22.916218: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-12-02 10:33:22.918214: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-12-02 10:33:22.918651: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-12-02 10:33:22.918741: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-12-02 10:33:22.918823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-02 10:33:22.918978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-02 10:33:22.919072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2022-12-02 10:33:22.920271: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-12-02 10:33:22.920353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-02 10:33:22.920478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5
coreClock: 1.77GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s
2022-12-02 10:33:22.920491: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-02 10:33:22.920503: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-12-02 10:33:22.920513: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-12-02 10:33:22.920522: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-12-02 10:33:22.920531: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-12-02 10:33:22.920540: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-12-02 10:33:22.920549: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-12-02 10:33:22.920558: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-12-02 10:33:22.920599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-02 10:33:22.920710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-02 10:33:22.920788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2022-12-02 10:33:22.920806: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-02 10:33:23.251557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-12-02 10:33:23.251579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2022-12-02 10:33:23.251585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2022-12-02 10:33:23.251731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-02 10:33:23.251851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-02 10:33:23.251940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-02 10:33:23.252014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4805 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
Traceback (most recent call last):
  File ""/home/nimashiri/.vscode/extensions/ms-python.python-2022.18.2/pythonFiles/lib/python/debugpy/_vendored/pydevd/pydevd.py"", line 3433, in <module>
    main()
  File ""/home/nimashiri/.vscode/extensions/ms-python.python-2022.18.2/pythonFiles/lib/python/debugpy/_vendored/pydevd/pydevd.py"", line 3426, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""/home/nimashiri/.vscode/extensions/ms-python.python-2022.18.2/pythonFiles/lib/python/debugpy/_vendored/pydevd/pydevd.py"", line 2502, in run
    return self._exec(is_module, entry_point_fn, module_name, file, globals, locals)
  File ""/home/nimashiri/.vscode/extensions/ms-python.python-2022.18.2/pythonFiles/lib/python/debugpy/_vendored/pydevd/pydevd.py"", line 2509, in _exec
    globals = pydevd_runpy.run_path(file, globals, '__main__')
  File ""/home/nimashiri/.vscode/extensions/ms-python.python-2022.18.2/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 321, in run_path
    return _run_module_code(code, init_globals, run_name,
  File ""/home/nimashiri/.vscode/extensions/ms-python.python-2022.18.2/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 135, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File ""/home/nimashiri/.vscode/extensions/ms-python.python-2022.18.2/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 124, in _run_code
    exec(code, run_globals)
  File ""example.py"", line 5, in <module>
    my_var = tf.Variable(..., name=""my_variable"")
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py"", line 262, in __call__
    return cls._variable_v2_call(*args, **kwargs)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py"", line 244, in _variable_v2_call
    return previous_getter(
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py"", line 237, in <lambda>
    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/tensorflow/python/ops/variable_scope.py"", line 2654, in default_variable_creator_v2
    return resource_variable_ops.ResourceVariable(
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py"", line 264, in __call__
    return super(VariableMetaclass, cls).__call__(*args, **kwargs)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 1574, in __init__
    self._init_from_args(
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 1717, in _init_from_args
    initial_value = ops.convert_to_tensor(initial_value,
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py"", line 163, in wrapped
    return func(*args, **kwargs)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 1540, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py"", line 339, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py"", line 264, in constant
    return _constant_impl(value, dtype, shape, name, verify_shape=False,
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py"", line 276, in _constant_impl
    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py"", line 301, in _constant_eager_impl
    t = convert_to_eager_tensor(value, ctx, dtype)
  File ""/home/nimashiri/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py"", line 98, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
ValueError: Attempt to convert a value (Ellipsis) with an unsupported type (<class 'ellipsis'>) to a Tensor.
"
